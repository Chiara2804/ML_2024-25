{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7e28e08a",
   "metadata": {},
   "source": [
    "**ML COURSE 2024-2025**\n",
    "# LAB6 UNSUPERVISED ML - Clustering and Anomaly Detection\n",
    "In this lab you will see K-Means and Agglomerative Hierarchical Clustering, and Hotelling T2 and Isolation Forest for anomaly detection (Lectures 19-20). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268ee22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from dataclasses import dataclass\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utils import generate_blobs_dataset, centroids_animation\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1015f8",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e7bc53",
   "metadata": {},
   "source": [
    "### K-Means"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945509f5",
   "metadata": {},
   "source": [
    "We took inspiration from [here](https://medium.com/@avijit.bhattacharjee1996/implementing-k-means-clustering-from-scratch-in-python-a277c23563ac)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b99e43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_distances(X, centroids): \n",
    "    \"\"\"Returns the pairwise distance of each point in X with respect to each centroid.\n",
    "    \n",
    "    Args:\n",
    "        X (np.ndarray): Data points of shape (n_samples, n_features).\n",
    "        centroids (np.ndarray): Centroids of shape (n_centroids, n_features).\n",
    "    \"\"\"\n",
    "    # distances = np.linalg.norm(X[:, np.newaxis] - self.centroids, axis=2) # more efficient but less intuitive\n",
    "    pairwise_distances = np.zeros((X.shape[0], centroids.shape[0]))\n",
    "    for i in range(X.shape[0]): \n",
    "        for j in range(centroids.shape[0]): \n",
    "            pairwise_distances[i, j] = np.linalg.norm(X[i] - centroids[j])\n",
    "    return pairwise_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a372f2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KMeansClustering: \n",
    "    def __init__(self, n_clusters:int, max_iters:int=100, tol:float=1e-6): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_clusters: int - the number of clusters.\n",
    "            max_iters: int - maximum number of iterations.\n",
    "            tol: float - tolerance for convergence.\n",
    "            random_state: int - seed for reproducibility.\n",
    "        \"\"\"\n",
    "        self.k = n_clusters\n",
    "        self.max_iters = max_iters\n",
    "        self.tol = tol\n",
    "        self.centroids = None\n",
    "        self.labels_ = None         # here we store the labels for the training point\n",
    "        self.history = []           # here we store the history of centroids \n",
    "\n",
    "    def _assign_clusters(self, X): \n",
    "        distances = pairwise_distances(X, self.centroids)\n",
    "        clusters = np.argmin(distances, axis=1)\n",
    "        return clusters\n",
    "    \n",
    "    def _update_centroids(self, X, labels): \n",
    "        new_centroids = np.array([X[labels == i].mean(axis=0) for i in range(self.k)])\n",
    "        return new_centroids\n",
    "\n",
    "    def fit(self, X): \n",
    "        self.history = []\n",
    "        self.labels_history = []\n",
    "\n",
    "        # centroids randomly initialized at the beginning\n",
    "        self.centroids = X[np.random.choice(X.shape[0], self.k, replace=False)]\n",
    "        labels = self._assign_clusters(X)\n",
    "\n",
    "        self.history.append(self.centroids.copy())\n",
    "        self.labels_history.append(labels.copy())\n",
    "        \n",
    "        # iterate until convergence or max_iters\n",
    "        for _ in range(self.max_iters): \n",
    "            new_centroids = self._update_centroids(X, labels)\n",
    "            if np.all(np.abs(new_centroids - self.centroids) < self.tol): \n",
    "                break\n",
    "            self.centroids = new_centroids\n",
    "            labels = self._assign_clusters(X)\n",
    "\n",
    "            self.history.append(self.centroids.copy())\n",
    "            self.labels_history.append(labels.copy())\n",
    "        \n",
    "        # assign the final training labels\n",
    "        self.labels_ = labels\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e956e45",
   "metadata": {},
   "source": [
    "#### Kmeans convergence is sensible to centroids initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faf6652",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "custom_centers = [[-10, -15], [0, 0], [10, -15], [25, -5], [10, 15]]\n",
    "Xblobs, _ = make_blobs(n_samples=1000, centers=custom_centers, cluster_std=1.5, random_state=0)\n",
    "\n",
    "kmeans = KMeansClustering(n_clusters=5)\n",
    "kmeans.fit(Xblobs)\n",
    "\n",
    "# plot results \n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.scatter(Xblobs[:, 0], Xblobs[:, 1], c=kmeans.labels_)\n",
    "plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c='red', marker='x', s=100, label='Centroids')\n",
    "plt.show()\n",
    "\n",
    "centroids = np.stack(kmeans.history, axis=0).transpose(1, 0, 2)\n",
    "labels = np.stack(kmeans.labels_history, axis=0)\n",
    "centroids_animation(Xblobs, centroids, labels, t_interval=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7338ea",
   "metadata": {},
   "source": [
    "### Determine the optimal number of clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773c3104",
   "metadata": {},
   "source": [
    "#### Elbow Method"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53873da4",
   "metadata": {},
   "source": [
    "The elbow method is a heuristic that helps us determine a good number of clusters.  \n",
    "This helps us when we don't know how many clusters we should tell our clustering algorithm to find.  \n",
    "This is a graphical method that works by computing a score over a range of possibile $k$ (number of clusters) values.\n",
    "\n",
    "The score we compute is the mean _Within-Cluster Dispersion_ ($W(k)$), defined as:\n",
    "\n",
    "$$\n",
    "W(k) = \\sum_{i=1}^{k} \\sum_{x \\in C_i} ||x - \\mu_i||^2\\\\\n",
    "W_{\\text{mean}}(k) = \\frac{1}{k} W(k) \n",
    "$$\n",
    "\n",
    "Where $C_i$ is the $i$-th cluster, $\\mu_i$ is the centroid of the $i$-th cluster, and $x$ is a point in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d1d1071",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_Wk(X, labels, centroids):\n",
    "    \"\"\"Compute the within-cluster dispersion Wk, with k clusters, for each centroid.\"\"\"\n",
    "    n_clusters = centroids.shape[0]\n",
    "    Wk = np.zeros(n_clusters)\n",
    "    for i in range(n_clusters):\n",
    "        Wk[i] = np.sum((X[labels == i] - centroids[i]) ** 2)\n",
    "    return np.sum(Wk)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bc41f7",
   "metadata": {},
   "source": [
    "Now we need to iterate over a range of $k$ values, compute the mean within-cluster dispersion $W_{\\text{mean}}(k)$ for each $k$, and plot the results.\n",
    "\n",
    "We can see that the best number of clusters is usually in the elbow of the curve, where the $W_{\\text{mean}}(k)$ starts to decrease more slowly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aeda4ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_wkmean(X, k_range, wkmean_list):\n",
    "    # Plot the Wk for each k\n",
    "    _, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "    # Plot the Wk for each k\n",
    "    axes[0].plot(k_range, wkmean_list, marker=\"o\")\n",
    "    axes[0].set_title(\"W_k for Different Values of k\")\n",
    "    axes[0].set_xlabel(\"Number of Clusters (k)\")\n",
    "    axes[0].set_ylabel(\"W_k\")\n",
    "    axes[0].set_xticks(k_range)\n",
    "    axes[0].grid()\n",
    "\n",
    "    # Scatter plot of the data\n",
    "    axes[1].scatter(X[:, 0], X[:, 1], s=10, alpha=0.7)\n",
    "    axes[1].set_title(\"Scatter Plot of the Data\")\n",
    "    axes[1].set_xlabel(\"Feature 1\")\n",
    "    axes[1].set_ylabel(\"Feature 2\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# let's experiment with different number of clusters\n",
    "n_true_clusters_range = range(2, 5)\n",
    "k_range = range(1, 10)  # range of k to test for each experiment\n",
    "\n",
    "for n_classes in n_true_clusters_range:\n",
    "    # dataset\n",
    "    n_samples, n_features = 200, 4\n",
    "    X, _ = generate_blobs_dataset(n_samples, n_features, n_classes, seed=0)\n",
    "\n",
    "    wmean_list = []\n",
    "    for k in k_range:\n",
    "        # model\n",
    "        kmeans = KMeansClustering(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        # compute Wk mean\n",
    "        w_k_mean = 1/k * compute_Wk(X, kmeans.labels_, kmeans.centroids)\n",
    "        wmean_list.append(w_k_mean)\n",
    "\n",
    "    plot_wkmean(X, k_range, wmean_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483cbfde",
   "metadata": {},
   "source": [
    "#### Gap Statistic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e24e8126",
   "metadata": {},
   "source": [
    "**ü§î Key Question**\n",
    "> *‚ÄúIs the clustering we observe better than what we would expect if the data had no real structure?‚Äù*\n",
    "\n",
    "Gap Statistic: a principled way to determine the number of clusters $k$ by comparing your clustering results with those you'd expect from **uniformly-distributed random data**.\n",
    "\n",
    "**STEPS**: \n",
    "1. Generate $n_{ref}$ synthetic reference dataset:  \n",
    "    - same **numbers of samples and dimensions** of the original data\n",
    "    - same **bounds** of the original data\n",
    "    - but values are sampled from a **uniform distribution** (no inherent clustering)\n",
    "    - we need $n_{ref}$ reference datasets to average and to not depend too much on the specific realization\n",
    "\n",
    "2. For each $k$ number of clusters:\n",
    "    1. Fit a clustering model on your data with $k$ clusters and compute within-cluster dispersions $W(k)$ as defined above. \n",
    "    2. For each of the synthetic datasets $j \\in \\{1, 2, ..., n_{ref}\\}$: \n",
    "        - Fit a clustering model on the ref data $j$ with $k$ clusters and compute within-cluster dispersions $W_j^{ref}(k)$.   \n",
    "    3. Compute the **gap statistic** as:  \n",
    "        $$\n",
    "        \\text{Gap}(k) = \\mathbb{E}_j[\\log W_j^{\\text{ref}}(k)] - \\log W(k)\n",
    "        $$  \n",
    "        - A higher gap means your clustering result is **better than random**.\n",
    "        - It quantifies the gain of using k clusters compared to a structureless dataset.\n",
    "3. Select the optimal number of clusters.  \n",
    "    Choose the **smallest** $k$ such that:  \n",
    "    $$\n",
    "    \\text{Gap}(k) \\geq \\text{Gap}(k + 1) - s_{k+1}\n",
    "    $$\n",
    "    where: \n",
    "    - $s_{k+1}$ is the **standard deviation** of the **log dispersions** from the reference datasets.\n",
    "    - This accounts for uncertainty in the random data comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b34951",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_reference_dataset(n_samples, n_features, lower_bound, upper_bound):\n",
    "    \"\"\"Generate 1 synthetic dataset with uniform distribution.\"\"\"\n",
    "    return np.random.uniform(lower_bound, upper_bound, size=(n_samples, n_features))\n",
    "\n",
    "\n",
    "def optimal_k_with_gap(X, klist, n_ref_datasets=10):\n",
    "    n_samples, n_features = X.shape\n",
    "    lower_bounds = [np.min(X[:, i]) for i in range(n_features)]\n",
    "    upper_bounds = [np.max(X[:, i]) for i in range(n_features)]\n",
    "\n",
    "    # Step 1: generate reference datasets\n",
    "    ref_datasets = [generate_reference_dataset(n_samples, n_features, lower_bounds, upper_bounds) for _ in range(n_ref_datasets)]\n",
    "    gap_list = []\n",
    "    stds_log_Wk_list = []\n",
    "    # Step 2: iterate over k \n",
    "    for k in klist: \n",
    "        # Step 2.1 Fit k-means on our data using k clusters \n",
    "        np.random.seed(4)  # this is a horrible thing to ensure 'proper' centroids... just for the sake of the example!\n",
    "        kmeans = KMeansClustering(n_clusters=k)\n",
    "        kmeans.fit(X)\n",
    "        Wk = compute_Wk(X, kmeans.labels_, kmeans.centroids)\n",
    "\n",
    "        # Step 2.2 Fit k-means on each reference dataset using k clusters\n",
    "        Wk_ref = []\n",
    "        for X_ref in ref_datasets: \n",
    "            kmeans_ref = KMeansClustering(n_clusters=k)\n",
    "            kmeans_ref.fit(X_ref)\n",
    "            Wk_ref.append(compute_Wk(X_ref, kmeans_ref.labels_, kmeans_ref.centroids))\n",
    "\n",
    "        # compute gap statistic \n",
    "        log_Wk_rf = np.log(Wk_ref)\n",
    "        gap = np.mean(log_Wk_rf) - np.log(Wk)\n",
    "        gap_list.append(gap)\n",
    "        print(f\"Gap statistic for k={k}: {gap}\")\n",
    "\n",
    "        # (corrected): standard deviation for each number k\n",
    "        s_k = np.std(log_Wk_rf)\n",
    "        stds_log_Wk_list.append(s_k)\n",
    "        \n",
    "\n",
    "    # Step 3: select optimal k (corrected)\n",
    "    # take the minimum k for which gap(k) > upper_bound   optimal_k = None\n",
    "    optimal_k = None\n",
    "    for i in range(len(gap_list)-1):\n",
    "        if gap_list[i] >= gap_list[i+1] - stds_log_Wk_list[i+1]:\n",
    "            optimal_k = klist[i]\n",
    "            break\n",
    "    if optimal_k == None: \n",
    "        optimal_k = klist[-1] # fallback. \n",
    "        print(f\"Warning: optimal k not found. Using k={optimal_k}\")\n",
    "        \n",
    "    return optimal_k, gap_list, stds_log_Wk_list\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af21307f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "klist = range(1, 7)\n",
    "optimal_k, gap_list, std_list = optimal_k_with_gap(Xblobs, klist, n_ref_datasets=10)\n",
    "print(f\"Optimal number of clusters: {optimal_k}\")\n",
    "\n",
    "kmeansopt = KMeansClustering(n_clusters=optimal_k).fit(Xblobs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52ad2fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the gap statistic plot \n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.plot(klist, gap_list, marker='o', c='tab:blue')\n",
    "plt.errorbar(klist, gap_list, yerr=std_list, fmt='o', capsize=10, c='tab:red')\n",
    "plt.title('Gap Statistic')\n",
    "plt.xlabel('Number of Clusters (k)')\n",
    "plt.ylabel('Gap Statistic')\n",
    "plt.xticks(klist)\n",
    "plt.legend()\n",
    "plt.grid()\n",
    "\n",
    "# plot the clusters\n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.scatter(Xblobs[:, 0], Xblobs[:, 1], c=kmeansopt.labels_)\n",
    "plt.scatter(kmeansopt.centroids[:, 0], kmeansopt.centroids[:, 1], c='red', marker='x', s=100, label='Centroids')\n",
    "plt.title(f\"Optimal number of clusters: {optimal_k}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34b4b881",
   "metadata": {},
   "source": [
    "### Agglomerative Hierarchical Clustering (bottom up)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0fb2d1",
   "metadata": {},
   "source": [
    "![hc.png](https://cdn.prod.website-files.com/633ec8b202a7496fb9c9dda9/6407a42fca237b08ec8a110e_hierarchical-agglomerative-clustering.png)\n",
    "\n",
    "1. Each point starts in its own clustering. \n",
    "2. At each step, we look for the two closest clusters to merge. \n",
    "3. We stop when 1 or n_clusters are reached\n",
    "\n",
    "**Distance among clusters**  \n",
    "There are many ways to define the distance between two clusters. Here we see the **Single Linkage** and **Average Linkage** hierarchical clustering. \n",
    "\n",
    "For each pair of clusters $C_i, C_j$, we define the min distance as: \n",
    "$$\n",
    "d(C_i, C_j) \\doteq \\min_{a\\in C_i,b\\in C_j} d(a,b)\n",
    "$$\n",
    "\n",
    "and the average distance as: \n",
    "\n",
    "$$\n",
    "d(C_i, C_j) \\doteq \\frac{1}{|C_i||C_j|}\\sum_{a\\in C_i}\\sum_{b\\in C_j}d(a,b)\n",
    "$$\n",
    "\n",
    "![imag2.png](https://miro.medium.com/v2/resize:fit:1400/1*Vv_0OE7Wwr3_CujLmKyKPA.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "451beaad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_cluster_distance(C1, C2): \n",
    "    \"\"\"\n",
    "    Compute the min distance between the two clusters using the Euclidean distance for points \n",
    "    \n",
    "    Args:\n",
    "        C1: np.ndarray - points in the first cluster, shape (n1, n_features)\n",
    "        C2: np.ndarray - points in the second cluster, shape (n2, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        float - the minimum linkage distance between the two clusters\n",
    "    \"\"\"\n",
    "    dist = np.inf\n",
    "    for i in range(C1.shape[0]): \n",
    "        for j in range(C2.shape[0]): \n",
    "            dist = min(dist, np.linalg.norm(C1[i] - C2[j]))\n",
    "    return dist\n",
    "   \n",
    "def average_cluster_distance(C1, C2): \n",
    "    \"\"\"\n",
    "    Compute the distance between the two clusters using Euclidean distance for points. \n",
    "    \n",
    "    Args:\n",
    "        C1: np.ndarray - points in the first cluster, shape (n1, n_features)\n",
    "        C2: np.ndarray - points in the second cluster, shape (n2, n_features)\n",
    "    \n",
    "    Returns:\n",
    "        float - the average linkage distance between the two clusters\n",
    "    \"\"\"\n",
    "    dist = 0 \n",
    "    for i in range(C1.shape[0]): \n",
    "        for j in range(C2.shape[0]): \n",
    "            dist += np.linalg.norm(C1[i] - C2[j])\n",
    "    dist = dist / (C1.shape[0] * C2.shape[0])\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3e5b76",
   "metadata": {},
   "source": [
    "#### How to memorize the agglomerative process? \n",
    "We use a matrix (cluster assignment matrix) <code>cmatrix</code>.\n",
    "\n",
    "- n rows: each row is an iteration step\n",
    "- n columns: each column is a sample index\n",
    "\n",
    "The element `(i,j)` represents the label/cluster assigned to the sample `j` in `X_train` at iteration `i`. \n",
    "\n",
    "**Example**:\n",
    "\n",
    "| Step | Sample a | Sample b | Sample c | Sample d | Sample e |                                                  |\n",
    "|------|----------|----------|----------|----------|----------|--------------------------------------------------------|\n",
    "| **0**    | 0        | 1        | 2        | 3        | 4        | Initial: each sample in its own cluster                |\n",
    "| **1**    | 0        | 1        | 2        | 1        | 3        | Samples b and d merged into cluster 1                  |\n",
    "| **2**    | 0        | 0        | 1        | 0        | 2        | Cluster 0 (sample a) and cluster 1 (samples b-d) are merged into cluster 0             |\n",
    "| **3**    | 0        | 0        | 1        | 0        | 1        | Clusters 1 and 2 are merged into cluster 1                |\n",
    "| **4**    | 0        | 0        | 0        | 0        | 0        | Final: all samples in one cluster                      |\n",
    "\n",
    "‚ö†Ô∏è **Note**: the i-th row of the matrix contains n_samples-i clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d054e56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AgglomerativeHierarchicalClustering: \n",
    "    \"\"\"Agglomerative Hierarchical Clustering using Average Linkage and Euclidean distance.\"\"\"\n",
    "    def __init__(self, n_clusters, linkage=\"average\", verbose=False): \n",
    "        assert linkage in [\"min\", \"average\"], \"Linkage must be 'min' or 'average'.\"\n",
    "        self.n_clusters = n_clusters\n",
    "        self.cmatrix = None\n",
    "        self.centroids = None\n",
    "        self.X_train = None\n",
    "        self.verbose = verbose          # if True, print the cluster assignment matrix and clusters list at each step of build_clusters\n",
    "        self.linkage = linkage\n",
    "\n",
    "    def fit(self, X): \n",
    "        \"\"\"\n",
    "        Fit the model to the data and save the cluster assignment matrix. \n",
    "        Returns the centroids of the clusters. \n",
    "\n",
    "        Args:\n",
    "            X: np.ndarray - data to cluster, shape (n_samples, n_features)\n",
    "            n_clusters: int - number of clusters to use for prediction\n",
    "            verbose: bool - if True, print the cluster assignment matrix and clusters list at each step of the fit\n",
    "        \"\"\"\n",
    "        self.X_train = X\n",
    "        self._build_clusters(self.n_clusters)\n",
    "        self.centroids = self.get_centroids(self.n_clusters)\n",
    "    \n",
    "    def _build_clusters(self, n_clusters): \n",
    "        \"\"\"\n",
    "        Perform the aggregations of clusters until n_clusters clusters are left \n",
    "        and save the cluster assignment matrix at each step.\n",
    "        \"\"\"\n",
    "        n_samples, n_features = self.X_train.shape\n",
    "\n",
    "        # to get n_clusters clusters we need to perform n_samples-n_clusters iterations \n",
    "        n_iters = n_samples - n_clusters\n",
    "\n",
    "        # initialize cmatrix and clusters list: each sample belongs to its own cluster at the beginning\n",
    "        self.cmatrix = np.zeros((n_iters + 1, n_samples), dtype=int)        # we want to save also the initial state (+1)\n",
    "        self.cmatrix[0, :] = np.arange(n_samples) \n",
    "        clusters = [[i] for i in range(n_samples)]   \n",
    "\n",
    "        if self.verbose: \n",
    "            print('Iteration 0')\n",
    "            print('Clusters ', clusters)\n",
    "            print('Cmatrix ')\n",
    "            print(self.cmatrix)\n",
    "            print('\\n')\n",
    "\n",
    "        # iterate until we have n_clusters clusters left (we need n_samples-n_clusters iterations) \n",
    "        for i in range(1, n_iters + 1): \n",
    "            # find the two closest clusters \n",
    "            best_pair = (None, None)\n",
    "            min_dist = np.inf\n",
    "            for j in range(len(clusters)):\n",
    "                for k in range(j+1, len(clusters)):     # we don't need to check the same pair twice\n",
    "                    Cj = self.X_train[clusters[j]]\n",
    "                    Ck = self.X_train[clusters[k]]\n",
    "                    if self.linkage == \"min\":\n",
    "                        dist = min_cluster_distance(Cj, Ck)\n",
    "                    elif self.linkage == \"average\":\n",
    "                        dist = average_cluster_distance(Cj, Ck)\n",
    "                    if dist < min_dist:\n",
    "                        best_pair = (j, k)\n",
    "                        min_dist = dist\n",
    "            \n",
    "            # merge the two closest clusters\n",
    "            j, k = best_pair\n",
    "            clusters[j] += clusters[k]                  # we are adding the sample indices in cluster k to cluster j\n",
    "            del clusters[k]                             # and we are removing the cluster/list k\n",
    "\n",
    "            # update the ith row of cmatrix \n",
    "            labels = np.zeros(n_samples, dtype=int)     # for each sample we have to assign the new cluster label\n",
    "            for cluster_idx, clust in enumerate(clusters):  \n",
    "                for sample_idx in clust:\n",
    "                    labels[sample_idx] = cluster_idx\n",
    "            self.cmatrix[i, :] = labels\n",
    "\n",
    "            if self.verbose: \n",
    "                print('Iteration', i)\n",
    "                print('Clusters ', clusters)\n",
    "                print('Cmatrix ')\n",
    "                print(self.cmatrix)\n",
    "                print('\\n')\n",
    "                        \n",
    "    def get_centroids(self, n_clusters): \n",
    "        \"\"\"\n",
    "        Compute the centroids of the clusters using the training set and the cmatrix. The centroid is\n",
    "        the barycenter of the cluster.\n",
    "\n",
    "        Notice: cmatrix[i,:] containts the cluster labels for n_samples-i clusters. \n",
    "        To get the row for n_clusters clusters, we need to take the row n_samples-n_clusters. \n",
    "\n",
    "        Returns:\n",
    "            np.ndarray - centroids of the clusters, shape (n_clusters, n_features)\n",
    "        \"\"\"\n",
    "        labels = self.cmatrix[self.X_train.shape[0]-n_clusters, :]\n",
    "        centroids = np.zeros((n_clusters, self.X_train.shape[1]))\n",
    "        for i in range(n_clusters): \n",
    "            # get the samples values in the cluster i\n",
    "            indices = (labels == i)\n",
    "            samples = self.X_train[indices]\n",
    "            # compute the barycenter\n",
    "            centroids[i,:] = np.mean(samples, axis=0)\n",
    "        return centroids\n",
    "    \n",
    "    def predict(self, X, n_clusters=None): \n",
    "        \"\"\"\n",
    "        Predict the labels for the samples in X by choosing the closest centroid. \n",
    "        If n_clusters is not specified, it will use the number of clusters used in fit. \n",
    "        Otherwise, it will recompute the centroids for the new number of clusters \n",
    "        from the cmatrix.\n",
    "        \n",
    "        Args:\n",
    "            X: np.ndarray - data to cluster, shape (n_samples, n_features)\n",
    "            n_clusters: int - number of clusters to use for prediction (optional)\n",
    "        \"\"\"\n",
    "\n",
    "        if self.cmatrix is None: \n",
    "            raise RuntimeError(\"You must fit the model before predicting.\")\n",
    "        \n",
    "        if n_clusters is None: \n",
    "            # use the number of clusters used in fit \n",
    "            distances = pairwise_distances(X, self.centroids)\n",
    "        else: \n",
    "            if n_clusters > self.cmatrix.shape[0]: \n",
    "                raise ValueError(\"n_clusters must be less than or equal to the number of clusters used in fit.\")\n",
    "            # recompute the centroids for the new number of clusters\n",
    "            centroids = self.get_centroids(n_clusters)\n",
    "            # compute the distances between each sample and each centroid\n",
    "            distances = pairwise_distances(X, centroids)\n",
    "        \n",
    "        # assign each sample to the nearest centroid\n",
    "        labels = np.argmin(distances, axis=1)\n",
    "        return labels.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f995524f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example\n",
    "X_train = np.array([[1.0, 2.0], [5.0, 8.0], [1.0, 1.0],[2.0, 3.0],[1.5, 1.8],[5.0, 6.5],])\n",
    "X_test = np.array([[1.0, 2.5], [5.5, 8.0], [6.0, 8.5]])\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter(X_train[:,0], X_train[:, 1], label=\"Train\", color=\"gray\")\n",
    "plt.scatter(X_test[:,0], X_test[:, 1], label=\"Test\", color=\"red\", marker=\"*\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8372a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit and predict with average or min linkage\n",
    "model = AgglomerativeHierarchicalClustering(n_clusters=2, verbose=True, linkage=\"average\")\n",
    "model.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4085e3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could use predict(X_train) to get the labels, but we don't need to! The **training** labels \n",
    "# are already in the cmatrix. \n",
    "y_pred_train = model.cmatrix[-1, :]\n",
    "\n",
    "# predict labels for new data X_test, instead, requires predict\n",
    "y_pred_test = model.predict(X_test)\n",
    "\n",
    "# plot the clusters\n",
    "plt.figure(figsize=(3,3))\n",
    "plt.scatter(X_train[:,0], X_train[:, 1], c=y_pred_train, cmap='coolwarm', label=\"Train\")\n",
    "plt.scatter(X_test[:,0], X_test[:, 1], c=y_pred_test, cmap='coolwarm', label=\"Test\", marker=\"*\")\n",
    "# plot centroids \n",
    "plt.scatter(model.centroids[:,0], model.centroids[:, 1], label=\"Centroids\", color=\"black\", marker=\"x\", s=100)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa55a537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing different hierarchical linkage methods on toy datasets \n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_linkage_comparison.html\n",
    "from sklearn import datasets \n",
    "\n",
    "# generate datasets \n",
    "n_samples = 100\n",
    "noisy_circles = datasets.make_circles(n_samples=n_samples, noise=0.05, random_state=170, factor=0.5)\n",
    "X_noisy_circles, y_noisy_circles = noisy_circles\n",
    "\n",
    "noisy_moons = datasets.make_moons(n_samples=n_samples, noise=0.05, random_state=170)\n",
    "X_noisy_moons, y_noisy_moons = noisy_moons\n",
    "\n",
    "blobs = datasets.make_blobs(n_samples=n_samples, random_state=170, cluster_std=1.5)\n",
    "X_blobs, y_blobs = blobs\n",
    "\n",
    "# run the clustering \n",
    "hc_avg_noisy_circles = AgglomerativeHierarchicalClustering(linkage=\"average\", n_clusters=2)\n",
    "hc_avg_noisy_circles.fit(X_noisy_circles) \n",
    "noisy_circles_labels = hc_avg_noisy_circles.cmatrix[-1, :]\n",
    "\n",
    "hc_min_noisy_circles = AgglomerativeHierarchicalClustering(linkage=\"min\", n_clusters=2)\n",
    "hc_min_noisy_circles.fit(X_noisy_circles)\n",
    "noisy_circles_labels_min = hc_min_noisy_circles.cmatrix[-1, :]\n",
    "\n",
    "hc_avg_noisy_moons = AgglomerativeHierarchicalClustering(linkage=\"average\", n_clusters=2)\n",
    "hc_avg_noisy_moons.fit(X_noisy_moons)\n",
    "noisy_moons_labels = hc_avg_noisy_moons.cmatrix[-1, :]\n",
    "\n",
    "hc_min_noisy_moons = AgglomerativeHierarchicalClustering(linkage=\"min\", n_clusters=2)\n",
    "hc_min_noisy_moons.fit(X_noisy_moons)\n",
    "noisy_moons_labels_min = hc_min_noisy_moons.cmatrix[-1, :]\n",
    "\n",
    "hc_avg_blobs = AgglomerativeHierarchicalClustering(linkage=\"average\", n_clusters=3)\n",
    "hc_avg_blobs.fit(X_blobs)\n",
    "blobs_labels = hc_avg_blobs.cmatrix[-1, :]\n",
    "\n",
    "hc_min_blobs = AgglomerativeHierarchicalClustering(linkage=\"min\", n_clusters=3)\n",
    "hc_min_blobs.fit(X_blobs)\n",
    "blobs_labels_min = hc_min_blobs.cmatrix[-1, :]\n",
    "\n",
    "# plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.title(\"Noisy Circles - Average Linkage\")\n",
    "plt.scatter(X_noisy_circles[:, 0], X_noisy_circles[:, 1], c=noisy_circles_labels, cmap='coolwarm')\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.title(\"Noisy Circles - Min Linkage\")\n",
    "plt.scatter(X_noisy_circles[:, 0], X_noisy_circles[:, 1], c=noisy_circles_labels_min, cmap='coolwarm')\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.title(\"Noisy Moons - Average Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=noisy_moons_labels, cmap='coolwarm')\n",
    "plt.axis('equal')\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.title(\"Noisy Moons - Min Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=noisy_moons_labels_min, cmap='coolwarm')\n",
    "plt.axis('equal')\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.title(\"Blobs - Average Linkage\")\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=blobs_labels, cmap='coolwarm')\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.title(\"Blobs - Min Linkage\")\n",
    "plt.scatter(X_blobs[:, 0], X_blobs[:, 1], c=blobs_labels_min, cmap='coolwarm')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09784e3c",
   "metadata": {},
   "source": [
    "Let's plot intermediate results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f69d6cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_cluster_min = hc_min_noisy_moons.cmatrix[96, :]\n",
    "inter_cluster_avg = hc_avg_noisy_moons.cmatrix[96, :]\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Noisy Moons - Average Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=inter_cluster_avg, cmap='plasma')\n",
    "plt.axis('equal')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Noisy Moons - Min Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=inter_cluster_min, cmap='plasma')\n",
    "plt.tight_layout()\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "inter_cluster_min = hc_min_noisy_moons.cmatrix[97, :]\n",
    "inter_cluster_avg = hc_avg_noisy_moons.cmatrix[97, :]\n",
    "\n",
    "# plot \n",
    "plt.figure(figsize=(7, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Noisy Moons - Average Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=inter_cluster_avg, cmap='plasma')\n",
    "plt.axis('equal')\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Noisy Moons - Min Linkage\")\n",
    "plt.scatter(X_noisy_moons[:, 0], X_noisy_moons[:, 1], c=inter_cluster_min, cmap='plasma')\n",
    "plt.tight_layout()\n",
    "plt.axis('equal')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923ad641",
   "metadata": {},
   "source": [
    "**Average Linkage on Iris**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7212e0c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"drive/MyDrive/AA24-25ML/iris.csv\")\n",
    "Xiris = data.iloc[:, :-1].to_numpy()\n",
    "yiris = data.iloc[:, -1].to_numpy()\n",
    "\n",
    "hciris = AgglomerativeHierarchicalClustering(linkage=\"average\", n_clusters=3)\n",
    "hciris.fit(Xiris)\n",
    "iris_labels = hciris.cmatrix[-1, :]\n",
    "\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.title(\"Iris Dataset - Average Linkage\")\n",
    "plt.scatter(Xiris[:, 0], Xiris[:, 1], c=iris_labels, cmap='coolwarm')\n",
    "plt.xlabel(\"Sepal Length\")\n",
    "plt.ylabel(\"Sepal Width\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "656a858b",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f172570",
   "metadata": {},
   "source": [
    "### Hotelling $T^2$ Test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7480a1f1",
   "metadata": {},
   "source": [
    "For multivariate outlier detection. \n",
    "\n",
    "**What is an outlier?**  \n",
    "A sample that is very different from the rest of the data. \n",
    "\n",
    "**A simple way to check if a point is far from our \"normal\" data**  \n",
    "1. Compute the mean of the train data\n",
    "2. Calculate the square error of that point with respect to the mean\n",
    "3. To account for the scale of the data, we can divide the square error by the variance of the data\n",
    "\n",
    "#### Univariate case\n",
    "Let's first consider the univariate case: we have data points along a single feature, or dimension.\n",
    "\n",
    "The \"distance\" of a point $x$ from the mean $\\mu$ is given by:\n",
    "$$\n",
    "\\frac{(x - \\mu)^2}{\\sigma^2}\n",
    "$$\n",
    "where $\\sigma^2$ is the variance of the data.\n",
    "\n",
    "This is basically what we will do with Hotelling's $T^2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cd2a3ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "X = np.random.normal(size=(100))\n",
    "\n",
    "mean = np.mean(X)\n",
    "var = np.var(X)\n",
    "\n",
    "outlier = np.array([5])\n",
    "\n",
    "distance = (outlier - mean)**2 / var\n",
    "print(f\"Distance of outlier from mean: {distance}\")\n",
    "\n",
    "# scatter plot\n",
    "plt.scatter(X, [0]*len(X), alpha=0.3, label='Normal Data')\n",
    "plt.scatter(outlier, [0], color='red', alpha=0.6, label='Outlier')\n",
    "plt.axvline(x=mean, color='green', linestyle='--', label='Estimated Mean')\n",
    "sns.kdeplot(X, bw_adjust=0.5, fill=True, alpha=0.2, label='KDE')\n",
    "plt.ylim(-0.2, 0.5)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041310fa",
   "metadata": {},
   "source": [
    "#### Multivariate case\n",
    "\n",
    "In the multivariate case, we have $p$ features, and we can represent our data as a $n \\times p$ matrix $X$.\n",
    "\n",
    "We can compute the mean of the data as:\n",
    "$$\n",
    "\\mu = \\frac{1}{n} \\sum_{i=1}^n x_i\n",
    "$$\n",
    "where $x_i$ is the $i$-th row of the data matrix $X$.  \n",
    "We also compute the covariance matrix of the data as:\n",
    "$$\n",
    "S = cov(X) \n",
    "$$\n",
    "Hotelling's $T^2$ statistic is given by:\n",
    "$$\n",
    "T^2 = (x - \\mu)^T S^{-1} (x - \\mu)\n",
    "$$\n",
    "where $x$ is the point we want to test.\n",
    "\n",
    "> Ok, now we have a sort of distance measure, how do we decide if a point is an outlier or not?\n",
    "\n",
    "We can use a $\\chi^2$ distribution to determine the threshold for the $T^2$ statistic.\n",
    "The $T^2$ statistic follows a $\\chi^2$ distribution with $p$ degrees of freedom.\n",
    "\n",
    "We can compute the critical value, also called UCL (Upper Control Limit) as:\n",
    "$$\n",
    "UCL = \\chi^2_{p, 1 - \\alpha}\n",
    "$$\n",
    "where $p$ is the number of features and $\\alpha$ is the significance level.  \n",
    "The critical value is the value above which we consider a point to be an outlier.\n",
    "\n",
    "**What does that mean?**  \n",
    "Basically we are saying that we want a (for examle) 95% confidence that the point we are testing is not an outlier.\n",
    "In that case we have $\\alpha = 0.05$\n",
    "\n",
    "**In simple terms, what are we doing?**\n",
    "As for all things, we like to simplify problems.\n",
    "> *In physics one may say: \"Let's assume that this cow can be approximated as a sphere\".*\n",
    "In data science, the equivalent is: \"Let's assume that the data is normally distributed\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a9ad36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('drive/MyDrive/AA24-25ML/cow.csv')\n",
    "X = df.to_numpy()\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], s=10)\n",
    "plt.title('Very serious dataset')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2d7c7",
   "metadata": {},
   "source": [
    "With Hotelling's $T^2$ test we are assuming that the data is normally distributed, we compute mean and covariance of the train dataset, and check if a new point is too far from the mean, with a confidence level we chose (like 95% or 99%)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d82db1f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# basically, we are approximating the cow with a Gaussian:\n",
    "mean = np.mean(X, axis=0)\n",
    "cov = np.cov(X, rowvar=False)\n",
    "\n",
    "# plot\n",
    "x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
    "y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
    "x, y = np.mgrid[x_min:x_max:0.1, y_min:y_max:0.1]\n",
    "pos = np.dstack((x, y))\n",
    "rv = multivariate_normal(mean, cov)\n",
    "plt.figure(figsize=(8, 5))\n",
    "contour = plt.contourf(x, y, rv.pdf(pos), levels=10, cmap=\"hot\")\n",
    "plt.colorbar(contour)\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.1, color=\"C0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d902cf0f",
   "metadata": {},
   "source": [
    "From the contour plot of the gaussian distribution, you can see that we have various confidence ellipsoids we could chose to use, in order to detect outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113900d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chi2\n",
    "\n",
    "\n",
    "class HotellingT2:\n",
    "    def __init__(self, alpha) -> None:\n",
    "        \"\"\"Initialize the mean and (inverse) covariance matrix.\"\"\"\n",
    "        self.mean = 0.0\n",
    "        self.inv_cov = 0.0\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def fit(self, X_train):\n",
    "        \"\"\"Fit multivariate Gaussian distribution to the data.\"\"\"\n",
    "        self.mean = np.mean(X_train, axis=0)  # save the mean\n",
    "        self.cov = np.cov(X_train, rowvar=False)\n",
    "        self.inv_cov = np.linalg.inv(self.cov)  # save the inverse covariance matrix\n",
    "\n",
    "    def compute_T_squared(self, X):\n",
    "        \"\"\"Hotelling's T¬≤ statistic: T¬≤ = (x - mean)·µÄ Œ£‚Åª¬π (x - mean)\"\"\"\n",
    "        t2_scores = []\n",
    "        for x in X:  # for each sample in X\n",
    "            t2 = (x - self.mean).T @ self.inv_cov @ (x - self.mean)\n",
    "            t2_scores.append(t2)\n",
    "        return np.array(t2_scores)\n",
    "\n",
    "    @staticmethod\n",
    "    def chi_squared_threshold(p, alpha):\n",
    "        \"\"\"Chi-squared threshold for multivariate normality.\"\"\"\n",
    "        return chi2.ppf(1 - alpha, df=p)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"Predict outliers based on Hotelling's T¬≤ scores and chi-squared threshold.\"\"\"\n",
    "        # Compute the threshold for the chi-squared distribution\n",
    "        n_features = X.shape[1]\n",
    "        threshold = self.chi_squared_threshold(n_features, self.alpha)\n",
    "        t2_scores = self.compute_T_squared(X)  # Compute T¬≤ scores\n",
    "        return t2_scores > threshold  # Identify outliers based on the threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ecae3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outlier prediction on the original data\n",
    "\n",
    "alpha = 0.05 # choose a significance level\n",
    "\n",
    "# Create an instance of the HotellingT2 class\n",
    "hotelling_model = HotellingT2(alpha=alpha)\n",
    "\n",
    "# Fit the model to the training data\n",
    "hotelling_model.fit(X)\n",
    "\n",
    "# Predict outliers in the train data\n",
    "y_pred = hotelling_model.predict(X)\n",
    "\n",
    "# Plot the T^2 scores as a contour plot\n",
    "x_min, x_max, y_min, y_max = -16, 7, -2, 13\n",
    "x, y = np.mgrid[x_min:x_max:0.5, y_min:y_max:0.5]\n",
    "grid_points = np.column_stack((x.flatten(), y.flatten()))\n",
    "t2_grid_scores = hotelling_model.compute_T_squared(grid_points)\n",
    "threshold = hotelling_model.chi_squared_threshold(X.shape[1], alpha)\n",
    "contour = plt.contour(x, y, t2_grid_scores.reshape(x.shape), levels=[threshold])\n",
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.1, c=y_pred.astype(int), cmap=\"coolwarm\")\n",
    "plt.xlim(x_min, x_max)\n",
    "plt.ylim(y_min, y_max)\n",
    "plt.title(f\"Hotelling's T¬≤ decision boundary {1-alpha:.0%} confidence\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ebaf1c3",
   "metadata": {},
   "source": [
    "### Isolation Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1241946",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>üèãÔ∏è‚Äç‚ôÄÔ∏è Exercise</strong></span>\n",
    "\n",
    "Implement an Isolation Tree using the same structure of Decision Tree and complete the class Isolation Forest. Assume that there are **no** categorical features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e575bf2",
   "metadata": {},
   "source": [
    "We use the same class structure as in decision trees (Lab5). However, we don't need <code>_find_best_split</code> nor <code>_find_best_threshold</code> because each split\n",
    "is now random. Moreover, the prediction function changes - we don't have target labels, we use the **corrected** depth. Finally, this is an unsupervised method: no labels are needed!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "754fae79",
   "metadata": {},
   "source": [
    "Corrected depth: if we stop growing the tree because of max depth, then the depth of the node must be corrected as `depth(node) = depth(node) + c(n_samples_in_node)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0939a5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def c(n): \n",
    "    \"\"\" \n",
    "    Average depth of a tree with n_samples samples or equivalently\n",
    "    average path length of an unsuccessful search in a BST.\n",
    "    \"\"\"\n",
    "    if n <= 1:\n",
    "        return 0 \n",
    "    elif n == 2:\n",
    "        return 1\n",
    "    else:\n",
    "        return 2 * (np.log(n-1) + np.euler_gamma - (n-1)/n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32378003",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node: \n",
    "    \"\"\"Save useful attributes\"\"\"\n",
    "    ...\n",
    "    ...\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2a39b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationTree: \n",
    "    def __init__(self, max_depth): \n",
    "        self.max_depth = max_depth\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_dataset(): \n",
    "        ...\n",
    "    \n",
    "    @staticmethod\n",
    "    def _child_id(): \n",
    "        ...\n",
    "\n",
    "    def _random_split(self, ): \n",
    "        \"\"\" \n",
    "        Randomly select a feature and threshold for splitting.\n",
    "         \n",
    "        Returns:\n",
    "            feature and threshold.\n",
    "        \"\"\"\n",
    "        ...\n",
    "         \n",
    "    def _build_tree(self, ): \n",
    "        \"\"\"Construct the tree recursively.\"\"\"\n",
    "\n",
    "        # -- LEAF CONDITIONS --\n",
    "        # 0. All feature values of all samples are the same (no split possible)\n",
    "        # 1. The node has 0 or 1 samples (no split possible)\n",
    "        # 2. Max depth is reached\n",
    "        ...\n",
    "\n",
    "        # -- Internal Node: split randomly and create children --\n",
    "  \n",
    "        # Recursive split\n",
    "        \n",
    "    \n",
    "    def fit(self, X):\n",
    "        \"\"\"\n",
    "        Fit the isolation tree to the data. \n",
    "            \n",
    "        Args:\n",
    "            X: np.ndarray of shape (n_subsamples, n_features), the data to fit the tree to.\n",
    "        \"\"\"\n",
    "        self.nodes = dict()\n",
    "        self._build_tree(...)\n",
    "    \n",
    "    def get_depth(self, X): \n",
    "        \"\"\"\n",
    "        Return the depth reached by each sample in X in the tree.\n",
    "        \"\"\"\n",
    "        ...\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7ea604",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IsolationForest: \n",
    "    def __init__(self, \n",
    "                 contamination:int=0.1, \n",
    "                 n_estimators:int=100, \n",
    "                 max_samples:int=256, \n",
    "                 max_depth:int=8): \n",
    "        \"\"\"\n",
    "        Args:\n",
    "            contamination: float - the proportion of outliers in the data.\n",
    "            n_estimators: int - the number of trees in the forest.\n",
    "            max_samples: int - the maximum number of samples in each tree.\n",
    "            max_depth: int - the maximum depth of each tree.\n",
    "        \"\"\"\n",
    "        # set hyperparameters\n",
    "        self.contamination = contamination\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_samples = max_samples\n",
    "        self.max_depth = max_depth\n",
    "\n",
    "        # initialize attributes\n",
    "        self.threshold = None\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X): \n",
    "        \"\"\"Fit the forest to the dataset\n",
    "        For each tree:\n",
    "        1. Subsample the data (with or without replacement)\n",
    "        2. (subsample the features)\n",
    "        3. Fit the tree to the subsampled data\n",
    "        \"\"\"\n",
    "        self.max_samples = min(self.max_samples, X.shape[0])\n",
    "    \n",
    "        for t in range(self.n_estimators): \n",
    "            # randomly subsample the data \n",
    "            sampled_indices = np.random.choice(X.shape[0], self.max_samples, replace=False)\n",
    "            X_subsample = X[sampled_indices, :]\n",
    "\n",
    "            # fit tree\n",
    "            tree = IsolationTree(self.max_depth)\n",
    "            tree.fit(X_subsample)\n",
    "            self.trees.append(tree)\n",
    "\n",
    "        # define the threshold from the contamination parameter. \n",
    "        # It is important to make this step here, as the threshold is defined considering\n",
    "        # the anomaly scores of the training set (not any other set). \n",
    "        training_scores = self.decision_function(X)\n",
    "        self._set_threshold(training_scores)\n",
    "        return self\n",
    "\n",
    "\n",
    "    def _set_threshold(self, training_scores):\n",
    "        \"\"\"\n",
    "        See the anomaly score threshold based on contamination.\n",
    "        \"\"\"\n",
    "        self.threshold = np.percentile(training_scores, 100 * (1 - self.contamination))\n",
    "\n",
    "\n",
    "    def decision_function(self, X):\n",
    "        \"\"\"\n",
    "        Compute anomaly scores for each sample x in X as: a(x)=2**(-E_t[h_t(x)]/c(n_t)) where: \n",
    "            E_t[h_t(x)] is the average depth of the sample x in the trees t in the forest\n",
    "            c(n_t) is the normalization factor, average depth of a tree fitted with n_t samples.\n",
    "        \"\"\"\n",
    "        # get the depth of each sample in each tree\n",
    "        depths = ...\n",
    "\n",
    "        ...\n",
    "\n",
    "        # compute the anomaly score\n",
    "        scores = ...\n",
    "\n",
    "        return scores\n",
    "    \n",
    "    def predict(self, X): \n",
    "        \"\"\"\n",
    "        Return 0 if the point is predicted as normal, 1 if it is predicted as an anomaly.\n",
    "        It is based on the anomaly score and the contamination parameter. \n",
    "        \"\"\"\n",
    "        if self.threshold is None:\n",
    "            raise ValueError(\"The model has not been fitted yet. Please call fit() before predict().\")\n",
    "        \n",
    "        scores = self.decision_function(X)\n",
    "        return (scores > self.threshold).astype(int)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be2c232a",
   "metadata": {},
   "source": [
    "#### Isolation Forest on California Housing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42854ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('drive/MyDrive/AA24-25ML/california_housing.csv')\n",
    "X = data[[\"MedInc\", \"AveRooms\"]].to_numpy()\n",
    "print(X.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train your model and get scores\n",
    "...\n",
    "scores = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "868b6de5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# take the top 10, 100 anomalies\n",
    "sorted_indices = np.argsort(scores)[::-1]\n",
    "top_10_idx = sorted_indices[:10]\n",
    "top_100_idx = sorted_indices[:100]\n",
    "normal_indices = sorted_indices[100:]\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X[normal_indices,0], X[normal_indices, 1], label=\"Normal\", color=\"gray\", alpha=0.1)\n",
    "plt.scatter(X[top_100_idx,0], X[top_100_idx, 1], label=\"Top 100 anomalies\", color=\"red\", marker=\"x\")\n",
    "plt.scatter(X[top_10_idx,0], X[top_10_idx, 1], label=\"Top 10 anomalies\", color=\"green\", marker=\"+\", s=90, linewidths=3 )\n",
    "plt.legend()\n",
    "plt.xlabel(\"MedInc\")\n",
    "plt.ylabel(\"AveRooms\")\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a782353",
   "metadata": {},
   "source": [
    "#### Evaluate Unsupervised Anomaly Detection\n",
    "One way is **synthetic injection**: we add known anomalies to the dataset (e.g. noise, out-of-distributions) points and we check if our model finds them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77d33a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the mean and the standard deviation of the features\n",
    "means = np.mean(X, axis=0)\n",
    "stds = np.std(X, axis=0)\n",
    "\n",
    "# add samples with values that deviate from the mean of the features (anomalies)\n",
    "divergent_rows = np.array([\n",
    "    [means[0] + 7 * stds[0], means[1] + 3 * stds[1]],\n",
    "    [means[0] + 3 * stds[0], means[1] + 10 * stds[1]],\n",
    "    [means[0] + 6 * stds[0], means[1] + 5 * stds[1]],\n",
    "    [means[0] + 4 * stds[0], means[1] + 12 * stds[1]],\n",
    "    [means[0] + 3 * stds[0], means[1] + 8 * stds[1]],\n",
    "    [means[0] + 4 * stds[0], means[1] + 3 * stds[1]],\n",
    "])\n",
    "X_augmented = np.vstack([X, divergent_rows])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd6fd9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and get scores on X_augmented \n",
    "...\n",
    "scores2 = ...\n",
    "\n",
    "# Anomaly ranking\n",
    "sorted_indices = np.argsort(scores2)[::-1]\n",
    "top_10_idx = sorted_indices[:10]\n",
    "top_100_idx = sorted_indices[:100]\n",
    "normal_indices = sorted_indices[100:]\n",
    "\n",
    "# Injected point indices\n",
    "new_points_idx = np.arange(X.shape[0], X_augmented.shape[0])\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.scatter(X_augmented[new_points_idx, 0], X_augmented[new_points_idx, 1],label=\"Injected Outliers\", color=\"blue\", marker=\"^\", linewidths=3)\n",
    "plt.scatter(X_augmented[normal_indices,0], X_augmented[normal_indices, 1], label=\"Normal\", color=\"gray\", alpha=0.1)\n",
    "plt.scatter(X_augmented[top_100_idx,0], X_augmented[top_100_idx, 1], label=\"Top 100 anomalies\", color=\"red\", marker=\"x\")\n",
    "plt.scatter(X_augmented[top_10_idx,0], X_augmented[top_10_idx, 1], label=\"Top 10 anomalies\", color=\"green\", marker=\"+\", s=90, linewidths=3 )\n",
    "plt.legend()\n",
    "plt.xlabel(\"MedInc\")\n",
    "plt.ylabel(\"AveRooms\")\n",
    "plt.grid(alpha=0.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43d62add",
   "metadata": {},
   "source": [
    "#### (Optional) Isolation forest bias"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef02e806",
   "metadata": {},
   "source": [
    "Bias may mean different things in different contexts. In this case we mean that in some cases, the IF may represent an anomaly score that is not consistent with the actual data.\n",
    "\n",
    "Let's see a synthetic example to illustrate this. We will generate a dataset with 2D Gaussian clusters and then we will apply the Isolation Forest algorithm to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639b977d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import generate_blobs_dataset\n",
    "X, y = generate_blobs_dataset(n_samples=1000, n_features=2, n_classes=2, seed=0, phi=np.pi/4)\n",
    "\n",
    "iforest = IsolationForest()\n",
    "iforest.fit(X) \n",
    "scores = iforest.decision_function(X)\n",
    "\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=scores, alpha=0.8)\n",
    "plt.colorbar(scatter, label='Anomaly Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4792dc56",
   "metadata": {},
   "source": [
    "The result is pretty good. But let's evaluate the anomaly score of many points in the space. We will generate a grid of points and evaluate the anomaly score of each point. Then we will plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92262a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_2d_meshpoints\n",
    "\n",
    "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, resolution=50)\n",
    "n_features = X.shape[1]\n",
    "\n",
    "predict_proba = lambda x: iforest.decision_function(x)[..., np.newaxis]\n",
    "\n",
    "# contourf of the decision function on the grid points\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.contourf(xx, yy, predict_proba(X_grid).reshape(xx.shape), levels=20, cmap=\"plasma\")\n",
    "plt.colorbar(label='Anomaly Score')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3c3a32c",
   "metadata": {},
   "source": [
    "As we can see, darker colors represent \"normal\" data points, and in fact we see two dark spots where the clusters are. \n",
    "\n",
    "But there are also darker areas in the space, relatively far from the clusters, in these areas we may have anomalous points that our model is not able to detect."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
