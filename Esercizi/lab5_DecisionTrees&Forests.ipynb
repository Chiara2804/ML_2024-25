{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41c59c34",
   "metadata": {},
   "source": [
    "# Decision Trees and Forests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be3d9f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from typing import Type\n",
    "from matplotlib import pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24290ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base class for the criterion\n",
    "\n",
    "class Criterion:\n",
    "    OPTIMIZATION = \"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def impurity(y) -> float: ...\n",
    "\n",
    "    @staticmethod\n",
    "    def split_impurity(y_left, y_right) -> float: ...\n",
    "\n",
    "    @staticmethod\n",
    "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement) -> bool: ...\n",
    "\n",
    "\n",
    "# specifical class for Gini\n",
    "\n",
    "class GiniCriterion(Criterion):\n",
    "    OPTIMIZATION = \"minimize\"\n",
    "\n",
    "    @staticmethod\n",
    "    def impurity(y):\n",
    "        class_count = np.unique(y, return_counts=True)[1]\n",
    "        probs = class_count / len(y)\n",
    "        return 1 - np.sum(probs ** 2)\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_impurity(y_left, y_right):\n",
    "        \"\"\"Weighted sum of the gini impurity of two subsets\"\"\"\n",
    "        nL = len(y_left)\n",
    "        nR = len(y_right)\n",
    "        n = nL + nR\n",
    "        g_left = GiniCriterion.impurity(y_left)\n",
    "        g_right = GiniCriterion.impurity(y_right)\n",
    "        return (nL * g_left + nR * g_right) / n\n",
    "    \n",
    "    @staticmethod\n",
    "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
    "        improvement = node_impurity - split_impurity\n",
    "        return improvement < min_improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ece7fb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# define an example of target\n",
    "y1 = np.array([\"cat\", \"cat\", \"dog\", \"dog\", \"dog\"])\n",
    "\n",
    "# access the moethods of the class using the class name and the dot operator\n",
    "impurity = GiniCriterion.impurity(y1)\n",
    "print(f\"Gini impurity of y1 is {impurity:.2f}\")\n",
    "\n",
    "y2 = np.array([\"cat\", \"cat\"])\n",
    "\n",
    "impurity = GiniCriterion.impurity(y2)\n",
    "print(f\"Gini impurity of y2 is {impurity:.2f}\")\n",
    "\n",
    "total_impurity = GiniCriterion.impurity(np.concatenate((y1, y2)))\n",
    "print(f\"Gini impurity of y = concat(y1, y2) is {total_impurity:.2f}\")\n",
    "\n",
    "split_impurity = GiniCriterion.split_impurity(y1, y2)\n",
    "print(f\"Gini impurity of the split (y1, y2) is: {split_impurity:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e76747",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Node:\n",
    "    impurity: Optional[float]         # None if leaf\n",
    "    split_impurity: Optional[float]   # None if leaf\n",
    "    feature: Optional[int]            # None if leaf\n",
    "    threshold: Optional[float]        # None if leaf\n",
    "    prediction: Optional[float]       # None if not leaf\n",
    "    n_samples: int\n",
    "\n",
    "# usage\n",
    "node = Node(impurity=0.5, split_impurity = 0.3, feature=1, threshold=0.5, prediction=None, n_samples=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24252661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# la classe più generale possibile\n",
    "def node_prediction_classification(y):\n",
    "    \"\"\"ritorna la classe più comune nel nodo\"\"\"\n",
    "    classes, counts = np.unique(y, return_counts = True)\n",
    "    return classes[np.argmax(counts)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3070bcbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tree classifier model\n",
    "\n",
    "class DecisionTree:\n",
    "    def __init__(self, criterion: Type[Criterion], node_pref_fnc, max_depth, min_impurity_improvement):\n",
    "        self.criterion = criterion\n",
    "        self.node_pref_fnc = node_pref_fnc\n",
    "        self.max_depth = max_depth\n",
    "        self.min_impurity_improvement = min_impurity_improvement\n",
    "\n",
    "        self.categorical_feat_idxs = []\n",
    "        self.nodes = dict()\n",
    "\n",
    "    @staticmethod\n",
    "    def _split_dataset(X, feature, threshold, is_categorical):\n",
    "        if is_categorical:\n",
    "            # categorical feature\n",
    "            left_mask = X[:, feature] == threshold\n",
    "            right_mask = X[:, feature] != threshold\n",
    "        else:\n",
    "            # numerical feature\n",
    "            left_mask = X[:, feature] <= threshold\n",
    "            right_mask = X[:, feature] > threshold\n",
    "        return left_mask, right_mask\n",
    "    \n",
    "    @staticmethod\n",
    "    def _child_id(node_id, is_left):\n",
    "        if is_left:\n",
    "            return 2 * node_id + 1\n",
    "        else:\n",
    "            return 2 * node_id + 2\n",
    "        \n",
    "    def _find_best_split(self, X, y):\n",
    "        \"\"\"trovo la miglior feature e la migliro soglia per dividere i dati\"\"\"\n",
    "        n_features = X.shape[1]\n",
    "\n",
    "        best_feature:int = -1\n",
    "        best_threshold:float = 0.0\n",
    "        best_impurity = np.inf if self.criterion.OPTIMIZATION == \"minimize\" else -np.inf\n",
    "\n",
    "        for f in range(n_features):\n",
    "            is_categorical = f in self.categorical_feat_idxs\n",
    "            split_impurity, threshold = self._find_best_threshold(X[:, f], y, is_categorical)\n",
    "\n",
    "            if self.criterion.OPTIMIZATION == \"minimize\":\n",
    "                is_better = split_impurity < best_impurity\n",
    "            else: \n",
    "                is_better = split_impurity > best_impurity\n",
    "\n",
    "            if is_better:\n",
    "                best_feature = f\n",
    "                best_threshold = threshold\n",
    "                best_impurity = split_impurity\n",
    "\n",
    "        return best_impurity, best_feature, best_threshold\n",
    "    \n",
    "    def _find_best_threshold(self, X_f, y, is_categorical):\n",
    "        \"\"\"Try all possible thresholds/categories and return the best one\"\"\"\n",
    "        unique_values = np.unique(X_f)\n",
    "        best_impurity = np.inf if self.criterion.OPTIMIZATION == \"minimize\" else -np.inf\n",
    "\n",
    "        if len(unique_values) < 2:\n",
    "            return best_impurity, unique_values[0]  # no split possible\n",
    "\n",
    "        for value in unique_values:\n",
    "            # divide according to threshold/category\n",
    "            left_mask, right_mask = self._split_dataset(X_f, ..., value, is_categorical)\n",
    "            y_left, y_right = y[left_mask], y[right_mask]\n",
    "\n",
    "            # skip if the value does not split the data\n",
    "            if len(y_left) == 0 or len(y_right) == 0:\n",
    "                continue\n",
    "\n",
    "            # evaluate impurity of the split\n",
    "            split_impurity = self.criterion.split_impurity(y_left, y_right)\n",
    "\n",
    "            if self.criterion.OPTIMIZATION == \"minimize\":\n",
    "                is_better = split_impurity < best_impurity  # minimize\n",
    "            else:\n",
    "                is_better = split_impurity > best_impurity  # maximize\n",
    "\n",
    "            if is_better:\n",
    "                best_impurity = split_impurity\n",
    "                best_threshold = value\n",
    "\n",
    "        return best_impurity, best_threshold\n",
    "    \n",
    "    def _build_tree(self, X, y, depth, node_id):\n",
    "        n_samples_node = len(y)\n",
    "\n",
    "        # -- LEAF CONDITIONS --- posso splittare ancora visto le cose che mi sono arrivate?\n",
    "        # 0. All the feature values of all the samples are the same (no useful split possible)\n",
    "        # 1. The node has 0 or 1 samples (no split possible)\n",
    "        # 2. Max depth is reached\n",
    "        # 3. The impurity improvement < min_impurity_improvement (implemented after)\n",
    "\n",
    "        # leaf condition 0\n",
    "        X_all_same = all(len(np.unique(X[:, feat])) == 1 for feat in range(X.shape[1]))\n",
    "\n",
    "        if len(y) <= 1 or depth >= self.max_depth or X_all_same:\n",
    "            prediction = self.node_pred_fnc(y)\n",
    "            self.node[node_id] = Node(None, None, None, None, prediction, n_samples_node)\n",
    "            return\n",
    "        \n",
    "        # -- Internal Node: find BEST SPLIT and create children --\n",
    "        best_split_impurity, best_feature, best_threshold = self._find_best_split(X, y)\n",
    "\n",
    "        # (leaf condition 3)\n",
    "        current_node_impurity = self.criterion.impurity(y)\n",
    "        if self.criterion.is_tolerance_reached(current_node_impurity, best_split_impurity, self.min_impurity_improvement):\n",
    "            prediction = self.node_pred_fnc(y)\n",
    "            self.nodes[node_id] = Node(None, None, None, None, prediction, n_samples_node)    # leaf node\n",
    "            return\n",
    "\n",
    "        # -- Save node -- Se arriviamo qua non siamo in una foglia, salvo il nodo\n",
    "        self.nodes[node_id] = Node(current_node_impurity, best_split_impurity, best_feature, best_threshold, None, n_samples_node)\n",
    "\n",
    "        # -- Create children --\n",
    "        is_feat_cat = best_feature in self.categorical_feat_idxs\n",
    "        left_mask, right_mask = self._split_dataset(X, best_feature, best_threshold, is_feat_cat)\n",
    "\n",
    "        left_id = self._child_id(node_id, True)\n",
    "        right_id = self._child_id(node_id, False)\n",
    "\n",
    "        X_left, y_left = X[left_mask], y[left_mask]\n",
    "        self._build_tree(X_left, y_left, depth + 1, left_id) #ricorsiva\n",
    "\n",
    "        X_right, y_right = X[right_mask], y[right_mask]\n",
    "        self._build_tree(X_right, y_right, depth + 1, right_id) #ricorsiva\n",
    "\n",
    "    def fit(self, X, y, categorical_feat_idxs = None):\n",
    "        \"\"\"\n",
    "        Fit the decision tree to the data. The tree is built recursively by splitting the data at each node\n",
    "        based on the impurity criterion.\n",
    "        Args:\n",
    "            X: input matrix of shape (n_samples, n_features)\n",
    "            y: true target/labels of shape (n_samples,)\n",
    "            categorical_feat_idxs: list of categorical feature indices\n",
    "        \"\"\"\n",
    "        self.categorical_feat_idxs = categorical_feat_idxs if categorical_feat_idxs is not None else []\n",
    "\n",
    "        self.nodes = dict()     # Reset the nodes\n",
    "        self._build_tree(X, y, 0, 0)\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Predict the target variable for the given input data X.\n",
    "        Keep the same column order as in the training data.\n",
    "\n",
    "        Args:\n",
    "            X: input matrix of shape (n_samples, n_features)\n",
    "        \"\"\"\n",
    "        predictions = []\n",
    "        for x in X:\n",
    "            node_id = 0\n",
    "            node : Node = self.nodes[0]\n",
    "            while True:\n",
    "                if node.prediction is not None:\n",
    "                    predictions.append(node.prediction) # leaf node\n",
    "                    break\n",
    "                # else we need to go down the tree\n",
    "                if node.feature in self.categorical_feat_idxs:\n",
    "                    # categorical features\n",
    "                    is_left = x[node.feature] == node.threshold\n",
    "                else:\n",
    "                    # continuous features\n",
    "                    is_left = x[node.feature] <= node.threshold\n",
    "                node_id = self._child_id(node_id, is_left)\n",
    "                node = self.nodes[node_id]\n",
    "        return np.array(predictions)\n",
    "\n",
    "\n",
    "    def print_tree(self, node_id=0, prefx=\"\", is_left=True, feat_names=None):\n",
    "        \"\"\"\n",
    "        Prints the binary tree in a hierarchical format.\n",
    "        \"\"\"\n",
    "        if node_id not in self.nodes:\n",
    "            return\n",
    "\n",
    "        node = self.nodes[node_id]\n",
    "        conn = \"├── \" if is_left else \"└── \"\n",
    "\n",
    "        if node.feature is None:\n",
    "            print(f\"{prefx}{conn}{node_id}-Pred: {node.prediction}\")\n",
    "            return\n",
    "        else:\n",
    "            if feat_names is None:\n",
    "                f = f\"X[{node.feature}]\"\n",
    "            else:\n",
    "                f = feat_names[node.feature]\n",
    "            print(f\"{prefx}{conn}{node_id}-{f} <= {node.threshold} | Impurity:{node.impurity:.3f}\")\n",
    "\n",
    "        prefx += \"│   \" if is_left else \"    \"\n",
    "        l_id = self._child_id(node_id, is_left=True)\n",
    "        r_id = self._child_id(node_id, is_left=False)\n",
    "\n",
    "        self.print_tree(l_id, prefx, is_left=True, feat_names=feat_names)\n",
    "        self.print_tree(r_id, prefx, is_left=False, feat_names=feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a7aa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicazione\n",
    "data = pd.read_csv('iris.csv')\n",
    "display(data.head())\n",
    "\n",
    "feature_names = data.columns[:-1]\n",
    "target_col = 'species'\n",
    "\n",
    "X = data.drop(target_col, axis=1).to_numpy()\n",
    "y = data[target_col].to_numpy()\n",
    "\n",
    "print(\"X.shape_\", X.shape, \"y.shape_\", y.shape)\n",
    "\n",
    "# define model using the Gini impurity criterion\n",
    "tree = DecisionTree(criterion=GiniCriterion,\n",
    "                    node_pred_fnc = node_prediction_classification,\n",
    "                    max_depth = 3,\n",
    "                    min_impurity_improvement=0.0)\n",
    "\n",
    "est_gini, best_feature, best_threshold = tree._find_best_split(X, y)\n",
    "print(f\"Best gini impurity for the first split: {best_gini:.2e}\")\n",
    "print(f\"Best feature: '{feature_names[best_feature]}'\")\n",
    "print(f\"Best threshold: {best_threshold:.2f}\")\n",
    "\n",
    "plt.scatter(X[:, best_feature], y)\n",
    "plt.axvline(best_threshold, color=\"red\", linestyle=\"--\")\n",
    "plt.xlabel(feature_names[best_feature])\n",
    "plt.ylabel(\"species\")\n",
    "plt.title(f\"Best split for '{feature_names[best_feature]}' feature\")\n",
    "\n",
    "tree.fit(X, y, categorical_feat_idxs=None) # fit the model\n",
    "\n",
    "tree.print_tree()\n",
    "\n",
    "# training prediction and accuracy\n",
    "y_pred = tree.predict(X)\n",
    "print(\"Training accuracy:\", accuracy(y, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e5e61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import plot_decision_boundary_2d, create_2d_meshpoints\n",
    "\n",
    "def create_proba(dtree):\n",
    "    def tree_proba(X):\n",
    "        y_pred = dtree.predict(X)\n",
    "        y_pred = np.unique(y_pred, return_inverse=True)[1]\n",
    "        proba = np.zeros((X.shape[0], len(np.unique(y))))\n",
    "        for i, pred in enumerate(y_pred):\n",
    "            proba[i, pred] = 1\n",
    "        return proba\n",
    "    return tree_proba\n",
    "\n",
    "max_depth = 4\n",
    "print(\"Max depth:\", max_depth)\n",
    "tree2 = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
    "tree2.fit(X, y, categorical_feat_idxs=None)\n",
    "y_pred = tree2.predict(X)\n",
    "print(\"Train Accuracy:\", accuracy(y, y_pred))\n",
    "\n",
    "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, 200)\n",
    "\n",
    "probability_func = create_proba(tree2)\n",
    "\n",
    "n_features = X.shape[1]\n",
    "plot_decision_boundary_2d(\n",
    "    X_grid,\n",
    "    np.unique(y, return_inverse=True)[1],\n",
    "    probability_func,\n",
    "    xx,\n",
    "    yy,\n",
    "    X_2d,\n",
    "    n_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788c4e5",
   "metadata": {},
   "source": [
    "## Cost Complexity Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11691ec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_complexity(X, y, node_id, tree, ccps, total_N):\n",
    "    node = tree.nodes[node_id]\n",
    "\n",
    "    node_impurity = tree.criterion.impurity(y) * len(y) / total_N\n",
    "\n",
    "    if node.feature is None: # leaf node\n",
    "        return node_impurity, 1 # impurity, number of leaf\n",
    "    \n",
    "    # parent node: splitta il dataset e processa i figli\n",
    "    left_id = tree._child_id(node_id, is_left=True)\n",
    "    right_id = tree._child_id(node_id, is_left=False)\n",
    "    left_mask, right_mask = tree._split_dataset(\n",
    "        X, node.feature, node.threshold, node.feature in tree.categorical_feat_idxs\n",
    "    )\n",
    "\n",
    "    left_impurity, left_leaves = compute_cost_complexity(\n",
    "        X[left_mask], y[left_mask], left_id, tree, ccps, total_N\n",
    "    )\n",
    "    right_impurity, right_leaves = compute_cost_complexity(\n",
    "        X[right_mask], y[right_mask], right_id, tree, ccps, total_N\n",
    "    )\n",
    "\n",
    "    total_leaves = left_leaves + right_leaves\n",
    "\n",
    "    impurity_sum = left_impurity + right_impurity\n",
    "    ccps[node_id] = (node_impurity - impurity_sum) / (total_leaves - 1)\n",
    "\n",
    "    return impurity_sum, total_leaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324e2d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# applicazione sempre con irisi di questa parte\n",
    "subtrees_impurities = {}\n",
    "compute_cost_complexity(X, y, 0, tree, subtrees_impurities, total_N=len(y))\n",
    "\n",
    "# keep only the internal nodes\n",
    "subtrees_impurities = {node_id: imp for node_id, imp in subtrees_impurities.items() if tree.nodes[node_id].feature is not None}\n",
    "\n",
    "\n",
    "for node_id, imp in subtrees_impurities.items():  # Reusing the same impurity dictionary\n",
    "    print(f\"Subtree {node_id} imp: {imp:.4f}\")\n",
    "\n",
    "print(\"-\" * 50)\n",
    "tree.print_tree()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02070b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# given the node_id, we can remove the node and all its children\n",
    "# to get the prediction, we still need the training dataset\n",
    "def prune_tree(X, y, node_id, tree, ids_to_prune):\n",
    "    node = tree.nodes[node_id]\n",
    "\n",
    "    if node_id in ids_to_prune:\n",
    "        tree.nodes[node_id] = Node(None, None, None, None, tree.node_pred_fnc(y), node.n_samples)\n",
    "        return\n",
    "    \n",
    "    if node.feature is None:\n",
    "        return\n",
    "    \n",
    "    left_mask, right_mask = tree._split_dataset(X, node.feature, node.threshold, node.feature in tree.categorical_feats_idxs)\n",
    "    left_id = tree._child_id(node_id, is_left = True)\n",
    "    right_id = tree._child_id(node_id, is_left=False)\n",
    "    prune_tree(X[left_mask], y[left_mask], left_id, tree, ids_to_prune)\n",
    "    prune_tree(X[right_mask], y[right_mask], right_id, tree, ids_to_prune)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed4aefc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# come cambia l accuracy variando il numero di sotto alberi pruned\n",
    "tree = DecisionTree(\n",
    "    criterion=GiniCriterion,\n",
    "    node_pred_fnc=node_prediction_classification,\n",
    "    max_depth=3,\n",
    "    min_impurity_improvement=0.0,\n",
    ")\n",
    "\n",
    "# order the ccps by increasing ccp\n",
    "sorted_ccps = sorted(subtrees_impurities.items(), key=lambda x: x[1])\n",
    "accuracies = []\n",
    "for i in range(len(sorted_ccps)):\n",
    "    nodes_to_prune = [node_id for node_id, _ in sorted_ccps[:i + 1]]\n",
    "    print(f\"Pruning nodes: {nodes_to_prune}\")\n",
    "    # start from the original tree\n",
    "    tree.fit(X, y, categorical_feat_idxs=None)\n",
    "    # prune the tree\n",
    "    prune_tree(X, y, 0, tree, nodes_to_prune)\n",
    "    # get the predictions\n",
    "    y_pred = tree.predict(X)\n",
    "    # compute the accuracy\n",
    "    acc = accuracy(y, y_pred)\n",
    "    print(f\"Accuracy after pruning nodes {nodes_to_prune}: {acc:.4f}\")\n",
    "    # plot the tree\n",
    "    tree.print_tree()\n",
    "    print(\"-\" * 50)\n",
    "    accuracies.append((acc, len(nodes_to_prune)))\n",
    "\n",
    "accuracies = np.array(accuracies)\n",
    "plt.plot(accuracies[:, 1], accuracies[:, 0], marker=\"o\")\n",
    "plt.xlabel(\"Number of pruned nodes\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Number of pruned nodes\")\n",
    "plt.grid()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6da75783",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4714e27e",
   "metadata": {},
   "source": [
    "## Bootstrap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8564ce6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bootstrap_sample(X, y):\n",
    "    # select n samples from X with replacement\n",
    "    # first parameter: values among which we are selecting, second parameter: number of values that we are\n",
    "    # selecting, third parameter: with replacement\n",
    "    indices = np.random.choice(len(X), size=len(X), replace=True)\n",
    "    # filter X and y based on indices\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "def fit_bagging(X, y, T, max_depth):\n",
    "    \"\"\"Fit T trees using bagging\"\"\"\n",
    "    # forest\n",
    "    trees = []\n",
    "    for j in range(T):\n",
    "        # alternative dataset\n",
    "        X_sample, y_sample = bootstrap_sample(X, y)\n",
    "        # initialize the tree\n",
    "        tree = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
    "        # fit the tree\n",
    "        tree.fit(X_sample, y_sample, categorical_feat_idxs=None)\n",
    "        # add the tree to the forest\n",
    "        trees.append(tree)\n",
    "    return trees\n",
    "\n",
    "def predict_bagging(X, trees):\n",
    "    # matrix where each row corresponds to a different tree (each column correspond to a different sample)\n",
    "    predictions = np.array([tree.predict(X) for tree in trees]) # Shape (num_trees, num_samples)\n",
    "    y_pred = []\n",
    "    confidences = []\n",
    "    # iterate through the columns\n",
    "    for i in range(predictions.shape[1]):\n",
    "        # for column (sample) i, consider all rows (all predictions of all trees)\n",
    "        sample_predictions = predictions[:, i]\n",
    "        # get the number of times each class appears in the column\n",
    "        # (array containing the number of times class 0 appears, the number of times class 1 appears ...)\n",
    "        class_counts = np.bincount(sample_predictions)\n",
    "        # get the class that appears most frequently\n",
    "        majority_class = np.argmax(class_counts)\n",
    "        # we compute the confidence interval as the proportion of times that the majority class gets predicted\n",
    "        confidence = class_counts[majority_class] / len(trees)\n",
    "        # add the prediction to the prediction list\n",
    "        y_pred.append(majority_class)\n",
    "        # add the confidence to the confidence list\n",
    "        confidences.append(confidence)\n",
    "    return np.array(y_pred), np.array(confidences)\n",
    "\n",
    "# number of trees in the forest\n",
    "T = 4\n",
    "# depth of each tree\n",
    "max_depth = 2\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# train the forest\n",
    "trees = fit_bagging(X_train, y_train, T, max_depth)\n",
    "# get the vector of predictions\n",
    "y_pred, confidences = predict_bagging(X_test, trees)\n",
    "acc = accuracy(y_test, y_pred)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Predictions: \", y_pred)\n",
    "print(\"True labels: \", y_test)\n",
    "print(\"Confidence of the predictions: \", confidences)\n",
    "\n",
    "for i in range(len(y_pred)):\n",
    "    if y_pred[i] != y_test[i]: # predizioni sbagliate\n",
    "        print(\"Index: \", i)\n",
    "        print(\"Prediction: \", y_pred[i])\n",
    "        print(\"True label: \", y_test[i])\n",
    "        print(\"Confidence of the prediction: \", confidences[i])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4426f085",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange  # to print a progress bar\n",
    "\n",
    "np.random.seed(0)\n",
    "\n",
    "# we make the same computation several times, reporting the mean and variance of the accuracy metric\n",
    "trials = 5\n",
    "# number of trees in the forest\n",
    "T_sizes = [1, 4, 8, 16]\n",
    "# depth of each tree\n",
    "max_depth = 4\n",
    "\n",
    "trial_accuracies = []\n",
    "for trial in trange(trials, desc=\"Trials\", leave=True): # while this loop is computed, print a progress bar\n",
    "    accuracies = []\n",
    "    for T in T_sizes:\n",
    "        # train the forest\n",
    "        trees = fit_bagging(X_train, y_train, T, max_depth)\n",
    "        # get the vector of predictions\n",
    "        y_pred, confidences = predict_bagging(X_test, trees)\n",
    "        # compute the accuracy\n",
    "        acc = accuracy(y_test, y_pred)\n",
    "        accuracies.append(acc)\n",
    "    trial_accuracies.append(accuracies)\n",
    "\n",
    "# Calculate mean and std of accuracies\n",
    "mean_accuracies = np.mean(trial_accuracies, axis=0)\n",
    "std_accuracies = np.std(trial_accuracies, axis=0)\n",
    "\n",
    "# Plot accuracies over T_sizes with error bars\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.errorbar(\n",
    "    T_sizes, mean_accuracies, yerr=std_accuracies, fmt=\"o-\", color=\"b\", label=\"Accuracy\"\n",
    ")\n",
    "plt.xlabel(\"Number of Trees (T)\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Bagging: Accuracy vs Number of Trees\")\n",
    "plt.grid(True)\n",
    "plt.xticks(T_sizes)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c03c75d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
