{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4uAR2DEOcqzq"
      },
      "source": [
        "**ML COURSE 2024-2025**\n",
        "# LAB3: LASSO REGRESSION\n",
        "\n",
        "In this notebook you will implement gradient descent with LASSO regularization, and Nested K-fold Cross Validation.\n",
        "\n",
        "#### Summary\n",
        "- Part 1: Implementation and visualization of gradient descent algorithm.\n",
        "- Part 2: Recap on LASSO, LASSO regularization using gradient descent.\n",
        "- Part 3: Hyperparameter tuning, Nested K-Fold Cross Validation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ro1OtQSucqzv"
      },
      "source": [
        "## Gradient Descent\n",
        "\n",
        "<!--Since the Lasso regression does not have a closed form solution, we need to use an optimization algorithm to find the coefficients that minimize the loss function.-->\n",
        "\n",
        "The gradient descent algorithm is an iterative optimization algorithm that updates the coefficients in the opposite direction of the gradient of the loss function.\n",
        "\n",
        "The update rule for the coefficients is given by:\n",
        "\n",
        "$$\n",
        "\\beta_{new} = \\beta - \\alpha \\; \\nabla_\\beta J(\\beta)\n",
        "$$\n",
        "\n",
        "Where\n",
        "- $\\eta$ is the learning rate\n",
        "- $\\nabla_\\beta J(\\beta)$ is the gradient of the loss function\n",
        "- $\\beta_{new}$ is the updated coefficients\n",
        "\n",
        "**That may be a bit to digest, but don't worry! Let's get an intuition step by step.**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Moctpw_3cqzw"
      },
      "source": [
        "#### First of all, what is the gradient?\n",
        "\n",
        "Maybe this sentence is familiar:\n",
        "\n",
        "> The gradient is a vector that points in the direction of the steepest increase of a scalar function.\n",
        "\n",
        "Note that the the gradient is a vector that lives in the input space! In our case, the space of the coefficients."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AMCO-UniPD/ML_lab_DEI_public.git"
      ],
      "metadata": {
        "id": "7oMTtu_0csPN",
        "outputId": "a4b22f94-79b9-4ab4-9b9f-262c9caea0f7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'ML_lab_DEI_public'...\n",
            "remote: Enumerating objects: 136, done.\u001b[K\n",
            "remote: Counting objects: 100% (136/136), done.\u001b[K\n",
            "remote: Compressing objects: 100% (99/99), done.\u001b[K\n",
            "remote: Total 136 (delta 52), reused 98 (delta 23), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (136/136), 22.72 MiB | 14.48 MiB/s, done.\n",
            "Resolving deltas: 100% (52/52), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqbopzq6cqzx"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RIKVhgeJcqzy"
      },
      "outputs": [],
      "source": [
        "# This is our function, it could be anything, but in this case it's a simple quadratic function\n",
        "def f(x):\n",
        "    return (x**2) * 0.1\n",
        "\n",
        "\n",
        "# This is the derivative of the function, good old calculus remember?\n",
        "def df(x):\n",
        "    return (2 * x) * 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddG8GtzZcqzz"
      },
      "source": [
        "We can visualize the derivative as the slope of a function at a given point."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ8R9YH-cqzz"
      },
      "outputs": [],
      "source": [
        "from utils import plot_derivative\n",
        "\n",
        "plot_derivative(f, df, t=\"Quadratic function\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BR9d-BSGcqzz"
      },
      "outputs": [],
      "source": [
        "# we can also try with a different function\n",
        "plot_derivative(np.sin, np.cos, x=np.linspace(-2*np.pi, 2*np.pi, 100), t=\"Sine function\", n=7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tlirWjr7cqz0"
      },
      "source": [
        "If the gradient lives in the input space, it means that in this $\\mathbb{R}^1 \\rightarrow \\mathbb{R}^1$ case, the gradient is a 1D vector along the x-axis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lWJkIWb1cqz0"
      },
      "outputs": [],
      "source": [
        "from utils import plot_gradient_1d\n",
        "\n",
        "plot_gradient_1d(f, df, x=np.linspace(-10, 10, 100), n=10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vL-D6Q0Ucqz0"
      },
      "source": [
        "This plot shows arrows that start at a certain point along the x, and their length depends on the slope of the function at that point.\n",
        "\n",
        "Unlike other visualizations, this one shows the gradient as a vector in the input space, meaning along the x axis.\n",
        "\n",
        "Other visualizations may show the gradient as a vector that follows the function curve, but that may be misleading when first learning about gradients."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TDfjLuHIcqz1"
      },
      "source": [
        "Now we are redy to increase the dimensions! Let's see a function that has 2 input variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q7S9575-cqz1"
      },
      "outputs": [],
      "source": [
        "def f(x1, x2):\n",
        "    return (x1**2 + x2**2) * 0.1\n",
        "\n",
        "# this time the function has two variables, so we need to calculate the partial derivatives\n",
        "def df(x1, x2):\n",
        "    return np.array([2*x1, 2*x2]) * 0.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Va5ZNjcacqz1"
      },
      "source": [
        "The scalar function we want to plot is: $f(x, y) = x^2 + y^2, \\; f: \\mathbb{R}^2 \\to \\mathbb{R}$\n",
        "\n",
        "- remember that the input variables lie in the horizontal plane\n",
        "- The output of the function is the \"height\" of the surface\n",
        "- The color is also the output of the function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R_w6n-Bpcqz1"
      },
      "outputs": [],
      "source": [
        "from utils import plot_gradient_2d\n",
        "\n",
        "plot_gradient_2d(f, df, follow_surface=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YBOKSeacqz1"
      },
      "source": [
        "We can also visualize the gradient as vectors that follow the surface of the function, as it is a common way to visualize gradients, but remember what we said about the gradient living in the input space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pojjEDmncqz2"
      },
      "outputs": [],
      "source": [
        "plot_gradient_2d(f, df, follow_surface=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PW5yrSRlcqz2"
      },
      "outputs": [],
      "source": [
        "lambda_ = 10\n",
        "\n",
        "def f_lasso(x1, x2):\n",
        "    mse = (x1**2 + x2**2)\n",
        "    penalty = lambda_ * (np.abs(x1) + np.abs(x2))\n",
        "    return 0.1 * (mse + penalty)\n",
        "\n",
        "def df_lasso(x1, x2):\n",
        "    d_mse = np.array([2*x1, 2*x2])\n",
        "    d_penalty = lambda_ * np.array([np.sign(x1), np.sign(x2)])\n",
        "    return 0.1 * (d_mse + d_penalty)\n",
        "\n",
        "plot_gradient_2d(f_lasso, df_lasso, follow_surface=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPsZRrzFcqz2"
      },
      "source": [
        "## LASSO"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ySQtLf3ncqz2"
      },
      "source": [
        "LASSO requires to find coefficients which solves the following minimization problem:\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\beta &= \\arg \\min_{\\beta} J(\\beta) \\\\\n",
        "& = arg \\min_{\\beta} \\left[ \\sum_{i=1}^{n} \\left( y_i - \\beta_0 - \\sum_{j=1}^{p} \\beta_j x_{ij} \\right)^2 + \\lambda \\sum_{j=\\textcolor{red}{1}}^{p}|\\beta_j|\\right] \\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "<div style=\"background-color: lightblue; padding: 10px; color: black\">\n",
        "    <strong> üö´ Warning! </strong>\n",
        "    In LASSO, as in Ridge Regression, do <strong>NOT</strong> penalize the bias term Œ≤‚ÇÄ.\n",
        "</div>\n",
        "\n",
        "\n",
        "Let:\n",
        "- $\\mathbf{X}$ be the expanded input matrix, with the column of 1s to account for the bias term.\n",
        "- $\\beta \\doteq [\\beta_0, \\beta_1, ..., \\beta_p]^T \\in \\mathbb{R}^{p+1}$, with the bias term included.\n",
        "\n",
        "Then, the optimization problem can be rewritten in matrix form:  \n",
        "$$\n",
        "\\beta = \\arg \\min_{\\beta} \\left[ (\\mathbf{y} - \\mathbf{X} \\beta)^2  + \\lambda L_1 \\right]\n",
        "$$\n",
        "\n",
        "where:\n",
        "$$\n",
        "L_1  = \\sum_{j=\\textcolor{red}{1}}^{p}|\\beta_j| \\doteq |\\beta_1| + ... + |\\beta_p|\n",
        "$$\n",
        "</div>\n",
        "\n",
        "<div style=\"background-color: lightblue; padding: 10px; color: black\">\n",
        "    <strong> ‚ö†Ô∏è Warning! </strong>\n",
        "    <u>Standardization</u> in Lasso Regression is necessary (as in Ridge).\n",
        "</div>\n",
        "\n",
        "Differently from OLS and RR, LASSO hasn't a closed-form solution. We need to find the $\\beta$ that minimize the objective function using **gradient descent**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejPVh-Dmcqz3"
      },
      "source": [
        "### Gradient of the LASSO objective function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98ge637-cqz3"
      },
      "source": [
        "To use gradient descent, we need the gradient of the objective function we want to minimize, in this case the LASSO objective function $J(\\beta)$.\n",
        "\n",
        "Let $\\beta \\doteq [\\beta_0, \\beta_1, ..., \\beta_p]^T$ and $\\mathbf{X}$ the expanded input matrix. Then:\n",
        "\n",
        "1. **Gradient of the squared loss term** (Lecture 7 for derivation): $$\\nabla_\\beta (\\mathbf{y} - \\mathbf{X} \\beta)^2 = 2\\mathbf{X}^T\\mathbf{X}\\beta -2\\mathbf{X}^Ty = 2 \\mathbf{X}^T (\\mathbf{X} \\beta - y)$$\n",
        "\n",
        "2. **Gradient of the L1 penalty**. As seen during Lecture 10, we need to use the subgradient.\n",
        "$$\n",
        "\\nabla_\\beta (\\lambda L_1) =\n",
        "    \\lambda\n",
        "        \\begin{bmatrix}\n",
        "            \\frac{\\partial}{\\partial \\beta_0} L_1 \\\\\n",
        "            \\frac{\\partial}{\\partial \\beta_1} L_1 \\\\\n",
        "            \\vdots \\\\\n",
        "            \\frac{\\partial}{\\partial \\beta_p} L_1  \n",
        "        \\end{bmatrix} =\n",
        "    \\lambda\n",
        "        \\begin{bmatrix}\n",
        "            \\textcolor{red}{0} \\\\\n",
        "            \\operatorname{sgn}(\\beta_1) \\\\\n",
        "            \\vdots \\\\\n",
        "            \\operatorname{sgn}(\\beta_p)\n",
        "        \\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "**Final gradient expression:**\n",
        "$$\n",
        "\\nabla_\\beta J(\\beta) =2\\mathbf{X}^T\\mathbf{X}\\beta -2\\mathbf{X}^Ty + \\lambda \\begin{bmatrix} \\textcolor{red}{0} \\\\ \\operatorname{sgn}(\\beta_1) \\\\ \\vdots \\\\ \\operatorname{sgn}(\\beta_p) \\end{bmatrix}$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "m5vInpUfcqz3"
      },
      "outputs": [],
      "source": [
        "def lasso_gradient(X, y, beta, lambda_):\n",
        "    \"\"\"\n",
        "    Return the gradient of the Lasso objective function with respect to Œ≤.\n",
        "    \"\"\"\n",
        "    ...            # shapes 1 @ (p+1, n) @ ((n, p+1) @ (p+1, 1) - (n, 1)) = (p+1, 1)\n",
        "    ...                  # shapes (p+1, 1)\n",
        "    ...                                     # we do not penalize the intercept term (as in Ridge Regression)\n",
        "    return ...\n",
        "\n",
        "\n",
        "def GD_update(beta, learning_rate, gradient):\n",
        "    \"\"\"\n",
        "    Return updated Œ≤ using GD update rule Œ≤_new = Œ≤_old - learning_rate * ‚àáL(Œ≤_old).\n",
        "    \"\"\"\n",
        "    ...\n",
        "    return ...\n",
        "\n",
        "\n",
        "def train_LASSO_with_GD(X_train, y_train, lambda_, learning_rate, max_iter, tol):\n",
        "    \"\"\"\n",
        "    Optimize the Lasso objective function using gradient descent.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    X_train : ndarray of shape (n_samples, n_features + 1)\n",
        "        The matrix of input features, where features are already standardized.\n",
        "    y_train : ndarray of shape (n_samples, 1)\n",
        "        The vector of true target values\n",
        "    lambda_ : float\n",
        "        The regularization parameter.\n",
        "    learning_rate : float\n",
        "        The learning rate for gradient descent (alpha)\n",
        "    max_iter : int\n",
        "        The maximum number of iterations.\n",
        "    tol : float\n",
        "        The stopping criterion. The algorithm stops when the euclidean distance of the difference between the current beta and the previous beta is less than tol.\n",
        "    \"\"\"\n",
        "\n",
        "    # expand the feature matrix X with a column of ones for the intercept term\n",
        "    n_samples, n_features = X_train.shape\n",
        "    X_train = np.hstack([np.ones((n_samples, 1)), X_train])             # add a column of ones to the left of X\n",
        "\n",
        "    ...                           # initialize Œ≤ with random normal values. We include the intercept term in Œ≤.\n",
        "    i = 0\n",
        "    beta_difference = tol + 1\n",
        "    while i < max_iter and beta_difference > tol:\n",
        "\n",
        "        gradient = ...\n",
        "\n",
        "        beta_new = ...\n",
        "        beta_difference = np.linalg.norm(beta_new - beta)               # to check the stopping criterion\n",
        "        beta = beta_new\n",
        "        i = i + 1                                                          # increment the iteration counter\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_CI50LCcqz3"
      },
      "source": [
        "### Choice of the learning rate"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2Vv_YAVcqz3"
      },
      "source": [
        "Depending on the choice of the learning rate, three scenarios are possible:\n",
        "1. **Learning rate too small**: the algorithm converges very slowly.\n",
        "3. **Just right learning rate**: the algorithm converges to the minimum.\n",
        "2. **Learning rate too large**: the algorithm diverges."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6GUJewmkcqz4"
      },
      "outputs": [],
      "source": [
        "def compute_mse(X, beta, y):\n",
        "    \"\"\"Return the mean squared error between y_true and y_pred.\"\"\"\n",
        "    return np.mean((y - X @ beta) ** 2)\n",
        "\n",
        "# We can create a simple dataset\n",
        "def create_dataset():\n",
        "    np.random.seed(0)\n",
        "    n_features = 3\n",
        "    n_samples = 100\n",
        "    X = np.random.randn(n_samples, n_features)\n",
        "    X_expanded = np.hstack(\n",
        "        [np.ones((n_samples, 1)), X]\n",
        "    )  # add a column of ones for the intercept term\n",
        "    beta_true = [1, 2, 3, 4]\n",
        "    y = X_expanded @ np.array(beta_true) + np.random.randn(n_samples)\n",
        "    y = y.reshape(-1, 1)\n",
        "    return X, y\n",
        "\n",
        "\n",
        "X, y = create_dataset()\n",
        "n_samples, n_features = X.shape\n",
        "X_expanded = np.hstack([np.ones((n_samples, 1)), X])\n",
        "\n",
        "# Learning rates to test\n",
        "alphas = [0.0001, 0.001, 0.009]  # too small, just right, too large\n",
        "lambda_ = 0.1\n",
        "tol = 1e-6\n",
        "max_iter = 20\n",
        "\n",
        "# Train LASSO\n",
        "all_errors = []\n",
        "for learning_rate in alphas:\n",
        "    errors = []\n",
        "    beta = np.random.randn(n_features + 1, 1)                           # initialize Œ≤ with random normal values. We include the intercept term in Œ≤.\n",
        "    i = 0\n",
        "    beta_difference = tol + 1\n",
        "    while i < max_iter and beta_difference > tol:\n",
        "        gradient = lasso_gradient(X_expanded, y, beta, lambda_)\n",
        "        beta_new = GD_update(beta, learning_rate, gradient)\n",
        "        beta_difference = np.linalg.norm(beta_new - beta)               # to check the stopping criterion\n",
        "        beta = beta_new\n",
        "        error = compute_mse(X_expanded, beta, y)\n",
        "        errors.append(error)\n",
        "        i = 1 + i\n",
        "    all_errors.append(errors)\n",
        "\n",
        "# Plot\n",
        "a = np.array(all_errors)\n",
        "n_iters = a.shape[1]\n",
        "x = np.arange(n_iters) + 1\n",
        "plt.plot(x, a[0, :], label=f\"alpha={alphas[0]} - too small\")\n",
        "plt.plot(x, a[1, :], label=f\"alpha={alphas[1]} - just right\")\n",
        "plt.plot(x, a[2, :], label=f\"alpha={alphas[2]} - too large\")\n",
        "plt.xticks(x)\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vXMW7_dDcqz4"
      },
      "source": [
        "## Nested Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euTqv-oOcqz4"
      },
      "source": [
        "![image.png](https://vitalflux.com/wp-content/uploads/2020/08/Screenshot-2020-08-30-at-6.33.47-PM-300x198.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8ngBf2Qcqz4"
      },
      "source": [
        "0. Define hyperparameter combinations  \n",
        "\n",
        "1. Divide the dataset into K folds  \n",
        "\n",
        "2. **Outer loop**: For fold $k_i$ in K\n",
        "\n",
        "   1. Select fold $k_i$ as the <font color=\"#1F77B4\"><strong>test set</strong></font> and the remaining as <font color=\"#818791\"><strong>train set</strong></font>.  \n",
        "\n",
        "   2. **Hyper param loop** For each combination <font color=\"#2CA02C\"><strong>C</strong></font> of hyperparameters:  \n",
        "\n",
        "      1. Divide the <font color=\"#818791\"><strong>train set</strong></font> into D folds.\n",
        "\n",
        "      2. **Inner loop**: For fold $d_j$ in D:  \n",
        "\n",
        "         a. Select fold $d_j$ as <font color=\"#D35400\"><strong>validation set</strong></font> and the remaining as <font color=\"#818791\"><strong>inner train set</strong></font>.  \n",
        "\n",
        "         b. Train the model using combination of hyperparams <font color=\"#2CA02C\"><strong>C</strong></font> and using the <font color=\"#818791\"><strong>inner train set</strong></font>.  \n",
        "\n",
        "         c. Evaluate the performance on the <font color=\"#D35400\"><strong>validation set</strong></font>.  \n",
        "\n",
        "      3. Compute the average of the performance for <font color=\"#2CA02C\"><strong>C</strong></font>.  \n",
        "\n",
        "   3. Select the combination <font color=\"#2CA02C\"><strong>C</strong></font> with the best performance.  \n",
        "\n",
        "   4. Train the model using <font color=\"#2CA02C\"><strong>C</strong></font> and the <font color=\"#818791\"><strong>train set</strong></font>.  \n",
        "\n",
        "   5. Evaluate the performance on the <font color=\"#1F77B4\"><strong>test set</strong></font>.  \n",
        "\n",
        "3. Aggregate the performances over the K folds (e.g., compute average and std).  \n",
        "\n",
        "4. The best hyperparameters combination is the mean/mode of the best combinations obtained.  \n",
        "\n",
        "5. Retrain a new model using the entire dataset and the best hyperparameter combination."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0hX4AZhcqz5"
      },
      "source": [
        "## Nested Cross Validation for LASSO on real dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O2Ro5JjOcqz5"
      },
      "source": [
        "Let's implement the procedure above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mztEhXqccqz5"
      },
      "outputs": [],
      "source": [
        "# Load data\n",
        "data = pd.read_csv(\"drive/MyDrive/AA24-25ML/prostate.csv\")\n",
        "display(data.head())\n",
        "\n",
        "X = data.drop(columns=[\"lpsa\"]).to_numpy()             # we drop the the lpsa column that we use as target\n",
        "y = data[[\"lpsa\"]].to_numpy()\n",
        "\n",
        "# always check the shape of the data to be consistent\n",
        "print(\"X.shape:\", X.shape)\n",
        "print(\"y.shape:\", y.shape)\n",
        "\n",
        "n_samples = X.shape[0]\n",
        "n_features = X.shape[1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uQ5tKycJcqz5"
      },
      "outputs": [],
      "source": [
        "# 0. Define the hyperparameters combinations\n",
        "alphas = [0.00001, 0.0001, 0.001, 0.01]\n",
        "lambdas_ = [0.0001, 0.001, 0.01, 0.1, 1, 10, 100]\n",
        "hyperparams_comb = [(alpha, lambda_) for alpha in alphas for lambda_ in lambdas_]\n",
        "\n",
        "print(\"Number of alphas:\", len(alphas))\n",
        "print(\"Number of lambdas:\", len(lambdas_))\n",
        "print(\"Number of hyperparameters combinations:\", len(hyperparams_comb))\n",
        "print(\"Hyperparameters combinations:\", hyperparams_comb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hXBOd4YAcqz6"
      },
      "outputs": [],
      "source": [
        "# 1. Split the data into K folds\n",
        "K = 8\n",
        "fold_size = n_samples // K\n",
        "\n",
        "# K-fold CV assign each sample to a fold randomly. The easiest way to do this is to shuffle\n",
        "# the data and then divide it into K equal parts, each one containing fold_size samples.\n",
        "shuffled_indices = np.random.permutation(n_samples)\n",
        "X_shuffled = X[shuffled_indices]\n",
        "y_shuffled = y[shuffled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4EjYay1rcqz6"
      },
      "outputs": [],
      "source": [
        "n_max_iter_GD = 1000\n",
        "mse_values = []             # to store performance values across folds\n",
        "r2_values = []              # to store performance values across folds\n",
        "\n",
        "# 2. Outer loop: for each fold\n",
        "for i in range(K):\n",
        "\n",
        "    # take i-th fold as test set and the rest as training set\n",
        "    start_test = i * fold_size\n",
        "    end_test = (i + 1) * fold_size\n",
        "    X_test = X_shuffled[start_test:end_test]\n",
        "    y_test = y_shuffled[start_test:end_test]\n",
        "    X_train = np.concatenate([X_shuffled[:start_test], X_shuffled[end_test:]])\n",
        "    y_train = np.concatenate([y_shuffled[:start_test], y_shuffled[end_test:]])\n",
        "\n",
        "    # 1. Iterate over the possible combinations of hyperparameters to take the best one\n",
        "    best_combination = (0, 0)                       # starting values\n",
        "    best_mse = np.inf                               # starting values\n",
        "\n",
        "    for alpha, lambda_ in hyperparams_comb:\n",
        "        # Inner-loop: cross validation to select the best hyperparameters combination (the one with lowest mse)\n",
        "        # 1. Divide the training set into D folds\n",
        "        D = 4\n",
        "        n_samples_in_X_train = X_train.shape[0]\n",
        "        D_fold_size = n_samples_in_X_train // D\n",
        "\n",
        "        mse_inner_values = []\n",
        "        for j in range(D):\n",
        "            # a. take j-th fold as validation set and the rest as training set\n",
        "            start_val = j * D_fold_size\n",
        "            end_val = (j + 1) * D_fold_size\n",
        "            X_val = X_train[start_val:end_val]\n",
        "            y_val = y_train[start_val:end_val]\n",
        "            X_inner_train = np.concatenate([X_train[:start_val], X_train[end_val:]])\n",
        "            y_innter_train = np.concatenate([y_train[:start_val], y_train[end_val:]])\n",
        "\n",
        "            # standardize the data (using the X_inner_train set)\n",
        "            X_inner_train_mean = np.mean(X_inner_train, axis=0)\n",
        "            X_inner_train_std = np.std(X_inner_train, axis=0)\n",
        "            X_inner_train = (X_inner_train - X_inner_train_mean) / X_inner_train_std\n",
        "            X_val = (X_val - X_inner_train_mean) / X_inner_train_std\n",
        "\n",
        "            # b. Find the coefficients of the LASSO model using the inner training set + the current hyperparameters combination\n",
        "            beta_ = train_LASSO_with_GD(X_inner_train, y_innter_train, lambda_, alpha, n_max_iter_GD, 1e-6)\n",
        "\n",
        "            # c. Evaluate the model on the validation set\n",
        "            y_val_pred = beta_[0] + X_val @ beta_[1:]\n",
        "            mse = np.mean((y_val - y_val_pred)**2)\n",
        "            mse_inner_values.append(mse)\n",
        "\n",
        "        # 2. Compute the average performance on the validation set\n",
        "        mse_avg = np.mean(mse_inner_values)\n",
        "\n",
        "        # if the current hyperparam combination is the best (i.e., lowest MSE), update the best combination and the best MSE\n",
        "        if mse_avg < best_mse:\n",
        "            best_combination = (alpha, lambda_)\n",
        "            best_mse = mse_avg\n",
        "\n",
        "\n",
        "    # 5. Select the best hyperparameters combination based on the average performance on the validation set\n",
        "    best_alpha, best_lambda_ = best_combination\n",
        "    print(\"Best hyperparameters combination for iteration \", i, \" is: \", best_combination)\n",
        "\n",
        "    # 6. Train LASSO using the training set + the best hyperparameters combination\n",
        "    # standardize the data and add the intercept term\n",
        "    X_train_mean = np.mean(X_train, axis=0)\n",
        "    X_train_std = np.std(X_train, axis=0)\n",
        "    X_train_scaled = (X_train - X_train_mean) / X_train_std\n",
        "    X_test_scaled = (X_test - X_train_mean) / X_train_std\n",
        "    X_train_scaled = np.concatenate([np.ones((X_train_scaled.shape[0], 1)), X_train_scaled], axis=1)\n",
        "    X_test_scaled = np.concatenate([np.ones((X_test_scaled.shape[0], 1)), X_test_scaled], axis=1)\n",
        "\n",
        "    beta = np.random.randn(n_features + 1, 1)\n",
        "    for t in range(n_max_iter_GD):\n",
        "        gradient = lasso_gradient(X_train_scaled, y_train, beta, best_lambda_)\n",
        "        beta = GD_update(beta, best_alpha, gradient)\n",
        "\n",
        "    # 7. Evaluate the model on the test set\n",
        "    y_test_pred = X_test_scaled @ beta\n",
        "    mse = np.mean((y_test - y_test_pred)**2)\n",
        "    r2 = 1 - mse / np.var(y_test)\n",
        "\n",
        "    mse_values.append(mse)\n",
        "    r2_values.append(r2)\n",
        "\n",
        "# 8. Aggregate performance metrics, plot boxplots\n",
        "mse_list = np.array(mse_values)\n",
        "r2_list = np.array(r2_values)\n",
        "\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.boxplot(mse_list)\n",
        "plt.title(\"MSE\")\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.boxplot(r2_list)\n",
        "plt.title(\"R2\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ScZw3uCTcqz6"
      },
      "source": [
        "### Adaptive Learning Rate\n",
        "- Too high values of the learning rate -> the algorithm may miss the minimum and diverge.\n",
        "- Too low values -> the convergence may be very slow.\n",
        "\n",
        "Solution: use high values of the learning rate at the beginning, when beta is probably far from the optimal value, and decrease the learning rate incrementally.\n",
        "\n",
        "Dynamic adjustment of the learning rate:\n",
        "$$\n",
        "\\alpha_{t} = \\frac{\\alpha_0}{1+\\gamma t}\n",
        "$$\n",
        "where:\n",
        "- $ \\alpha_0 $ is the initial learning rate\n",
        "- $ t $ is the current iteration\n",
        "- $ \\gamma $ is a decay parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0XezUNj6cqz7"
      },
      "source": [
        "<span style=\"color:red\"><strong>üèãÔ∏è‚Äç‚ôÄÔ∏è Exercise</strong></span>\n",
        "\n",
        "Implement adaptive learning rate in the gradient descent algorithm.\n",
        "\n",
        "Before, with alpha=0.009 the loss was divergent, but by decreasing the learning rate each epoch, we can stabilize the learning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "izNgfSZOcqz7"
      },
      "outputs": [],
      "source": [
        "X, y = create_dataset()\n",
        "n_samples, n_features = X.shape\n",
        "X_expanded = np.hstack([np.ones((n_samples, 1)), X])\n",
        "\n",
        "# We set the parameters\n",
        "alphas = 0.009\n",
        "lambda_ = 0.1\n",
        "tol = 1e-6\n",
        "max_iter = 20\n",
        "\n",
        "# Train LASSO (static learning rate)\n",
        "beta = np.random.randn(n_features + 1, 1)                           # initialize Œ≤ with random normal values. We include the intercept term in Œ≤.\n",
        "i = 0\n",
        "beta_difference = tol + 1\n",
        "\n",
        "#errors relative to the static learning rate implementation\n",
        "static_errors = []\n",
        "while i < max_iter and beta_difference > tol:\n",
        "    gradient = lasso_gradient(X_expanded, y, beta, lambda_)\n",
        "    beta_new = GD_update(beta, learning_rate, gradient)\n",
        "    beta_difference = np.linalg.norm(beta_new - beta)               # to check the stopping criterion\n",
        "    beta = beta_new\n",
        "    error = compute_mse(X_expanded, beta, y)\n",
        "    static_errors.append(error)\n",
        "    i = 1 + i\n",
        "\n",
        "# Train LASSO (adaptive learning rate)\n",
        "beta = np.random.randn(n_features + 1, 1)                           # initialize Œ≤ with random normal values. We include the intercept term in Œ≤.\n",
        "i = 0\n",
        "beta_difference = tol + 1\n",
        "decay_rate=0.9\n",
        "\n",
        "#errors relative to the static learning rate implementation\n",
        "adaptive_errors = []\n",
        "while i < max_iter and beta_difference > tol:\n",
        "    ...\n",
        "    error = compute_mse(X_expanded, beta, y)\n",
        "    adaptive_errors.append(error)\n",
        "    i = 1 + i\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.plot(static_errors, label=f\"Static learning rate\")\n",
        "plt.plot(adaptive_errors, label=f\"Adaptive learning rate\")\n",
        "plt.xlabel(\"Number of iterations\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "capsule",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}