{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e20a250e",
   "metadata": {},
   "source": [
    "**ML COURSE 2024-2025**\n",
    "# LAB7  NEURAL NETWORKS 1\n",
    "In this lab you will see how to construct, train and evaluate a NN model using Keras. We will use MNIST as dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4931b0e",
   "metadata": {},
   "source": [
    "## Deep Learning Frameworks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3131a5ee",
   "metadata": {},
   "source": [
    "| **Framework**  | **Description**                                         | **Strengths**                                          | **Use Case**                          |\n",
    "|----------------|---------------------------------------------------------|--------------------------------------------------------|---------------------------------------|\n",
    "| <img src=\"https://keras.io/img/logo.png\" width=\"200\" />      | High-level API for building neural networks, runs on TensorFlow | Easy to use, simple model building                      | Quick prototyping and beginners       |\n",
    "|<img src=\"https://www.gstatic.com/devrel-devsite/prod/v8d1d0686aef3ca9671e026a6ce14af5c61b805aabef7c385b0e34494acbfc654/tensorflow/images/lockup.svg\" width=\"200\" />| Powerful, flexible platform for machine learning       | Scalable, strong production support, wide ecosystem     | Production, large-scale applications  |\n",
    "| <img src=\"https://images.icon-icons.com/2699/PNG/512/pytorch_logo_icon_169823.png\" width=\"200\" />     | Dynamic computation graph, more intuitive              | Debug-friendly, popular in research, dynamic graphs     | Research, experimentation, and flexibility |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762b6353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "# new library for deep learning!\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4c0251",
   "metadata": {},
   "source": [
    "## MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63f43bb2",
   "metadata": {},
   "source": [
    "The **MNIST (Modified National Institute of Standards and Technology)** dataset is a classic benchmark dataset in the field of machine learning and computer vision. It consists of **70,000 grayscale images** of handwritten digits from **0 to 9**, each sized **28 x28 pixels**.\n",
    "\n",
    "## Dataset Details\n",
    "\n",
    "- **Training set**: 60,000 images  \n",
    "- **Test set**: 10,000 images  \n",
    "- **Image format**: 28x28 grayscale  \n",
    "- **Classes**: 10 (digits 0 through 9)  \n",
    "- **Label format**: Integer label corresponding to the digit in the image\n",
    "\n",
    "Each image is a low-resolution scan of a digit, making the dataset ideal for:\n",
    "- Image classification\n",
    "- Deep learning model benchmarking\n",
    "- Feature extraction and representation learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd744e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff10357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and explore MNIST dataset\n",
    "# This loads the MNIST dataset and splits it into training and test sets\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Print data type and shape information for training and test sets\n",
    "print('Class of x_train: {}'.format(type(x_train)))\n",
    "print('Class of y_train: {}'.format(type(y_train)))  \n",
    "print('Shape of x_train: {}'.format(x_train.shape))  \n",
    "print('Shape of y_train: {}'.format(y_train.shape)) \n",
    "print('Shape of x_test: {}'.format(x_test.shape))    \n",
    "print('Shape of y_test: {}'.format(y_test.shape))    \n",
    "\n",
    "# Visualize 4 random samples from the training set\n",
    "# Randomly select 4 indices from the training set\n",
    "idxes = [np.random.randint(60000) for i in range(4)]\n",
    "\n",
    "# Create a new figure with a size of 15x8 inches\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot each randomly selected image with its corresponding label\n",
    "for i in range(4):\n",
    "  plt.subplot(1, 4, i+1)  # Create a subplot in a 1-row, 4-column grid\n",
    "  plt.imshow(x_train[idxes[i]], cmap='Greys')  # Display the image in grayscale\n",
    "  plt.title('Label = {}'.format(y_train[idxes[i]]))  # Set title to the digit label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d51f72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the distribution of classes in training and in test\n",
    "\n",
    "# Count the number of occurrences of each digit in the training labels\n",
    "train_class_counter = Counter(y_train) #dictionary with keys as digits and values as counts\n",
    "\n",
    "# Create a new figure and plot a bar chart for the training set class distribution\n",
    "plt.figure()\n",
    "plt.bar(train_class_counter.keys(), train_class_counter.values())\n",
    "plt.title('Class distribution in training dataset')  # Title for the plot\n",
    "\n",
    "# Count the number of occurrences of each digit in the test labels\n",
    "test_class_counter = Counter(y_test)\n",
    "\n",
    "# Create a new figure and plot a bar chart for the test set class distribution\n",
    "plt.figure()\n",
    "plt.bar(test_class_counter.keys(), test_class_counter.values())\n",
    "plt.title('Class distribution in test dataset')  # Title for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08801c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Don't forget about normalization of input data. \n",
    "# Min-max normalization serve a scalare ogni valore tra 0 e 1 \n",
    "#(min che pu√≤ assumere ogni pixel √® 0 max √® 255)\n",
    "# (prendere il valore - min) / (max- min)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e152126a",
   "metadata": {},
   "source": [
    "## Create a model with Keras: Sequential class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eb7404",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import Sequential, Input\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a09906",
   "metadata": {},
   "source": [
    "## Keras Classes and Layers Overview\n",
    "\n",
    "### Keras Model Class\n",
    "\n",
    "- **`Sequential`**:  \n",
    "  A simple way to build models layer-by-layer in a linear stack. Each layer receives the output of the previous one. Suitable for straightforward architectures.  \n",
    "  ‚ö†Ô∏è Use the **Functional API** if your model involves more complex topologies, such as multiple inputs, multiple outputs, or shared layers.\n",
    "\n",
    "### Keras Layers Used in This Lab\n",
    "\n",
    "- **`Dense`**:  \n",
    "  A fully connected layer where each input node is connected to every output node.\n",
    "\n",
    "- **`Flatten`**:  \n",
    "  Converts a multi-dimensional input (e.g., a 2D image) into a 1D vector. Commonly used to bridge convolutional layers and dense layers.\n",
    "\n",
    "- **`Dropout`**:  \n",
    "  A regularization technique that randomly sets a fraction of input units to zero during training, helping prevent overfitting (will see it compared to other regularization techniques later)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08f63c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to create a Keras Sequential model\n",
    "def create_model(compile_model=False):\n",
    "    # DEFINE a sequential model with a specific architecture\n",
    "    model = Sequential([ # architettura specifica\n",
    "        Input(shape=(28,28)), \n",
    "        Flatten(),\n",
    "        Dense(512, activation='relu'),\n",
    "        Dropout(0.2),\n",
    "        Dense(10, activation='relu')\n",
    "    ])\n",
    "\n",
    "    # If we want to TRAIN:\n",
    "    # If compile_model is True, compile the model before returning\n",
    "    if compile_model:\n",
    "        # Compiling the model configures the learning process:\n",
    "        # - Specifies the loss function (how model error is calculated)\n",
    "        # - Specifies the optimizer (how the model updates weights)\n",
    "        # - Specifies metrics to track (e.g., accuracy)\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',  # Loss for integer class labels\n",
    "            optimizer='adam',                        # Adam optimizer for efficient training\n",
    "            metrics=['accuracy']                     # Track accuracy as the performance metric\n",
    "        )\n",
    "    \n",
    "    return model  # Return the constructed (and optionally compiled) model\n",
    "\n",
    "# Create and compile the model\n",
    "model = create_model(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42971770",
   "metadata": {},
   "source": [
    "### Cross-Entropy\n",
    "\n",
    "Cross-entropy is a **loss function** commonly used in classification tasks. It measures the difference between two probability distributions:\n",
    "- The **true distribution**: based on actual labels (e.g., one-hot encoded, where one class has probability 1 and the rest have 0).\n",
    "- The **predicted distribution**: output by the model (e.g., softmax probabilities).\n",
    "\n",
    "It **penalizes incorrect predictions** more when they are confidently wrong (i.e., assigning high probability to the wrong class).\n",
    "\n",
    "### Sparse Categorical Cross-Entropy\n",
    "\n",
    "This is a variant of cross-entropy used when the **true labels are integers**, not one-hot encoded vectors.\n",
    "\n",
    "- For example, instead of a label like `[0, 0, 1, 0, ..., 0]` (one-hot), the label is simply `2`.\n",
    "- It is suitable for multi-class classification tasks where the target is a class index (e.g., 0 to 9 in MNIST).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21d8a0ca",
   "metadata": {},
   "source": [
    "Visualize the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0d7365",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PRINT A SUMMARY OF THE MODEL ARCHITECTURE\n",
    "# This includes the layer types, output shapes, and number of trainable parameters\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1d9409",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ee1980b",
   "metadata": {},
   "source": [
    "Now that we have defined the model, we train it on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a94aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN the model using the training data\n",
    "history = model.fit(\n",
    "    x_train,             # Input images\n",
    "    y_train,             # Corresponding labels\n",
    "    batch_size=32,       # Number of samples per gradient update \n",
    "    epochs=10,           # Number of times to iterate over the training data\n",
    "    validation_split=0.1 # Reserve 10% of the training data for validation \n",
    "                         # (calcola accuracy e loss su un dataset che non ha mai visto,\n",
    "                         # ovvero il 10% del training set)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f07f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure for plotting\n",
    "plt.figure()\n",
    "\n",
    "# Plot training accuracy over epochs\n",
    "# history.history['accuracy'] contains the training accuracy for each epoch\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "\n",
    "# Plot validation accuracy over epochs\n",
    "# history.history['val_accuracy'] contains the validation accuracy for each epoch\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "\n",
    "plt.grid()\n",
    "plt.title('Training vs Validation Accuracy')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dee46b01",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b0eab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EVALUATE the model on the test data\n",
    "# The model will compute the loss and accuracy on the test set (x_test, y_test)\n",
    "[test_loss, test_accuracy] = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test accuracy\n",
    "# This is the final performance metric for the model on unseen test data\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171f8ef0",
   "metadata": {},
   "source": [
    "### Get Predictions\n",
    "\n",
    "Note that the model outputs probabilities for each class. We then select the class with the highest probability as our final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2978f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model on the test data\n",
    "# The model outputs PROBABILITIES for each class for each test sample\n",
    "predictions = model.predict(x_test)\n",
    "\n",
    "# CONVERT THE PROBABILITIES INTO A PREDICTED LABEL\n",
    "# np.argmax returns the index of the maximum probability for each sample\n",
    "# axis=1 indicates we are selecting the index along each row (per sample)\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ba79b2",
   "metadata": {},
   "source": [
    "Let's see which are the most confused classes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6ab333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the indices where the predicted labels do not match the true labels\n",
    "# np.where returns the indices of elements that satisfy the condition (y_pred != y_test)\n",
    "mismatch = np.where(y_pred != y_test)[0]\n",
    "\n",
    "# Count the number of occurrences for each class in the mismatched predictions\n",
    "mismatch_class_counter = Counter(y_test[mismatch])\n",
    "\n",
    "# Initialize a dictionary to store the mismatch percentage for each class\n",
    "mismatch_percentage = dict()\n",
    "\n",
    "# Calculate the mismatch percentage for each digit class\n",
    "for digit in test_class_counter.keys():\n",
    "    # The percentage of mismatches for each class\n",
    "    mismatch_percentage[digit] = mismatch_class_counter[digit] / test_class_counter[digit] * 100\n",
    "\n",
    "# Create a new figure for plotting\n",
    "plt.figure()\n",
    "\n",
    "# Plot a bar chart showing the percentage of mismatches per class\n",
    "plt.bar(mismatch_percentage.keys(), mismatch_percentage.values())\n",
    "\n",
    "# Add a title to the plot\n",
    "plt.title('Class Distribution for Mismatch in Label Prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0aa72df",
   "metadata": {},
   "source": [
    "Let's print 4 random mispredicted samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e68d548d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shuffle the indices of the mismatched predictions to select random samples\n",
    "np.random.shuffle(mismatch)\n",
    "\n",
    "# Select the first 4 mismatched samples after shuffling\n",
    "idxes = mismatch[:4]\n",
    "\n",
    "# Create a new figure to visualize the images\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Loop through the selected mismatched samples and plot them\n",
    "for i in range(4):\n",
    "    # Create a subplot for each image\n",
    "    plt.subplot(1, 4, i+1)\n",
    "    \n",
    "    # Display the image with a grayscale colormap\n",
    "    plt.imshow(x_test[idxes[i]], cmap='Greys')\n",
    "    \n",
    "    # Add the true and predicted label as the title for each image\n",
    "    plt.title('True label = {}\\nPredicted label: {}'.format(y_test[idxes[i]], y_pred[idxes[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b559f44",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix\n",
    "# The confusion matrix is calculated using the true labels (y_test) and predicted labels (y_pred)\n",
    "# We take the logarithm of the confusion matrix values for better visualization of smaller values\n",
    "cm = np.log(1 + confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Create a figure and axis for plotting\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Display the confusion matrix as an image\n",
    "# imshow() is used to show the matrix as a heatmap with a red colormap ('Reds')\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "\n",
    "# Add a color bar next to the plot to show the value scale\n",
    "ax.figure.colorbar(im, ax=ax)\n",
    "\n",
    "# Label the x-axis and y-axis for clarity\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da58344",
   "metadata": {},
   "source": [
    "## Save the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed1eb5",
   "metadata": {},
   "source": [
    "We can save the model weights or the entire model (architecture + weights), either at the end of training or periodically during training (e.g., after every epoch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce327a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Commands that you need only if you are using Google Colab\n",
    "# mount drive folder\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5041a002",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the main folder where files will be saved\n",
    "folder = \"./drive/MyDrive/AA24-25ML/\"\n",
    "\n",
    "# Define the destination folder path by joining the main folder path with a subfolder name\n",
    "destination_folder = os.path.join(folder, '01_FFNN')\n",
    "\n",
    "# Check if the destination folder exists; if not, create it\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.mkdir(destination_folder)\n",
    "\n",
    "# List the contents of the destination folder\n",
    "# This will return the files and subdirectories inside the destination folder\n",
    "os.listdir(destination_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0bdb874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model weights to a file with the extension .weights.h5\n",
    "# This saves only the weights, not the entire model architecture\n",
    "model.save_weights(destination_folder + '/weights.weights.h5')\n",
    "\n",
    "# Create a new model instance with the same architecture\n",
    "# Since only the weights are being saved, we need to recreate the same model architecture\n",
    "model_reloaded = create_model(compile_model=False)\n",
    "\n",
    "# Load the saved weights into the newly created model\n",
    "# This restores the model's learned weights from the file\n",
    "model_reloaded.load_weights(destination_folder + '/weights.weights.h5')\n",
    "\n",
    "# Compile the reloaded model to prepare it for evaluation\n",
    "# We use the same loss function, optimizer, and metrics as in the original model\n",
    "model_reloaded.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Evaluate the reloaded model on the test data\n",
    "# This will return the test loss and accuracy for the reloaded model\n",
    "[reloaded_test_loss, reloaded_test_accuracy] = model_reloaded.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test accuracy from the original model (which was computed before reloading)\n",
    "print('Test accuracy from original model: {}'.format(test_accuracy))\n",
    "\n",
    "# Print the test accuracy from the reloaded model (after restoring the weights)\n",
    "print('Test accuracy from reloaded model: {}'.format(reloaded_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b73391ed",
   "metadata": {},
   "source": [
    "You can also save the weights during training, using Keras callbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7753b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving checkpoints during training with Keras CALLBACKS\n",
    "\n",
    "# Define a folder to store the checkpoints\n",
    "checkpoint_folder = destination_folder + \"/training_00_FFNN/\"\n",
    "\n",
    "# Create the checkpoint folder if it doesn't already exist\n",
    "if not os.path.exists(checkpoint_folder): \n",
    "    os.mkdir(checkpoint_folder)\n",
    "\n",
    "# Define the full path template for saving checkpoints\n",
    "# This includes the epoch number in the filename using str.format\n",
    "checkpoint_path = checkpoint_folder + \"/cp-{epoch:04d}.weights.h5\"\n",
    "\n",
    "# Extract the directory path (in case it's needed later)\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# DEFINE A KERAS CALLBACK to save model checkpoints during training\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,         # File path pattern to save checkpoints\n",
    "    verbose=1,                         # Log saving actions\n",
    "    save_weights_only=True,           # SAVE ONLY WEIGHTS, not the full model\n",
    "    save_freq='epoch'                 # Save weights at the end of every epoch\n",
    ")\n",
    "\n",
    "# Create and compile a new model instance\n",
    "model = create_model(compile_model=True)\n",
    "\n",
    "# Optionally save the initial weights before training starts (epoch 0)\n",
    "model.save_weights(checkpoint_path.format(epoch=0))\n",
    "\n",
    "# Train the model for 10 epochs with the checkpoint callback enabled\n",
    "# This will save weights to a new file at the end of each epoch\n",
    "model.fit(\n",
    "    x_train, y_train,\n",
    "    epochs=10,\n",
    "    callbacks=[cp_callback],\n",
    "    validation_data=(x_test, y_test),\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34be3bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAVING FULLY-FUNCTIONAL MODEL\n",
    "\n",
    "# Save the entire model to a single HDF5 (.h5) file\n",
    "# This includes the model architecture, weights, and optimizer state\n",
    "model.save(destination_folder + '/mnist_00_ffnn.h5')\n",
    "\n",
    "# Load the saved model from file\n",
    "# This recreates the model exactly as it was at the time of saving\n",
    "new_model = tf.keras.models.load_model(destination_folder + '/mnist_00_ffnn.h5')\n",
    "\n",
    "# Display the architecture of the reloaded model\n",
    "print(new_model.summary())\n",
    "\n",
    "# Evaluate the reloaded model on the test dataset\n",
    "# This verifies that the model's performance is preserved after saving and loading\n",
    "[new_test_loss, new_test_accuracy] = new_model.evaluate(x_test, y_test)\n",
    "\n",
    "# Compare the accuracy of the original model and the reloaded one\n",
    "print('\\n\\nTest accuracy from original model: {}'.format(test_accuracy))\n",
    "print('Test accuracy from new model: {}'.format(new_test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c2306c",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"><strong>üèãÔ∏è‚Äç‚ôÄÔ∏è Exercises</strong></span>\n",
    "- What happens if dropout is removed?\n",
    "- Does it help to add more dense layers?\n",
    "- Try to visually inspect the digits that are wrongly recognized by the FFNN: can you correctly classify them?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "323a22c8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ad5f6d0f",
   "metadata": {},
   "source": [
    "## Overview of Keras API for Creating Models\n",
    "\n",
    "In the previous example, we used the **Sequential API**, where a model is built by stacking layers linearly ‚Äî each layer has exactly one input and one output.\n",
    "\n",
    "However, `tf.keras` offers more flexible ways to define models:\n",
    "\n",
    "- **Functional API**  \n",
    "  Enables the creation of complex architectures, such as models with multiple inputs and outputs, shared layers, and NON-LINEAR topologies (e.g., skip connections or residual networks).\n",
    "\n",
    "- **Subclassing (Custom Model Definition)**  \n",
    "  Allows you to define a model by subclassing the `tf.keras.Model` class and manually implementing the `call()` method. This approach is ideal for building highly customized models or for research purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916ad2aa-bd34-4322-9c4e-f61631f10e65",
   "metadata": {},
   "source": [
    "### Functional API\n",
    "\n",
    "The Functional API allows us to build models with more complex topologies than the Sequential API, including:\n",
    "\n",
    "- **Multi-input models**\n",
    "- **Multi-output models**\n",
    "- **Models with shared layers** (the same layer used multiple times)\n",
    "- **Models with non-sequential data flows** (e.g., skip connections, residual networks)\n",
    "\n",
    "Despite the added flexibility, models built using the Functional API are compiled, trained, and evaluated in the same way as Sequential models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc4e5f32-f62e-4eb8-8af3-579d81c99e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Example: Building a Model with the Functional API\n",
    "\n",
    "from tensorflow.keras.layers import Input, Flatten, Dense, Dropout\n",
    "from tensorflow.keras.models import Model # NON SEQUENTIAL\n",
    "\n",
    "# The Functional API defines the data flow explicitly, starting from Input\n",
    "inputs = Input(shape=(28, 28))  # Symbolic placeholder for input data (no need to specify batch size)\n",
    "\n",
    "# Stesso modello di prima di Sequential, ma definito in altro modo\n",
    "x = Flatten()(inputs)           # Flatten the 2D image into a 1D vector\n",
    "x = Dense(512, activation='relu')(x)  # Fully connected hidden layer with ReLU activation\n",
    "x = Dropout(0.2)(x)             # Dropout layer for regularization\n",
    "predictions = Dense(10, activation='softmax')(x)  # Output layer with softmax for multi-class classification\n",
    "\n",
    "# Create the Model object by specifying inputs and outputs\n",
    "model = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "# Compile the model: define loss function, optimizer, and evaluation metric\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model on the training data\n",
    "model.fit(x_train, y_train, batch_size=32, epochs=5, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "model.evaluate(x_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f63648",
   "metadata": {},
   "source": [
    "### Perch√® non si pu√≤ sempre usare Sequential API: A More Involved Case - Multi-Output Model\n",
    "\n",
    "Suppose we want to train a model that assesses the credit risk of loan applicants. Each applicant is described by 30 tabular features (e.g., age, income, credit score, employment type, etc.).\n",
    "The model should output:\n",
    "- **Default risk** (binary classification): predict whether the applicant is likely to default on the loan.\n",
    "- **Expected loss amount** (regression): estimate how much money the lender stands to lose if the applicant defaults.\n",
    "\n",
    "Why a Multi-Ouput model? \n",
    "- Both outputs depend on the same features. Shared layers allow the model to learn common representations, improving performance.\n",
    "- It is computationally efficient ‚Äî one model, one inference step.\n",
    "\n",
    "This use case is a great candidate for Keras's **Functional API**, which allows handling multiple outputs (and inputs) within a single model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feb3cb3a-006c-4b6f-b034-6db320bdb816",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "\n",
    "input_data = Input(shape=(30,))\n",
    "\n",
    "# shared layers \n",
    "x = Dense(64, activation='relu')(input_data)\n",
    "x = Dense(32, activation='relu')(x)\n",
    "\n",
    "# Output 1: DEFAULT PROBABILITIES (classification)\n",
    "default_output = Dense(1, activation='sigmoid', name='default_risk')(x)\n",
    "\n",
    "# Output 2: EXPECTED LOSS (regression)\n",
    "loss_output = Dense(1, name='expected_loss')(x)\n",
    "\n",
    "model = Model(inputs=input_data, outputs=[default_output, loss_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58d11abc",
   "metadata": {},
   "source": [
    "### Custom Model Definition\n",
    "\n",
    "By subclassing `tf.keras.Model` and defining the forward pass (`call` method), it is possible to build completely custom models while still leveraging automatic gradient computation and all of Keras's high-level features.\n",
    "\n",
    "Additionally, custom layers can be created by subclassing `tf.keras.layers.Layer`.\n",
    "\n",
    "‚ö†Ô∏è **Note:** While this is the most flexible approach, it is also the most error-prone. It requires careful implementation of the forward pass and model structure.\n",
    "\n",
    "### Example: Custom MNIST Model via Subclassing\n",
    "\n",
    "The example below shows how to create a custom model by subclassing `tf.keras.Model`. This approach gives you full control over the forward pass:\n",
    "\n",
    "- `__init__()` is used to define the model architecture.\n",
    "- `call()` defines how data flows through the layers.\n",
    "\n",
    "The model consists of:\n",
    "- A `Flatten` layer to convert image input into a vector.\n",
    "- A dense hidden layer with ReLU activation.\n",
    "- A dropout layer to reduce overfitting.\n",
    "- An output layer with `sigmoid` activation (suitable for multi-label tasks).\n",
    "\n",
    "This architecture can be compiled, trained, and evaluated like any other Keras model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4a9cf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='custom_MNIST_model')\n",
    "        self.num_classes = num_classes\n",
    "        \n",
    "        # Define the model layers\n",
    "        self.Flatten = Flatten()                         # Flattens 2D image into 1D vector\n",
    "        self.dense_1 = Dense(512, activation='relu')     # First dense layer with ReLU activation\n",
    "        self.dropout = Dropout(0.2)                      # Dropout for regularization\n",
    "        self.dense_2 = Dense(num_classes, activation='sigmoid')  # Output layer with sigmoid (multi-label classification)\n",
    "\n",
    "    def call(self, inputs): \n",
    "        # Define the FORWARD PASS\n",
    "        x = self.Flatten(inputs)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dropout(x)\n",
    "        return self.dense_2(x)  # Output logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f68e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the custom model class with 10 output classes (e.g., for MNIST)\n",
    "model = MyModel(num_classes=10)\n",
    "\n",
    "# Compile the model by specifying the loss function, optimizer, and evaluation metric\n",
    "# - Loss: sparse_categorical_crossentropy (suitable for integer labels in multi-class classification)\n",
    "# - Optimizer: Adam (adaptive learning rate)\n",
    "# - Metrics: accuracy (to monitor classification performance)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    optimizer='adam',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model on the training data for 5 epochs\n",
    "model.fit(x_train, y_train, epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c24e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the trained model on the test set.\n",
    "# Returns the loss and accuracy values.\n",
    "loss, acc = model.evaluate(x_test, y_test)\n",
    "\n",
    "# Print the test accuracy\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a893b0a1",
   "metadata": {},
   "source": [
    "## [TensorBoard](https://github.com/tensorflow/tensorboard/blob/master/README.md)\n",
    "\n",
    "TensorBoard is a powerful dashboarding tool that helps with:\n",
    "\n",
    "- Training model visualization\n",
    "- Debugging workflows\n",
    "- Selecting and tuning hyperparameters\n",
    "- Inspecting model architecture\n",
    "- Exploring datasets\n",
    "- And more...\n",
    "\n",
    "![TensorBoard Cockpit Example](https://www.tensorflow.org/images/mnist_tensorboard.png)\n",
    "\n",
    "TensorBoard can be configured in many ways. Below, we explore the simplest setup using the `tf.keras.callbacks.TensorBoard` callback.\n",
    "\n",
    "This tool can be used in combination with other callbacks, such as:\n",
    "\n",
    "- Saving model checkpoints (`ModelCheckpoint`)\n",
    "- Implementing early stopping (`EarlyStopping`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef276d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset.\n",
    "# x_train and x_test are images of handwritten digits (28x28 grayscale).\n",
    "# y_train and y_test are the corresponding labels (0 through 9).\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize pixel values to the range [0, 1] by dividing by 255.\n",
    "# This helps the model train faster and improves performance.\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c7af1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the main folder path where models will be saved\n",
    "folder = \"./drive/MyDrive/AA24-25ML/\"\n",
    "\n",
    "# Define a subdirectory specifically for FFNN models with TensorBoard checkpoints\n",
    "destination_folder = os.path.join(folder, 'FFNN_tensorboard/checkpoints')\n",
    "\n",
    "# Create the directory if it doesn't already exist (recursively makes all intermediate directories)\n",
    "if not os.path.exists(destination_folder):\n",
    "    os.makedirs(destination_folder)\n",
    "\n",
    "# List all files and folders in the destination directory to confirm it's accessible\n",
    "os.listdir(destination_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c67c8452",
   "metadata": {},
   "source": [
    "### Callbacks Definition\n",
    "\n",
    "First, we used a callback to save the weights of the model at each epoch.  \n",
    "Now, we can define and add another callback to use **TensorBoard**.\n",
    "\n",
    "TensorBoard provides visualizations and metrics that help in monitoring and debugging training in real-time.  \n",
    "It can be used alongside other callbacks such as checkpoint saving or early stopping.\n",
    "\n",
    "We define two callbacks:\n",
    "\n",
    "1. **TensorBoard Callback**: This callback allows us to log training data for visualization in TensorBoard. It includes the following parameters:\n",
    "   - `log_dir`: Directory to store logs.\n",
    "   - `histogram_freq`: The frequency (in epochs) at which to compute activation and weight histograms.\n",
    "   - `write_graph`: Whether to write the model graph to the logs.\n",
    "   - `write_images`: Whether to write the model weights as image summaries.\n",
    "   - `update_freq`: How often to update the logs (can be `'epoch'`, `'batch'`, etc.).\n",
    "\\\\\n",
    "2. **ModelCheckpoint Callback**: This callback saves the model weights at regular intervals (in this case, after each epoch). It includes the following parameters:\n",
    "   - `filepath`: Path to save the model weights, with the format including the epoch number and validation loss.\n",
    "   - `monitor`: The metric to monitor, which is the validation loss here.\n",
    "   - `save_best_only`: Set to `False` to save weights at every epoch, or `True` to save only the best weights according to the monitored metric.\n",
    "   - `save_weights_only`: Set to `True` to save only the weights, not the entire model.\n",
    "   - `mode`: Defines whether the `monitor` value should be maximized or minimized.\n",
    "   - `save_freq`: The frequency at which to save the model weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18cb990",
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks = [\n",
    "            tf.keras.callbacks.TensorBoard(     # callback for TensorBoard\n",
    "                log_dir='tb_logs',\n",
    "                histogram_freq=1,\n",
    "                write_graph=True,\n",
    "                write_images=False,\n",
    "                update_freq='epoch'\n",
    "                ),\n",
    "            tf.keras.callbacks.ModelCheckpoint(\n",
    "                filepath= destination_folder+'/weights_{epoch:02d}-{val_loss:.2f}.weights.h5',\n",
    "                monitor='val_loss',\n",
    "                verbose=0,\n",
    "                save_best_only=False,   # this would save only the best model wrt validation loss\n",
    "                save_weights_only=True, \n",
    "                mode='auto',   \n",
    "                save_freq='epoch', # save every epoch\n",
    "                )\n",
    "            ]\n",
    "\n",
    "\n",
    "# Fit the model with the defined callbacks\n",
    "model = create_model(compile_model=True)  # Create the model (function `create_model` must define the model architecture)\n",
    "history = model.fit(\n",
    "    x_train,  # Training data\n",
    "    y_train,  # Training labels\n",
    "    batch_size=32,  # Number of samples per gradient update\n",
    "    epochs=10,  # Number of epochs to train the model\n",
    "    validation_split=0.1,  # Use 10% of the data for validation\n",
    "    callbacks=callbacks  # Pass the defined callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa762920",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "[test_loss, test_accuracy] = model.evaluate(x_test,y_test)\n",
    "print('Test accuracy: {}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cc192da",
   "metadata": {},
   "source": [
    "**Visualize tensorboard**  \n",
    "You can launch tensorboard from terminal and then open `localhost:6006` in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5d02e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# launch tensorboard from jupyter\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir tb_logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca120ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using API instead\n",
    "from tensorboard import notebook\n",
    "notebook.list() # View open TensorBoard instances\n",
    "\n",
    "# # Control TensorBoard display. If no port is provided,\n",
    "# # the most recently launched TensorBoard is used\n",
    "notebook.display(height=1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641f7eee-0109-4731-b432-4f9d369e018e",
   "metadata": {},
   "source": [
    "## Regularization\n",
    "\n",
    "There are many regularization methodologies that can be useful in training Neural Networks:\n",
    "\n",
    "- **L1/L2 Regularization**: These add penalty terms to the loss function based on the weights of the model. L1 regularization (LASSO) encourages sparsity, while L2 (Ridge) regularization penalizes large weights.\n",
    "- **Dropout**: A technique where during training, random neurons are dropped out, i.e., set to zero, with a certain probability. This helps prevent overfitting by ensuring the model doesn't rely too heavily on any particular neuron.\n",
    "- **Early Stopping**: A technique where training stops if the validation loss stops improving for a specified number of epochs, helping to prevent overfitting.\n",
    "- **Batch Normalization**: This normalizes the output of each layer so that it has a mean of 0 and a standard deviation of 1. This helps speed up training and can improve performance.\n",
    "\n",
    "Let's see some of these regularization techniques in practice. We'll focus on a simple Feed-Forward Neural Network (FFNN) architecture for the MNIST classification problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0a0e07e-98be-4cfe-97ca-70292e46ae24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the MNIST dataset from TensorFlow\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize the pixel values from [0, 255] to [0, 1]\n",
    "# This is done by dividing the image pixel values by 255.0\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2372ff3e-d3e8-435e-a5c0-6bd96abfce7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_dir = destination_folder + \"/04_Regularization/reg_example_logs\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f711f8-c14c-4397-8b4b-c9afa18138cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential model using Keras\n",
    "model = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28)),  # Input layer, specifying the shape of the input (28x28 pixels)\n",
    "        Flatten(),              # Flatten the 2D input (28x28) into a 1D vector (784 elements)\n",
    "        Dense(512, activation='relu'),  # Fully connected layer with 512 neurons and ReLU activation\n",
    "        Dense(10, activation='softmax') # Output layer with 10 neurons (one for each class) and softmax activation for multi-class classification\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model: specify loss function, optimizer, and evaluation metric\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification\n",
    "    optimizer='adam',                        # Adam optimizer for efficient training\n",
    "    metrics=['accuracy'],                    # Use accuracy as the evaluation metric during training and testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e3e53a9-2206-426e-8174-eddd34b51dab",
   "metadata": {},
   "source": [
    "### L1 Regularization\n",
    "When defining a dense layer, we can specify the type of regularization to apply. In the case of **L1 regularization**, we add a penalty to the loss function based on the absolute values of the weights and biases. This penalty encourages sparsity by pushing some of the weights to zero, which helps to prevent overfitting by reducing model complexity.\n",
    "\n",
    "In Keras, you can specify L1 regularization using the `kernel_regularizer` and `bias_regularizer` parameters in the `Dense` layer. The **lambda** parameter (taking the value of `0.01` in the example below) controls the strength of the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920da56b-6b0d-4139-964b-602556951c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Sequential model with L1 regularization\n",
    "model_l1 = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28)),  # Input layer with shape (28, 28), representing MNIST images\n",
    "        Flatten(),              # Flatten the 2D image (28x28) into a 1D vector (784 elements)\n",
    "        \n",
    "        # First Dense layer with L1 regularization on both weights and biases\n",
    "        Dense(512, activation='relu',  # 512 neurons in the layer with ReLU activation\n",
    "              kernel_regularizer=tf.keras.regularizers.l1(0.01),  # Apply L1 regularization to weights (0.01 is the regularization strength)\n",
    "              bias_regularizer=tf.keras.regularizers.l1(0.01)),   # Apply L1 regularization to biases\n",
    "\n",
    "        # Second Dense layer (output layer) with L1 regularization\n",
    "        Dense(10, activation='softmax',  # Output layer with 10 neurons (for the 10 classes in MNIST)\n",
    "              kernel_regularizer=tf.keras.regularizers.l1(0.01),  # Apply L1 regularization to weights\n",
    "              bias_regularizer=tf.keras.regularizers.l1(0.01))    # Apply L1 regularization to biases\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Compile the model: specify loss function, optimizer, and evaluation metric\n",
    "model_l1.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification (labels are integers)\n",
    "    optimizer='adam',                        # Adam optimizer for efficient gradient descent\n",
    "    metrics=['accuracy'],                    # Track accuracy as the evaluation metric during training and testing\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d323ddf6-4496-4a3c-b468-05fa08497a22",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "When defining a dense layer, we can specify the type of regularization to apply. In the case of **L2 regularization**, we add a penalty to the loss function based on the squared values of the weights and biases. This penalty encourages smaller weights, which helps to prevent overfitting by reducing the complexity of the model.\n",
    "\n",
    "In Keras, you can specify L2 regularization using the `kernel_regularizer` and `bias_regularizer` parameters in the `Dense` layer. The **lambda** parameter (taking the value of `0.01` in the example below) controls the strength of the regularization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6231c45-ede1-452d-861e-2688a54225e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model_l2 = Sequential(\n",
    "    [\n",
    "        # Input layer: shape of each input is (28, 28), corresponding to the MNIST image size\n",
    "        Input(shape=(28, 28)), \n",
    "        \n",
    "        # Flatten the 2D input images into 1D vector (28*28 = 784 features)\n",
    "        Flatten(),\n",
    "        \n",
    "        # First Dense layer:\n",
    "        # 512 neurons, ReLU activation, L2 regularization applied on weights and biases\n",
    "        # L2 regularization with lambda = 0.01 on kernel (weights) and bias\n",
    "        Dense(512, activation='relu',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.01),   # L2 regularization on kernel (weights)\n",
    "              bias_regularizer=tf.keras.regularizers.l2(0.01)),    # L2 regularization on bias\n",
    "        \n",
    "        # Second Dense layer (output layer):\n",
    "        # 10 neurons for classification into 10 categories (digits 0-9), softmax activation\n",
    "        # L2 regularization applied on weights and biases\n",
    "        Dense(10, activation='softmax',\n",
    "              kernel_regularizer=tf.keras.regularizers.l2(0.01),   # L2 regularization on kernel (weights)\n",
    "              bias_regularizer=tf.keras.regularizers.l2(0.01))     # L2 regularization on bias\n",
    "    ])\n",
    "\n",
    "# The compile step specifies the model's training configuration\n",
    "# We use sparse_categorical_crossentropy for multi-class classification and Adam optimizer\n",
    "model_l2.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification (integer labels)\n",
    "    optimizer='adam',                        # Adam optimizer\n",
    "    metrics=['accuracy'],                    # We will track accuracy during training and evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf45b0bf-5abb-423d-914c-3913c4230e5e",
   "metadata": {},
   "source": [
    "### Dropout\n",
    "\n",
    "**Dropout** is a regularization technique used to prevent overfitting in neural networks. During training, it randomly \"drops\" (sets to zero) a fraction of the input units in a layer at each update step. This prevents the network from becoming overly reliant on any single feature or neuron, encouraging the network to learn more robust and generalized representations.\n",
    "\n",
    "In Keras, dropout is applied using the `Dropout` layer. The fraction of dropped units is controlled by the parameter `rate`, where a value of 0.5 means 50% of the units are dropped during training. Dropout is only active during training and does not affect the model's behavior during inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b40ec0-aa6f-4a59-ba57-e0a3a4b93971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Sequential model\n",
    "model_dropout = Sequential(\n",
    "    [\n",
    "        # Input layer: shape of each input is (28, 28), corresponding to the MNIST image size\n",
    "        Input(shape=(28, 28)), \n",
    "        \n",
    "        # Flatten the 2D input images into a 1D vector (28*28 = 784 features)\n",
    "        Flatten(),\n",
    "        \n",
    "        # First Dense layer:\n",
    "        # 512 neurons, ReLU activation function\n",
    "        # This layer learns a dense representation of the input features\n",
    "        Dense(512, activation='relu'),\n",
    "        \n",
    "        # Dropout layer: randomly sets 50% of the input units to 0 during training\n",
    "        # This helps prevent overfitting by reducing reliance on specific neurons\n",
    "        Dropout(0.5),\n",
    "        \n",
    "        # Second Dense layer (output layer):\n",
    "        # 10 neurons for classification into 10 categories (digits 0-9), softmax activation\n",
    "        # This layer outputs probabilities for each class\n",
    "        Dense(10, activation='softmax')\n",
    "    ])\n",
    "\n",
    "# The compile step specifies the model's training configuration\n",
    "# We use sparse_categorical_crossentropy for multi-class classification and Adam optimizer\n",
    "model_dropout.compile(\n",
    "    loss='sparse_categorical_crossentropy',  # Loss function for multi-class classification (integer labels)\n",
    "    optimizer='adam',                        # Adam optimizer\n",
    "    metrics=['accuracy'],                    # We will track accuracy during training and evaluation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6efa6a8-143e-4a8d-b52b-3dbe5bd45438",
   "metadata": {},
   "source": [
    "### Batch normalization\n",
    "### Batch Normalization (Ioffe and Szegedy, 2014)\n",
    "\n",
    "**Batch normalization** normalizes the activations of the previous layer for each batch. It applies a transformation to maintain the mean activation close to 0 and the activation standard deviation close to 1. This helps to stabilize and speed up the training process by reducing internal covariate shift.\n",
    "\n",
    "This layer is particularly useful in deep networks to improve performance and training speed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d956a6-bd90-4687-84d2-1b7c705b3d76",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "\n",
    "# Defining the model with BatchNormalization layers\n",
    "model_bn = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28)),  # Input layer, 28x28 pixel image\n",
    "        Flatten(),  # Flatten the 2D image into a 1D vector\n",
    "        BatchNormalization(),  # Apply BatchNormalization to the input\n",
    "        Dense(512, activation='relu'),  # Fully connected layer with ReLU activation\n",
    "        BatchNormalization(),  # Apply BatchNormalization after the dense layer\n",
    "        Dense(10, activation='softmax')  # Output layer with softmax activation for multi-class classification\n",
    "    ])\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "# The loss function used is sparse categorical cross-entropy since the labels are integers.\n",
    "model_bn.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',  # Adam optimizer for better training speed\n",
    "                  metrics=['accuracy'],  # Track accuracy during training\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10233d0-fe04-4b62-b6e9-4c929812cc90",
   "metadata": {},
   "source": [
    "### Early Stopping\n",
    "\n",
    "**Early stopping** is a technique used during training to prevent overfitting by monitoring the performance of the model on a validation dataset. When the monitored performance metric (such as validation loss or accuracy) stops improving for a specified number of epochs (called **patience**), training is halted.\n",
    "\n",
    "This technique helps to stop training before the model starts to overfit, saving computational resources and improving the model's generalization ability.\n",
    "\n",
    "#### Key Parameters:\n",
    "- **monitor**: The metric to monitor (e.g., `val_loss`, `val_accuracy`).\n",
    "- **patience**: The number of epochs with no improvement after which training will be stopped.\n",
    "- **restore_best_weights**: If `True`, the model weights from the epoch with the best performance will be restored.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8387ceb6-f22e-4ce6-b4b7-f7fdd755f63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping callback definition\n",
    "# Monitor the validation loss and stop training if it doesn't improve for 2 consecutive epochs\n",
    "ESCallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=2)\n",
    "\n",
    "# Define a simple Sequential model with fully connected layers\n",
    "model_es = Sequential(\n",
    "    [\n",
    "        Input(shape=(28, 28)),  # Input layer for 28x28 pixel images\n",
    "        Flatten(),  # Flatten the 2D input image into a 1D vector\n",
    "        Dense(512, activation='relu'),  # Fully connected layer with 512 units and ReLU activation\n",
    "        Dense(10, activation='softmax')  # Output layer with 10 units (one for each class) and softmax activation\n",
    "    ])\n",
    "\n",
    "# The compile step specifies the training configuration.\n",
    "# Using sparse categorical cross-entropy as the loss function because the labels are integers\n",
    "# Adam optimizer is used for faster convergence\n",
    "# Accuracy is tracked as a metric during training\n",
    "model_es.compile(loss='sparse_categorical_crossentropy',\n",
    "                  optimizer='adam',  # Adam optimizer for faster and more efficient training\n",
    "                  metrics=['accuracy'],  # Track accuracy during training\n",
    "                  )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895d8010-1562-4906-a224-fadd6e9b7317",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95076a5-cb6c-483d-a220-1bccea400249",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model without regularization, with TensorBoard logging enabled for visualization\n",
    "history = model.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                    validation_split=0.1,  # Use 10% of the training data for validation\n",
    "                    callbacks=[ \n",
    "                        tf.keras.callbacks.TensorBoard(\n",
    "                            log_dir=log_dir + r'\\no_regularization',  # Directory for TensorBoard logs\n",
    "                            histogram_freq=1,  # Enable histogram computation for activations\n",
    "                            write_graph=True,  # Save the graph for visualization\n",
    "                            write_images=False,  # Do not save images of the model weights\n",
    "                            update_freq='epoch'  # Update logs at each epoch\n",
    "                        )\n",
    "                    ], verbose=1)  # Show training progress\n",
    "\n",
    "# Training the model with L1 regularization and TensorBoard logging enabled for visualization\n",
    "history_l1 = model_l1.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                          validation_split=0.1,\n",
    "                          callbacks=[\n",
    "                              tf.keras.callbacks.TensorBoard(\n",
    "                                  log_dir=log_dir + r'\\l1_regularization',  # Directory for TensorBoard logs\n",
    "                                  histogram_freq=1,\n",
    "                                  write_graph=True,\n",
    "                                  write_images=False,\n",
    "                                  update_freq='epoch'\n",
    "                              )\n",
    "                          ], verbose=1)\n",
    "\n",
    "# Training the model with L2 regularization and TensorBoard logging enabled for visualization\n",
    "history_l2 = model_l2.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                          validation_split=0.1,\n",
    "                          callbacks=[\n",
    "                              tf.keras.callbacks.TensorBoard(\n",
    "                                  log_dir=log_dir + r'\\l2_regularization',  # Directory for TensorBoard logs\n",
    "                                  histogram_freq=1,\n",
    "                                  write_graph=True,\n",
    "                                  write_images=False,\n",
    "                                  update_freq='epoch'\n",
    "                              )\n",
    "                          ], verbose=1)\n",
    "\n",
    "# Training the model with Dropout regularization and TensorBoard logging enabled for visualization\n",
    "history_dropout = model_dropout.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                                    validation_split=0.1,\n",
    "                                    callbacks=[\n",
    "                                        tf.keras.callbacks.TensorBoard(\n",
    "                                            log_dir=log_dir + r'\\dropout_regularization',  # Directory for TensorBoard logs\n",
    "                                            histogram_freq=1,\n",
    "                                            write_graph=True,\n",
    "                                            write_images=False,\n",
    "                                            update_freq='epoch'\n",
    "                                        )\n",
    "                                    ], verbose=1)\n",
    "\n",
    "# Training the model with Batch Normalization regularization and TensorBoard logging enabled for visualization\n",
    "history_bn = model_bn.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                          validation_split=0.1,\n",
    "                          callbacks=[\n",
    "                              tf.keras.callbacks.TensorBoard(\n",
    "                                  log_dir=log_dir + r'\\batch_norm_regularization',  # Directory for TensorBoard logs\n",
    "                                  histogram_freq=1,\n",
    "                                  write_graph=True,\n",
    "                                  write_images=False,\n",
    "                                  update_freq='epoch'\n",
    "                              )\n",
    "                          ], verbose=1)\n",
    "\n",
    "# Training the model with Early Stopping regularization and TensorBoard logging enabled for visualization\n",
    "history_es = model.fit(x_train, y_train, batch_size=32, epochs=10,\n",
    "                       validation_split=0.1,\n",
    "                       callbacks=[\n",
    "                           tf.keras.callbacks.TensorBoard(\n",
    "                               log_dir=log_dir + r'\\early_stop_regularization',  # Directory for TensorBoard logs\n",
    "                               histogram_freq=1,\n",
    "                               write_graph=True,\n",
    "                               write_images=False,\n",
    "                               update_freq='epoch'\n",
    "                           ),\n",
    "                           ESCallback  # EarlyStopping callback to monitor validation loss\n",
    "                       ], verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2f17eec-f6a0-40f5-8412-cfd7ad39911f",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a26ea9-6f02-4b91-a76f-0b140f92ea80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the models on the test data and store the test accuracies for each model\n",
    "_, test_accuracy = model.evaluate(x_test, y_test)  # Model with no regularization\n",
    "_, test_accuracy_l1 = model_l1.evaluate(x_test, y_test)  # Model with L1 regularization\n",
    "_, test_accuracy_l2 = model_l2.evaluate(x_test, y_test)  # Model with L2 regularization\n",
    "_, test_accuracy_dropout = model_dropout.evaluate(x_test, y_test)  # Model with Dropout regularization\n",
    "_, test_accuracy_bn = model_bn.evaluate(x_test, y_test)  # Model with Batch Normalization regularization\n",
    "_, test_accuracy_es = model_es.evaluate(x_test, y_test)  # Model with Early Stopping regularization\n",
    "\n",
    "# Print the test accuracies for each model with proper formatting\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('no regularization', test_accuracy))\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('l1 regularization', test_accuracy_l1))\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('l2 regularization', test_accuracy_l2))\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('dropout regularization', test_accuracy_dropout))\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('batch norm regularization', test_accuracy_bn))\n",
    "print('{:<30}:\\t Test accuracy: {}'.format('early stopping regularization', test_accuracy_es))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
