{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09c5b3e7",
   "metadata": {},
   "source": [
    "**ML COURSE 2024-2025**\n",
    "# LAB8  NEURAL NETWORKS 2 \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b201f3",
   "metadata": {},
   "source": [
    "## Convolutional Neural Networks (CNNs) - usate principalmente per immagini\n",
    "\n",
    "Convolutional Neural Networks (CNNs) are built from several key layers:\n",
    "\n",
    "- **Convolutional layers**: These use sparse filters to capture spatial patterns, making them ideal for tasks like image recognition.\n",
    "- **Pooling layers**: Pooling reduces the sensitivity of the network to small translations in the input, ensuring that small shifts in the input don't dramatically affect the output.\n",
    "- **Dense layers**: These fully connected layers perform high-level reasoning after the convolutional and pooling layers.\n",
    "\n",
    "The [LeNet architecture](https://ieeexplore.ieee.org/abstract/document/726791) is often considered the foundation of CNNs. In this project, we’ll design a network inspired by LeNet's structure. We will design it using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8701cbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87556036",
   "metadata": {},
   "source": [
    "### Fashion-MNIST\n",
    "\n",
    "**Fashion-MNIST** is a benchmark dataset designed as a more challenging drop-in replacement for the original MNIST handwritten digits dataset. It contains grayscale images of fashion products from 10 categories, aiming to better represent modern computer vision tasks.\n",
    "\n",
    "- **Number of samples**: 70,000 (60,000 training, 10,000 test)\n",
    "- **Image size**: 28×28 pixels\n",
    "- **Color channels**: Grayscale (1 channel)\n",
    "- **Number of classes**: 10\n",
    "- **Categories**:\n",
    "\n",
    "  0. T-shirt/top  \n",
    "  1. Trouser  \n",
    "  2. Pullover  \n",
    "  3. Dress  \n",
    "  4. Coat  \n",
    "  5. Sandal  \n",
    "  6. Shirt  \n",
    "  7. Sneaker  \n",
    "  8. Bag  \n",
    "  9. Ankle boot\n",
    "\n",
    "Fashion-MNIST serves as a more realistic alternative to MNIST, offering similar ease of use while presenting a greater challenge for modern algorithms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac274783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Fashion MNIST instead of MNIST\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Normalize (scale) the pixel values to [0, 1]\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcfe4d32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Count the number of occurrences of each digit in the training labels\n",
    "train_class_counter = Counter(y_train) #dictionary with keys as digits and values as counts\n",
    "\n",
    "# Create a new figure and plot a bar chart for the training set class distribution\n",
    "plt.figure()\n",
    "plt.bar(train_class_counter.keys(), train_class_counter.values())\n",
    "plt.title('Class distribution in training dataset')  # Title for the plot\n",
    "\n",
    "# Count the number of occurrences of each digit in the test labels\n",
    "test_class_counter = Counter(y_test)\n",
    "\n",
    "# Create a new figure and plot a bar chart for the test set class distribution\n",
    "plt.figure()\n",
    "plt.bar(test_class_counter.keys(), test_class_counter.values())\n",
    "plt.title('Class distribution in test dataset')  # Title for the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e180b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print data type and shape information for training and test sets\n",
    "print('Class of x_train: {}'.format(type(x_train)))\n",
    "print('Class of y_train: {}'.format(type(y_train)))  \n",
    "print('Shape of x_train: {}'.format(x_train.shape))  \n",
    "print('Shape of y_train: {}'.format(y_train.shape)) \n",
    "print('Shape of x_test: {}'.format(x_test.shape))    \n",
    "print('Shape of y_test: {}'.format(y_test.shape))    \n",
    "\n",
    "# Visualize 10 random samples from the training set on 2 rows\n",
    "idxes = [np.random.randint(len(x_train)) for _ in range(10)]\n",
    "plt.figure(figsize=(15, 6))  # Adjusted size for 2 rows\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "for i, idx in enumerate(idxes):\n",
    "    plt.subplot(2, 5, i + 1)  # 2 rows, 5 columns\n",
    "    plt.imshow(x_train[idx].squeeze(), cmap='Greys')\n",
    "    plt.title(f\"{class_names[y_train[idx]]} (label = {y_train[idx]})\", fontsize=9)\n",
    "\n",
    "plt.suptitle(\"Random Samples from Fashion-MNIST Training Set\", fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c3e268",
   "metadata": {},
   "source": [
    "## Model Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a92e29",
   "metadata": {},
   "source": [
    "We will now define our Convolutional Neural Network (CNN) using Keras’ `Sequential` API. The model consists of several key components:\n",
    "\n",
    "- **Convolutional layers (`Conv2D`)**  \n",
    "  These layers apply learnable filters to extract spatial features from the input images. For example, the first convolutional layer applies 6 filters of size 3×3 to the 28×28 grayscale input images.\n",
    "\n",
    "- **Activation functions (`ReLU`)**  \n",
    "  After each convolution, the ReLU (Rectified Linear Unit) activation is applied to introduce non-linearity into the model, allowing it to learn complex patterns.\n",
    "\n",
    "- **Pooling layers (`MaxPooling2D`)**  \n",
    "  These downsample the feature maps by taking the maximum value in a local window (usually 2×2). This reduces spatial dimensions and helps make the model more robust to shifts and distortions in the input.\n",
    "\n",
    "- **Flatten layer**  \n",
    "  After the convolutional and pooling layers, the 3D output is flattened into a 1D vector so it can be passed to fully connected (dense) layers.\n",
    "\n",
    "- **Dense (fully connected) layers**  \n",
    "  • The first dense layer contains 128 neurons with ReLU activation to learn complex representations.  \n",
    "  • The final dense layer contains 10 neurons with softmax activation, which outputs a probability distribution over the 10 Fashion MNIST classes.\n",
    "\n",
    "- **Compilation step**  \n",
    "  • **Loss function**: `sparse_categorical_crossentropy` is used because the target labels are integers (not one-hot encoded).  \n",
    "  • **Optimizer**: `adam` is an adaptive learning rate optimizer known for good performance.  \n",
    "  • **Metric**: We track accuracy during training and evaluation.\n",
    "\n",
    "This model structure is well-suited for image classification tasks like Fashion MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b81da1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Convolution2D, MaxPool2D, Flatten, Dense\n",
    "\n",
    "def create_CNN_model(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # NEW: 1st Convolutional Layer\n",
    "    # Applies 32 filters of size 3x3, followed by ReLU activation\n",
    "    # Input shape is (28, 28, 1) for grayscale images (like Fashion MNIST)\n",
    "    model.add(Convolution2D(32, kernel_size=(3, 3),\n",
    "                    activation='relu',\n",
    "                    # kernel_initializer='he_normal',\n",
    "                    input_shape=(28, 28, 1)))\n",
    "    \n",
    "    # NEW: 1st Max Pooling Layer\n",
    "    # Reduces spatial dimensions (typically by 2x)\n",
    "    model.add(MaxPool2D((2, 2)))\n",
    "\n",
    "    # NEW: 2nd Convolutional Layer\n",
    "    # Applies 64 filters of size 3x3\n",
    "    # followed by ReLU activation\n",
    "    model.add(Convolution2D(64, \n",
    "                    kernel_size=(3, 3), \n",
    "                    activation='relu'))\n",
    "    \n",
    "    # NEW: 2nd Max Pooling Layer\n",
    "    model.add(MaxPool2D(pool_size=(2, 2)))\n",
    "\n",
    "    # NEW: 3rd Convolutional Layer\n",
    "    # Applies 128 filters of size 3x3\n",
    "    # followed by ReLU activation\n",
    "    model.add(Convolution2D(128, (3, 3), activation='relu'))\n",
    "\n",
    "    # As the last time\n",
    "    # Flatten images to 1D vectors\n",
    "    model.add(Flatten())\n",
    "\n",
    "    # Fully Connected (Dense) Layer with 128 neurons and ReLU activation\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    # Output Layer: 10 units (for 10 classes) with softmax activation\n",
    "    # Softmax outputs probabilities for multi-class classification\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "\n",
    "    # Compile the model\n",
    "    # - Loss: sparse categorical crossentropy (for integer labels)\n",
    "    # - Optimizer: Adam (adaptive gradient-based optimizer)\n",
    "    # - Metric: accuracy (to track during training/validation)\n",
    "    model.compile(optimizer='adam',\n",
    "                    loss='sparse_categorical_crossentropy',\n",
    "                    metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e2069fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an instance of the model\n",
    "cnn_model = create_CNN_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad703c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the original shape of the training data (x_train) before adding the channel dimension.\n",
    "print('Original shape of training data: {}'.format(x_train.shape))\n",
    "\n",
    "# Add a channel dimension to the training data to prepare it for a CNN, which expects 4D input (samples, height, width, channels).\n",
    "# np.expand_dims(x_train, axis=3) adds an extra dimension along the last axis (axis=3)\n",
    "# The result is the data in the correct shape for input into a convolutional neural network.\n",
    "print('Addition of channel dimension results in shape {}'.format(np.expand_dims(x_train, axis=3).shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe5c09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from google.colab import drive\n",
    "#drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3a61c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "logdir = 'drive/MyDrive/02_Callbacks'\n",
    "if not os.path.exists(logdir):\n",
    "    os.mkdir(logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebaead3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EARLYSTOPPING CALLBACK: stops training when the validation loss has not improved for a given number of epochs (patience).\n",
    "# 'monitor' specifies which metric to track (here, 'val_loss' which is the validation loss).\n",
    "# 'patience' specifies how many epochs to wait before stopping after no improvement.\n",
    "ES_callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# TENSORBOARD CALLBACK: allows visualization of training progress via TensorBoard.\n",
    "# 'log_dir' specifies where the logs will be saved for later visualization.\n",
    "# 'write_graph' ensures the computation graph of the model is saved for visualization.\n",
    "# 'update_freq' specifies how often to log (here, it logs after each epoch).\n",
    "TB_callback = tf.keras.callbacks.TensorBoard(\n",
    "    log_dir=logdir+'/tb_logs',  # Save logs to the 'tb_logs' directory inside the log directory\n",
    "    write_graph=True,           # Save the computation graph for TensorBoard visualization\n",
    "    update_freq='epoch'        # Log every epoch\n",
    ")\n",
    "\n",
    "# MODELCHECKPOINT CALLBACK callback: saves the model's weights during training.\n",
    "# 'filepath' specifies where the weights will be saved, using a format that includes the epoch number and validation loss.\n",
    "# 'monitor' tells the callback to monitor 'val_loss' (validation loss) to decide when to save weights.\n",
    "# 'save_best_only' is False, meaning it will save weights after every epoch, not just when 'val_loss' improves.\n",
    "# 'save_weights_only' ensures only the weights (not the full model) are saved.\n",
    "# 'save_freq' is set to 'epoch' so that weights are saved after every epoch.\n",
    "MC_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=logdir + '/weights.{epoch:02d}-{val_loss:.2f}.weights.h5',  # Save weights in the correct format (.weights.h5)\n",
    "    monitor='val_loss',  # Track validation loss to decide when to save the weights\n",
    "    verbose=0,  # No output during saving\n",
    "    save_best_only=False,  # Save after every epoch, not just when the validation loss improves\n",
    "    save_weights_only=True,  # Only the weights (not the entire model) will be saved\n",
    "    mode='auto',  # Automatically selects the mode based on the 'monitor' value (in this case, 'val_loss')\n",
    "    save_freq='epoch'  # Save the weights after each epoch\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f508e246",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the CNN model on the training data (x_train, y_train) using the fit method.\n",
    "# Before feeding the data to the convolutional model, we add an extra dimension to x_train for the channel (axis=3), \n",
    "# as Conv2D expects input data in the shape of (samples, height, width, channels).\n",
    "# np.expand_dims(x_train, axis=3) adds a channel dimension of size 1 to each image, making the shape (60000, 28, 28, 1).\n",
    "history_cnn = cnn_model.fit(np.expand_dims(x_train,axis=3),y_train,epochs=10,validation_split=0.1,callbacks=[ES_callback,TB_callback,MC_callback],batch_size=1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa2a687b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import matplotlib to plot the training history of the CNN model.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a figure with a specific size (15x6 inches) for the plot.\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Plot the training accuracy over the epochs from the history of the model.\n",
    "# 'history_cnn.history['accuracy']' contains the training accuracy for each epoch.\n",
    "# 'label='training'' is used to add a label to the plot for the training accuracy curve.\n",
    "plt.plot(history_cnn.history['accuracy'], label='training')\n",
    "\n",
    "# Plot the validation accuracy over the epochs from the history of the model.\n",
    "# 'history_cnn.history['val_accuracy']' contains the validation accuracy for each epoch.\n",
    "# 'label='validation'' is used to add a label to the plot for the validation accuracy curve.\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='validation')\n",
    "\n",
    "# Set the title of the plot to 'CNN'.\n",
    "plt.title('CNN')\n",
    "\n",
    "# Add a legend to the plot to label the training and validation curves.\n",
    "plt.legend()\n",
    "\n",
    "# Display a grid on the plot for better readability of the values.\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f18a506c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logdir+'/tb_logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f703e822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Launch from terminal:\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir \"drive/MyDrive/02_Callbacks/tb_logs\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aff9ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions from the model on the test data\n",
    "# The model outputs probabilities for each class for each test sample\n",
    "predictions = cnn_model.predict(x_test)\n",
    "\n",
    "# Convert the probabilities into predicted class labels\n",
    "# np.argmax returns the index of the maximum probability for each sample\n",
    "# axis=1 indicates we are selecting the index along each row (per sample)\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34103093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find mismatches\n",
    "mismatch = np.where(y_pred != y_test)[0]\n",
    "mismatch_class_counter = Counter(y_test[mismatch])\n",
    "mismatch_percentage = {}\n",
    "\n",
    "# Compute mismatch percentage per class\n",
    "for digit in test_class_counter.keys():\n",
    "    mismatch_percentage[digit] = mismatch_class_counter[digit] / test_class_counter[digit] * 100\n",
    "\n",
    "# Create bar chart\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.bar(mismatch_percentage.keys(), mismatch_percentage.values())\n",
    "\n",
    "# Set x-axis labels to class names\n",
    "plt.xticks(ticks=list(mismatch_percentage.keys()), labels=[class_names[d] for d in mismatch_percentage.keys()], rotation=45)\n",
    "\n",
    "plt.title('Class-wise Mismatch Percentage')\n",
    "plt.ylabel('Mismatch Percentage (%)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e48148ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Compute the confusion matrix and take log for better visualization\n",
    "cm = np.log(1 + confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Create figure and axis\n",
    "fig, ax = plt.subplots(figsize=(8, 8))\n",
    "\n",
    "# Plot heatmap\n",
    "im = ax.imshow(cm, interpolation='nearest', cmap='Reds')\n",
    "\n",
    "# Add colorbar\n",
    "fig.colorbar(im, ax=ax)\n",
    "\n",
    "# Set axis labels\n",
    "ax.set_xlabel('Predicted labels')\n",
    "ax.set_ylabel('True labels')\n",
    "\n",
    "# Set tick marks and labels to class names\n",
    "ax.set_xticks(np.arange(len(class_names)))\n",
    "ax.set_yticks(np.arange(len(class_names)))\n",
    "\n",
    "ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "# Optional: add gridlines for readability\n",
    "ax.grid(False)\n",
    "\n",
    "plt.title('Log-Scaled Confusion Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09717046",
   "metadata": {},
   "source": [
    "# Comparison with Feedforward Neural Network\n",
    "Let's circle back to the Sequential model used in the last lab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e4c179",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- KERAS ---\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
    "\n",
    "\n",
    "def create_fnn_model(compile_model=False):\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28)),              # input layer specifying the input shape           \n",
    "        Flatten(),                          \n",
    "        Dense(512, activation='relu'),      # dense layer with ReLU activation included \n",
    "        Dropout(0.2),                       \n",
    "        Dense(10, activation='softmax')     # output layer with softmax activation   \n",
    "    ])\n",
    "    \n",
    "    if compile_model:                       # we can add training parameters directly to the model\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',  \n",
    "            optimizer='adam',                        \n",
    "            metrics=['accuracy']                     \n",
    "        )\n",
    "    \n",
    "    return model  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c6321f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the FNN model\n",
    "fnn_model = create_fnn_model(compile_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5258155f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the FNN model\n",
    "history_fnn = fnn_model.fit(\n",
    "    np.expand_dims(x_train, axis=3),  # Add the channel dimension (28, 28, 1)\n",
    "    y_train,\n",
    "    epochs=10,  # Train for 10 epochs\n",
    "    validation_split=0.1,  # Use 10% of the training data for validation\n",
    "    batch_size=1024  # Set the batch size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e815df5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training and validation accuracy for both the CNN and FNN\n",
    "\n",
    "plt.figure(figsize=(15,6))\n",
    "\n",
    "# Plot the CNN accuracy\n",
    "plt.plot(history_cnn.history['accuracy'], label='CNN Training Accuracy')\n",
    "plt.plot(history_cnn.history['val_accuracy'], label='CNN Validation Accuracy')\n",
    "\n",
    "# Plot the FNN accuracy\n",
    "plt.plot(history_fnn.history['accuracy'], label='FNN Training Accuracy')\n",
    "plt.plot(history_fnn.history['val_accuracy'], label='FNN Validation Accuracy')\n",
    "\n",
    "# Set the title and labels\n",
    "plt.title('Comparison of CNN and FNN Performance')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164554fa",
   "metadata": {},
   "source": [
    "**NOTE**: When the images are in color, particularly with a higher number of training epochs, the performance improvement of the CNN over the FNN becomes more pronounced."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be1b7a7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Exercise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f122ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras import datasets, layers, models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b12e60",
   "metadata": {},
   "source": [
    "Implement a CNN and test it on CIFAR10 dataset using Keras\n",
    "\n",
    "Credits and solutions: [https://www.tensorflow.org/tutorials/images/cnn?hl=it](https://www.tensorflow.org/tutorials/images/cnn?hl=it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2c51eb",
   "metadata": {},
   "source": [
    "**Download and prepare the CIFAR10 dataset**\n",
    "\n",
    "The CIFAR10 dataset contains 60,000 color images in 10 classes, with 6,000 images in each class. The dataset is divided into 50,000 training images and 10,000 testing images. The classes are mutually exclusive and there is no overlap between them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6014c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images/255.0, test_images/255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e606be1",
   "metadata": {},
   "source": [
    "**Verify the data**\n",
    "\n",
    "To verify that the dataset looks correct, let's plot the first 25 images from the training set and display the class name below each image:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b554abd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
    "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "for i in range(25):\n",
    "    plt.subplot(5,5,i+1)\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(...)\n",
    "    # The CIFAR labels happen to be arrays, \n",
    "    # which is why you need the extra index\n",
    "    plt.xlabel(class_names[train_labels[i][0]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339c9cad",
   "metadata": {},
   "source": [
    "**Create the convolutional base and the dense layer on the top**\n",
    "\n",
    "Create a model with: \n",
    "- Conv2D-MaxPooling2D-Conv2D-MaxPoolin2D-Conv2D\n",
    "- Then,Properly add the dense layers on top, for classification. \n",
    "\n",
    "Compile the mode using usual arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e9117d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(layer.Convolution2D(32, kernel_size = (3, 3), activation='relu', input_shape=(32, 32, 3)))\n",
    "model.add(layer.MaxPool2D(2,2))\n",
    "model.add(layer.Convolution2D(64, kernel_size=(3,3), activation='relu'))\n",
    "model.add(layer.MaxPool2D(2,2))\n",
    "model.add(layer.Convolution2D(128, kernel_size= (3,3), activation='relu'))\n",
    "model.add(layer.Flatten())\n",
    "model.add(layer.Dense(64, activation='relu'))\n",
    "model.add(layer.Dense(10, activation='softmax'))\n",
    "\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68006e9a",
   "metadata": {},
   "source": [
    "Visualize the model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad997d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()\n",
    "tf.keras.utils.plot_modek(cnn_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922eaad7",
   "metadata": {},
   "source": [
    "Fit the model using 10 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfc58c",
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(train_images, train_labels, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4f311c",
   "metadata": {},
   "source": [
    "Plot training curves on training and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8949aa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train accuracy\n",
    "plt.plot(history.history['accuracy'], label='accuracy')\n",
    "# validation accuracy\n",
    "plt.plot(history.history['val_accuracy'], label = 'val_accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.ylim([0.5, 1])\n",
    "plt.legend(loc='lower right')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04630e9",
   "metadata": {},
   "source": [
    "Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f48ab6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels, verbose= 2)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2fbd9f6",
   "metadata": {},
   "source": [
    "## Part 2: Keras vs PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75969c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "774add80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load MNIST dataset ---\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0942d27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Load MNIST dataset ---\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "print(type(x_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e199e0",
   "metadata": {},
   "source": [
    "### Model definition: Sequential API (Keras vs Torhc)\n",
    "\n",
    "In Lab 7, we saw how to define models using the `Sequential` API in Keras. PyTorch also has a similar concept with the `Sequential` container, but there are important differences between the two. Let’s explore both.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aad31f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL DEFINITION WITH KERAS ---\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Flatten, Dense, Dropout, Input\n",
    "\n",
    "\n",
    "def create_model_keras(compile_model=False):\n",
    "    model = Sequential([\n",
    "        Input(shape=(28, 28)),              # input layer SPECYFING THE INPUT shape           \n",
    "        Flatten(),                          \n",
    "        Dense(512, activation='relu'),      # dense layer with ReLU activation included \n",
    "        Dropout(0.2),                       \n",
    "        Dense(10, activation='softmax')     # output layer with softmax activation   \n",
    "    ])\n",
    "    \n",
    "    if compile_model:                       # we can add training parameters directly to the model\n",
    "        model.compile(\n",
    "            loss='sparse_categorical_crossentropy',  \n",
    "            optimizer='adam',                        \n",
    "            metrics=['accuracy']                     \n",
    "        )\n",
    "    \n",
    "    return model  \n",
    "\n",
    "model_keras = create_model_keras(compile_model=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c06676e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- MODEL DEFINITION WITH PYTORCH ---\n",
    "from torch.nn import Sequential, Flatten, Linear, ReLU, Dropout\n",
    "from torch.optim import Adam\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "def create_model_pytorch(): \n",
    "    # Define a sequential model \n",
    "    model = Sequential( \n",
    "        # NO need of explicit input layer in PyTorch\n",
    "        Flatten(),                                                                 \n",
    "        Linear(in_features=784, out_features=512),        # Linear (Dense equivalent). Specify both input and output size\n",
    "        ReLU(),                                           # Activation in a separate layer!\n",
    "        Dropout(0.2),                                       \n",
    "        Linear(in_features=512, out_features=10),         \n",
    "                                                          # NO softmax here! The loss function CrossEntropyLoss() will apply softmax internally!! \n",
    "    )\n",
    "\n",
    "    #NOTE: no compile method in PyTorch. We will use different objects for loss and optimizer.\n",
    "    return model\n",
    "\n",
    "model_torch = create_model_pytorch()\n",
    "optimizer = Adam(model_torch.parameters())          # Careful: optimizer and loss are not part of the model in PyTorch\n",
    "loss_fn = CrossEntropyLoss()                        # CrossEntropyLoss applies softmax internally, expects integers labels (not one-hot encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77fa20a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7e76697",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Keras --\n",
    "model_keras.fit(x_train, y_train, epochs=5, batch_size=32, validation_split=0.1)        # very straightforward fit method. accept numpy arrays"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87566970",
   "metadata": {},
   "source": [
    "In PyTorch we need to manually handle: \n",
    "- mini batch \n",
    "- train-validation split \n",
    "- evaluation (on both train set and val set)\n",
    "\n",
    "Moreover, PyTorch requires torch.tensor and not numpy.array!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76a9f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PyTorch - Dà PIù CONTROLLO SU OGNI STEP DEL PROCESSO --\n",
    "# --- 1: Without batching (i.e., full-batch gradient descent) ---\n",
    "\n",
    "# Convert training data (NumPy arrays) to PyTorch tensors\n",
    "x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)  # CrossEntropyLoss expects targets as Long tensors\n",
    "\n",
    "# Split data into training and validation sets\n",
    "val_split = 0.1\n",
    "x_train_train, x_train_val, y_train_train, y_train_val = train_test_split(\n",
    "    x_train_tensor, y_train_tensor, test_size=val_split\n",
    ")\n",
    "\n",
    "# Set number of training epochs\n",
    "epochs = 5 \n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs): \n",
    "    model_torch.train()                             # Set model to training mode (enables dropout, batch norm, etc.)\n",
    "    optimizer.zero_grad()                           # Clear previous gradients (from last step)\n",
    "\n",
    "    y_pred_logits = model_torch(x_train_train)      # FORWARD PASS: compute raw model outputs (logits)\n",
    "    loss = loss_fn(y_pred_logits, y_train_train)    # Compute loss between predicted and true labels\n",
    "    loss.backward()                                 # Backward pass: compute gradients w.r.t. model parameters\n",
    "    optimizer.step()                                # Update model weights based on gradients\n",
    "\n",
    "    # Calculate training accuracy (no gradient tracking needed here)\n",
    "    with torch.no_grad(): \n",
    "        train_preds = torch.argmax(y_pred_logits, dim=1)             # Convert logits to predicted classes\n",
    "        train_correct = (train_preds == y_train_train).sum().item()  # Count correct predictions\n",
    "        train_acc = train_correct / y_train_train.size(0)            # Compute accuracy\n",
    "\n",
    "    # Validation phase (disable dropout, batch norm)\n",
    "    model_torch.eval()                         \n",
    "    with torch.no_grad():                      # Don't compute gradients during evaluation\n",
    "        y_pred_logits_val = model_torch(x_train_val)                    # Forward pass on validation data\n",
    "        val_loss = loss_fn(y_pred_logits_val, y_train_val)             # Compute validation loss\n",
    "        val_preds = torch.argmax(y_pred_logits_val, dim=1)             # Predicted class labels\n",
    "        val_correct = (val_preds == y_train_val).sum().item()          # Count correct predictions\n",
    "        val_acc = val_correct / y_train_val.size(0)                    # Compute validation accuracy\n",
    "\n",
    "    # Print metrics for this epoch\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {loss.item():.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "          f\"Val Loss: {val_loss.item():.4f} | Val Acc: {val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156c590",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PyTorch --\n",
    "# --- 2. With batching (mini-batch gradient descent) ---\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Create model, optimizer, and loss function\n",
    "model_torch = create_model_pytorch()\n",
    "optimizer = Adam(model_torch.parameters())  # In PyTorch, optimizer is defined separately\n",
    "loss_fn = CrossEntropyLoss()                # CrossEntropyLoss includes softmax internally; expects integer (not one-hot) labels\n",
    "\n",
    "# Convert data to tensors\n",
    "x_data_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "y_data_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "# Train-validation split\n",
    "val_split = 0.1\n",
    "x_train_tensor, x_val_tensor, y_train_tensor, y_val_tensor = train_test_split(\n",
    "    x_data_tensor, y_data_tensor, test_size=val_split, random_state=42\n",
    ")\n",
    "\n",
    "# --- Create Datasets and DataLoaders ---\n",
    "batch_size = 32  # Define mini-batch size (similar to Keras)\n",
    "\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)  # No need to shuffle validation data\n",
    "\n",
    "# Debug print to check number of batches per epoch\n",
    "print(f\"PyTorch training batches per epoch: {len(train_loader)}\")\n",
    "print(f\"PyTorch validation batches per epoch: {len(val_loader)}\")\n",
    "\n",
    "# --- PyTorch Training Loop with Mini-Batches and Validation ---\n",
    "print(\"\\n--- PyTorch Training ---\")\n",
    "epochs = 5\n",
    "for epoch in range(epochs):\n",
    "    # --- Training Phase - iteriamo su più batch in ogni epoca ---\n",
    "    model_torch.train()  # Set model to training mode (enables dropout, batch norm, etc.)\n",
    "    running_train_loss = 0.0\n",
    "    train_correct = 0\n",
    "    train_total = 0\n",
    "\n",
    "    # ! Iterate over training mini-batches (ovvero sul dataloader)\n",
    "    for batch_idx, (inputs, labels) in enumerate(train_loader):\n",
    "        # Reset gradients from previous step\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_torch(inputs)\n",
    "        loss = loss_fn(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Accumulate training loss and correct predictions\n",
    "        running_train_loss += loss.item() * inputs.size(0)  # Sum loss weighted by batch size\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        train_total += labels.size(0)\n",
    "        train_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute average loss and accuracy for training epoch\n",
    "    epoch_train_loss = running_train_loss / train_total\n",
    "    epoch_train_acc = train_correct / train_total\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model_torch.eval()  # Set model to evaluation mode (disables dropout, etc.)\n",
    "    running_val_loss = 0.0\n",
    "    val_correct = 0\n",
    "    val_total = 0\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient tracking for evaluation\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model_torch(inputs)\n",
    "            loss = loss_fn(outputs, labels)\n",
    "\n",
    "            running_val_loss += loss.item() * inputs.size(0)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            val_total += labels.size(0)\n",
    "            val_correct += (predicted == labels).sum().item()\n",
    "\n",
    "    # Compute average validation loss and accuracy\n",
    "    epoch_val_loss = running_val_loss / val_total\n",
    "    epoch_val_acc = val_correct / val_total\n",
    "\n",
    "    # --- Epoch Summary ---\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}] | \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_acc:.4f} | \"\n",
    "          f\"Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_acc:.4f}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57fdd0a9",
   "metadata": {},
   "source": [
    "As you can see, PyTorch requires significantly more code compared to Keras.\n",
    "However, the training process is much clearer and more flexible for the user to adjust.\n",
    "\n",
    "As we mentioned in the previous lecture, Keras has the advantage of simplicity,\n",
    "while PyTorch excels in debugging and flexibility, making it more suitable for research purposes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f415ec07",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e979bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- Keras --\n",
    "# Evaluate the model on the test set (returns loss and accuracy)\n",
    "test_loss, test_acc = model_keras.evaluate(x_test, y_test, verbose=0)\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n",
    "\n",
    "# Make predictions on the test set\n",
    "# The output is a probability distribution over classes for each sample\n",
    "predictions = model_keras.predict(x_test)\n",
    "\n",
    "# Convert predicted probabilities to class labels\n",
    "y_pred = np.argmax(predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df27f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- PyTorch --\n",
    "model_torch.eval()  # Set the model to evaluation mode (disables dropout, batch norm, etc.)\n",
    "\n",
    "# Convert test data to tensors\n",
    "x_test_tensor = torch.tensor(x_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)\n",
    "\n",
    "# Forward pass to get raw predictions (logits). \n",
    "#Con no_grad() calcolo le predizioni su cui calcolare l'attivazione per ottenere le labels\n",
    "with torch.no_grad():  # Disable gradient tracking for evaluation\n",
    "    y_pred_test_logits = model_torch(x_test_tensor)\n",
    "\n",
    "# Convert logits to predicted class labels\n",
    "y_pred_test = torch.argmax(y_pred_test_logits, dim=1)\n",
    "\n",
    "# Compute test loss\n",
    "test_loss = loss_fn(y_pred_test_logits, y_test_tensor).item()\n",
    "\n",
    "# Compute test accuracy\n",
    "test_correct = (y_pred_test == y_test_tensor).sum().item()\n",
    "test_acc = test_correct / y_test_tensor.size(0)\n",
    "\n",
    "print(f\"Test Loss: {test_loss:.4f} | Test Acc: {test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e058f3",
   "metadata": {},
   "source": [
    "### Save and load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295b6e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "destination_folder = \"./drive/MyDrive/AA24-25ML/LAB8/\"\n",
    "\n",
    "# -- Keras --\n",
    "model_keras.save(destination_folder + \"model_keras.h5\")\n",
    "model_keras_loaded = tf.keras.models.load_model(destination_folder + \"model_keras.h5\") \n",
    "\n",
    "# -- PyTorch --\n",
    "torch.save(model_torch, destination_folder + \"model_torch.pth\") \n",
    "model_torch_loaded = torch.load(destination_folder + \"model_torch.pth\", weights_only=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27b3051",
   "metadata": {},
   "source": [
    "### Custom Model Definition in PyTorch\n",
    "\n",
    "In the last lab, we saw how Keras allows for custom model creation by subclassing `tf.keras.Model` and implementing the `call()` method to define the forward pass.\n",
    "\n",
    "PyTorch follows a similar approach!\n",
    "\n",
    "To define a custom model in PyTorch, you subclass `torch.nn.Module` and implement two key methods:\n",
    "\n",
    "- `__init__()` – defines the layers and architecture of the model.\n",
    "- `forward()` – specifies the forward pass (equivalent to `call()` in Keras), i.e., how data flows through the model.\n",
    "\n",
    "This structure gives you full flexibility to design complex models and control the computation process.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48dbaee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class MyModel(nn.Module): \n",
    "    def __init__(self): \n",
    "        super().__init__()\n",
    "        self.flatten = Flatten()\n",
    "        self.linear1 = Linear(784, 512)\n",
    "        self.relu = ReLU()\n",
    "        self.dropout = Dropout(0.2)\n",
    "        self.linear2 = Linear(512, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "model_torch = MyModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
