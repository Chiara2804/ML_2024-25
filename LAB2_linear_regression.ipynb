{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1GgpK5JMu7y8"
      },
      "source": [
        "**ML COURSE 2024-2025**\n",
        "# LAB2: Linear / Ridge Regression and Cross-Validation\n",
        "\n",
        "#### Summary\n",
        "- **Linear Regression**\n",
        "    - MSE and $R^2$ score\n",
        "    - Feature augmentation\n",
        "- **Ridge Regression**\n",
        "    - Normalization and Unnormalization\n",
        "- **Cross-Validation**\n",
        "    - K-Fold Cross-Validation\n",
        "    - Monte-Carlo Cross-Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j6uvWpshu7y-"
      },
      "source": [
        "## Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_pP8cPBhu7y_"
      },
      "source": [
        "In this notebook, we will implement linear regression from scratch using numpy.\n",
        "\n",
        "> Linear regression: find the best line that fits the data.\n",
        "\n",
        "**But how do we quantify what \"best\" means?**\n",
        "\n",
        "We need a way to mathematically \"judge\" how well our linear model fits the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "YiqaGeSDu7y_",
        "outputId": "49c7eab5-8c3a-4a00-eb9e-937210f6b1c5"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Make the random generation fixed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generate data with a noisy linear function\n",
        "x = np.linspace(0, 1, 20)\n",
        "y = x + np.random.normal(0, 0.1, 20)\n",
        "\n",
        "line1 = 0.1 * x\n",
        "line2 = 0.5 * x\n",
        "\n",
        "plt.scatter(x, y)\n",
        "plt.plot(x, line1, \"r\")\n",
        "plt.plot(x, line2, \"g\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9RW26nyu7zB"
      },
      "source": [
        "Is the red line better than the green line? We need a mathematical way to answer this question.\n",
        "\n",
        "> To evaluate how well our linear model fits the data, we will use the *mean squared error (MSE)* loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 259
        },
        "id": "KzBngF4ou7zB",
        "outputId": "9898c842-3345-4254-a66e-07fa0dd56d54"
      },
      "outputs": [],
      "source": [
        "from utils import plot_mse\n",
        "\n",
        "# Check the MSE of different lines\n",
        "line_1 = 0.1 * x\n",
        "line_2 = 0.5 * x\n",
        "line_3 = 0.9 * x\n",
        "\n",
        "plot_mse(x, y, line_1, line_2, line_3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAtbEG0iu7zC"
      },
      "source": [
        "The slope is one of the parameters of a linear model, and we can add an offset too:\n",
        "\n",
        "$$\\hat{y} = mx + b$$\n",
        "\n",
        "Where:\n",
        "- $\\hat{y}$ is the target variable (with $\\hat{\\;}$ because it's the model's prediction)\n",
        "- $x$ is the input variable\n",
        "- $m$ is the slope\n",
        "- $b$ is the offset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNPl5lLpu7zC"
      },
      "source": [
        "As you can see, as we change the parameters of our model, the error changes.\n",
        "\n",
        "The formula we used to evaluate how well the model fits the data is the following:\n",
        "\n",
        "$$\n",
        "MSE = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "where:\n",
        "- $n$ is the number of data points\n",
        "- $y_i$ is the actual value of the data point\n",
        "- $\\hat{y}_i$ is the predicted value of the data point"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BTLEuhI9u7zD"
      },
      "outputs": [],
      "source": [
        "# Numpy can be used to define the mean squared error\n",
        "# - y is the true value\n",
        "# - y_pred is the predicted value\n",
        "def mse(y, y_pred):\n",
        "    return np.mean((y - y_pred) ** 2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zow2s4mUu7zD"
      },
      "source": [
        "That formula tells us how well our model fits the data, but how do we find the best parameters?\n",
        "\n",
        "We need an algorithm to find the best parameters (slope and offset) that minimize the loss function.\n",
        "\n",
        "‚≠ê Luckily, linear regression with MSE loss function can be solved analytically with the Ordinary Least Squares (OLS) method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QursYh38u7zE"
      },
      "source": [
        "### Ordinary Least Squares (OLS)\n",
        "\n",
        "The OLS method finds the best parameters by minimizing the mean squared error loss function.\n",
        "\n",
        "To find the slope that best fits the data, we divide the covariance of $x$ and $y$ by the variance of $x$:\n",
        "\n",
        "$$\n",
        "m = \\frac{cov(x, y)}{var(x)}\n",
        "$$\n",
        "\n",
        "And to find the offset, we use the following formula:\n",
        "\n",
        "$$\n",
        "b = \\bar{y} - m \\bar{x}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "- $\\bar{x}$ is the mean of $x$\n",
        "- $\\bar{y}$ is the mean of $y$\n",
        "\n",
        "This is the scalar form of the formula, meaning it works for a single input variable and a single target variable.\n",
        "\n",
        "**But what if we have multiple input variables?**\n",
        "\n",
        "We have to solve a system of linear equations, which can be done in a compact form using matrices. The closed-form solution for OLS in matrix form, for $\\beta = [\\beta_0, \\beta_1, ..., \\beta_p]^T$ is:\n",
        "$$\n",
        "\\mathbf{\\beta}= (\\mathbf{X^T} \\mathbf{X})^{-1}\\mathbf{X^T} \\mathbf{y}\n",
        "$$\n",
        "where:\n",
        "- $y$ is the target variable\n",
        "- we have expanded the input matrix $\\mathbf{X}$ with a column of 1s to compute also the bias term $\\beta_0$:\n",
        "$$\n",
        "\\mathbf{X} =\n",
        "\\begin{bmatrix}\n",
        "    1 & x_{11} & x_{12} & \\dots & x_{1p} \\\\\n",
        "    1 & x_{21} & x_{22} & \\dots & x_{2p} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & x_{n1} & x_{n2} & \\dots & x_{np}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "Once we have the $\\beta$ vector, we can use it to predict $\\hat{y}$ given the input vector $X$:\n",
        "\n",
        "$$\n",
        "\\hat{y} = X \\beta\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KU8cS7-yu7zF"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "\n",
        "# Create some multiple linear regression data\n",
        "np.random.seed(0)\n",
        "n = 300 # number of data points\n",
        "# multiply by 50 to make the features range from 0 to 50, as an example\n",
        "x1 = np.random.rand(n) * 50\n",
        "x2 = np.random.rand(n) * 50\n",
        "\n",
        "# This is the true function we want to estimate\n",
        "m1 = 1.5\n",
        "m2 = -1\n",
        "b = 50\n",
        "y = m1 * x1 + m2 * x2 + b\n",
        "\n",
        "# We can write this in matrix form, by creating directly a matrix X\n",
        "X_matrix = np.random.rand(n, 2) * 50 # input matrix\n",
        "\n",
        "# And stacking the coefficients in a vector M\n",
        "M = np.array([m1, m2]) # true model coefficients\n",
        "\n",
        "# We can then write the linear regression as a matrix multiplication\n",
        "y = X_matrix @ M + b # recalculates y using matrix multiplication (@). It's an equivalent way of expressing the linear equation in a more compact form.\n",
        "\n",
        "# Add some noise to the data, because real data is usually noisy\n",
        "y = y + np.random.normal(0, 0.1, n) # (mean, standard deviation, output shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TsYU3qwdu7zF"
      },
      "source": [
        "**Example**\n",
        "\n",
        "In this case we have 2 input variables:\n",
        "- $x1$ is the number of hours studied\n",
        "- $x2$ is the number of hours spent on social media (doomscrolling)\n",
        "\n",
        "We would like to predict the final grade (out of 100) which we call $y$, given those 2 input variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 423
        },
        "id": "P8KS2_2lu7zF",
        "outputId": "b8aacde2-76a6-424f-d4fd-e57ed0ff8c68"
      },
      "outputs": [],
      "source": [
        "# Create a DataFrame just to plot the data\n",
        "df = pd.DataFrame({\"x1 Study Hours\": X_matrix[:, 0], \"x2 Doomscrolling Hours\": X_matrix[:, 1], \"y Exam Grade\": y})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "id": "l0ElnfMeu7zF",
        "outputId": "1af45242-f93c-44eb-fa56-57aefbe8e1a7"
      },
      "outputs": [],
      "source": [
        "fig, axs = plt.subplots(1, 2, figsize=(12, 5), sharey=True)\n",
        "sns.scatterplot(data=df, x=\"x1 Study Hours\", y=\"y Exam Grade\", ax=axs[0])\n",
        "sns.scatterplot(data=df, x=\"x2 Doomscrolling Hours\", y=\"y Exam Grade\", ax=axs[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wptayosvu7zG",
        "outputId": "853dfcc2-0470-411d-b95f-8226ae0843c7"
      },
      "outputs": [],
      "source": [
        "# Step 1: Add a column of ones to X to account for the intercept term\n",
        "X = np.hstack((np.ones((X_matrix.shape[0], 1)), X_matrix))\n",
        "print(f\"first 10 elements of X:\\n{X[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GifgwEQNu7zG"
      },
      "outputs": [],
      "source": [
        "# Step 2: Calculate the OLS parameters\n",
        "# Remembder this equation? Œ≤ = (X^T X)^-1 X^T y\n",
        "\n",
        "# X.T is the transpose of X\n",
        "# np.linalg.inv() computes the inverse of a matrix\n",
        "# @ is the matrix multiplication operator\n",
        "beta = np.linalg.inv(X.T @ X) @ X.T @ y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Wzw1QLYu7zH"
      },
      "source": [
        "Define a function so that we can use this estimator in the future."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngF1or1Vu7zH"
      },
      "outputs": [],
      "source": [
        "def compute_beta_lin_reg(X, y):\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X)) # FIXED - aggiungo una colonna di zeri\n",
        "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eakGJgTQu7zI",
        "outputId": "f950eebf-8385-4156-cd68-52a03ec195d1"
      },
      "outputs": [],
      "source": [
        "# Extract the coefficients from beta\n",
        "# in vector form\n",
        "M_model = beta[1:]\n",
        "b_model = beta[0]\n",
        "\n",
        "# Print the coefficients\n",
        "print(f\"M_model = {M_model}\")\n",
        "print(f\"b_model = {b_model}\")\n",
        "\n",
        "print(\"\\nWhile the true coefficients are:\")\n",
        "print(f\"M = {M}\")\n",
        "print(f\"b = {b}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4zQZYqeu7zI"
      },
      "source": [
        "In this fictional example, we have found out that:\n",
        "\n",
        "The linear relation between the input variables and the target variable as follows:\n",
        "\n",
        "> For each 1.5 hours studied, the final grade increases by 1 point, and for each hour spent on social media, the final grade decreases by 1 points.\n",
        "\n",
        "The bias, or offset, is 50 because if you don't study at all and spend no time on social media, you will get 50 points (basically the average grade)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uew0DNzJu7zI",
        "outputId": "55d516a0-f3b5-4f7b-c6b2-6212daa64083"
      },
      "outputs": [],
      "source": [
        "# Compute predictions\n",
        "y_pred = X @ beta\n",
        "\n",
        "# Compute the MSE\n",
        "print(f\"MSE: {mse(y, y_pred):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RBLlZOzju7zJ"
      },
      "source": [
        "## $R^2$ Score"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw0y4kUJu7zJ"
      },
      "source": [
        "The $R^2$ score is a metric that tells us how well our model fits the data, but it's more interpretable than the mean squared error.\n",
        "\n",
        "This score is a value between 0 and 1, where 1 means the model perfectly explains the linear relationship between the input and target variables.\n",
        "\n",
        "The formula for the $R^2$ score is the following:\n",
        "\n",
        "$$\n",
        "R^2 = 1 - \\frac{SS_{res}}{SS_{tot}}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "$$\n",
        "SS_{res} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
        "$$\n",
        "\n",
        "$$\n",
        "SS_{tot} = \\sum_{i=1}^{n} (y_i - \\bar{y})^2\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VatQItHku7zJ"
      },
      "outputs": [],
      "source": [
        "# Given y and y_pred, we can compute the residuals\n",
        "rss = np.sum((y - y_pred)**2)\n",
        "# Residual Sum of Squares (RSS)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J3JgoIZuu7zJ"
      },
      "outputs": [],
      "source": [
        "# Total Sum of Squares (TSS)\n",
        "# This is the sum of the squared differences between the true value and the mean of the true values\n",
        "tss = np.sum((y - np.mean(y))**2)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sz3fMY1mu7zJ"
      },
      "outputs": [],
      "source": [
        "# Now we can compute the R^2\n",
        "R2 = 1 - (rss/tss)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e679AX_bu7zK"
      },
      "source": [
        "Let's implement a function that calculates the $R^2$ score in a compact and reusable way.\n",
        "\n",
        "This is very important for code clarity and to avoid potential bugs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vs5baqau7zK"
      },
      "outputs": [],
      "source": [
        "# We can put all the operations we just did in a single function\n",
        "def r_squared(y, y_pred):\n",
        "    rss = np.sum((y - y_pred) ** 2)\n",
        "    tss = np.sum((y - np.mean(y)) ** 2)\n",
        "    return 1 - rss / tss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LD44pXWSu7zK"
      },
      "source": [
        "Let's try with a bad model to see how the $R^2$ score changes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "llAQF_Cku7zK",
        "outputId": "6824907f-6621-465d-95f7-8ef13da31d3b"
      },
      "outputs": [],
      "source": [
        "beta_bad = np.array([0, -1, 1])\n",
        "\n",
        "# Compute predictions\n",
        "y_pred_bad = X @ beta_bad\n",
        "\n",
        "# Compute the MSE\n",
        "print(f\"MSE: {mse(y, y_pred_bad):.4f}\")\n",
        "\n",
        "# Compute the R^2\n",
        "print(f\"R^2: {r_squared(y, y_pred_bad):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37Pf6r3Iu7zK"
      },
      "source": [
        "Here the $R^2$ score is even negative, which means the model is worse than predicting the mean of the target variable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EYzOa0Mu7zK"
      },
      "source": [
        "**Disclaimer:**\n",
        "\n",
        "I know that 3d plots are usually not the best way to visualize data, but in this case, it's a good way to see how the model fits the data.\n",
        "\n",
        "Let's see the 3d plot of the good model and the bad model.\n",
        "\n",
        "- The plane represents the linear model\n",
        "- The points represent the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "collapsed": true,
        "id": "rd8h79dyu7zL",
        "outputId": "67c619b6-6392-4238-bf79-da44d2ca8f8e"
      },
      "outputs": [],
      "source": [
        "from utils import plot_multiple_lin_reg\n",
        "\n",
        "\n",
        "plot_multiple_lin_reg(df, \"y Exam Grade\", beta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 542
        },
        "collapsed": true,
        "id": "DPR3evkqu7zL",
        "outputId": "f8a617ed-619a-4b48-fd46-21718ff6f55c"
      },
      "outputs": [],
      "source": [
        "plot_multiple_lin_reg(df, \"y Exam Grade\", beta_bad)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X1WMwFxEu7zL"
      },
      "source": [
        "## Feature Augmentation\n",
        "\n",
        "Feature augmentation is a technique used to improve the expressiveness of linear models.\n",
        "\n",
        "In this case, we will add polynomial features to the input variables to better fit the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 676
        },
        "id": "w3wsXMtYu7zL",
        "outputId": "ae447faf-d12c-4818-e2d1-e1cf8ed89e4c"
      },
      "outputs": [],
      "source": [
        "# Let's create data with only one feature\n",
        "np.random.seed(0)\n",
        "n = 20\n",
        "x = np.random.rand(n)\n",
        "y = 3 * x ** 2 -2 * x + 1 + np.random.normal(0, 0.1, n) #non √® lineare, ma possiamo comunque provare a fittare un modello lineare\n",
        "\n",
        "# Create a DataFrame just to plot the data\n",
        "df = pd.DataFrame({\"x\": x, \"y\": y})\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "n-85GOJ9u7zL",
        "outputId": "d2df45a8-869c-4bec-f442-286aa26893f5"
      },
      "outputs": [],
      "source": [
        "sns.scatterplot(data=df, x=\"x\", y=\"y\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e5TC_k7gu7zL"
      },
      "source": [
        "Now let's try fitting a linear model to the data, this is the best we can do with a linear model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 484
        },
        "id": "HxvdvF4Eu7zY",
        "outputId": "bcb906fd-c04d-4c32-f9a9-eafd4c5c8bfe"
      },
      "outputs": [],
      "source": [
        "beta = compute_beta_lin_reg(x.reshape(-1, 1), y) # beta will hold the coefficients of our linear model (slope and intercept)\n",
        "\n",
        "model = beta[1] * x + beta[0]\n",
        "\n",
        "sns.scatterplot(data=df, x=\"x\", y=\"y\")\n",
        "plt.plot(x, model, \"r\")\n",
        "\n",
        "print(f\"MSE: {mse(y, model):.4f}\")\n",
        "print(f\"R^2: {r_squared(y, model):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v10HKLRgu7zY"
      },
      "source": [
        "To fix this, we can add **polynomial features** to the input variables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eQwQbKQau7zY"
      },
      "outputs": [],
      "source": [
        "# First, create an array which is the square of the input variable.\n",
        "x_2 = x ** 2\n",
        "\n",
        "# And then stack the two arrays together, to create the X matrix, where each row is [x^2, x]\n",
        "X = np.vstack((x_2, x)).T"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VgD2i4j3u7zY"
      },
      "outputs": [],
      "source": [
        "# Then we can use our usual function to compute the linear regression coefficients\n",
        "beta = compute_beta_lin_reg(X, y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "054hZC00u7zY",
        "outputId": "b3a7aeca-5ddf-40a4-b1d9-808ff76a3eb1"
      },
      "outputs": [],
      "source": [
        "# And compute the predictions as the X matrix multiplied by the coefficients plus the intercept\n",
        "y_pred = X @ beta[1:] + beta[0]\n",
        "\n",
        "sns.scatterplot(data=df, x=\"x\", y=\"y\", label=\"True values\")\n",
        "\n",
        "# We can plot the model as well, so we may want to compute the x values as a range from the minimum to the maximum\n",
        "x_min, x_max = np.min(x), np.max(x)\n",
        "x_grid = np.linspace(x_min, x_max, 100)\n",
        "\n",
        "# Compute the y values using the model\n",
        "y_model = beta[0] + beta[1] * x_grid ** 2 + beta[2] * x_grid\n",
        "\n",
        "# And plot the model\n",
        "plt.plot(x_grid, y_model, \"r\", label=\"Model\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUf2v9Ixu7zZ"
      },
      "source": [
        "This works very well if we know the data may have a polynomial relationship. But in general we may not know the relationship between the input and target variables, especially when working with high dimensional data.\n",
        "\n",
        "So in general, we can add as many polynomial features as we want, let's try."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KzlrGVPku7zZ"
      },
      "outputs": [],
      "source": [
        "# Define a function that augments a feature with polynomial terms\n",
        "def augment_feature(x, degree):\n",
        "    X = np.vstack([x ** i for i in range(1, degree + 1)]).T\n",
        "    return X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYpCUuDmu7zZ"
      },
      "source": [
        "We may have overfit the data, the model is too complex and doesn't generalize well."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 466
        },
        "id": "_cBO6nMeu7zZ",
        "outputId": "d93e02e0-eecd-4185-fd9c-a78b869cbbdc"
      },
      "outputs": [],
      "source": [
        "degree = 9\n",
        "X_poly = augment_feature(x, degree)\n",
        "\n",
        "beta = compute_beta_lin_reg(X_poly, y)\n",
        "\n",
        "X_grid = np.linspace(np.min(x), np.max(x), 1000)\n",
        "X_grid = augment_feature(X_grid, degree)\n",
        "\n",
        "y_pred = X_grid @ beta[1:] + beta[0]\n",
        "\n",
        "sns.scatterplot(data=df, x=\"x\", y=\"y\", label=\"Input data\")\n",
        "plt.plot(X_grid[:, 0], y_pred, c=\"r\", label=\"Model\")\n",
        "plt.legend()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHCZg471u7zZ"
      },
      "source": [
        "## Ridge Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GnnlQXB1u7zZ"
      },
      "source": [
        "Ridge Regression estimates coefficients by minimizing\n",
        "$$\n",
        "RSS + \\lambda \\sum_{j=\\textcolor{red}{1}}^{p} \\beta _j^2\n",
        "$$\n",
        "\n",
        "#### Do not penalize the bias term üö´   \n",
        "Note that the regularization parameter $\\lambda$ is applied only to $\\beta_1, ..., \\beta_p$, not to the intercept/bias term $\\beta_0$. Why?\n",
        "- We want to shrink the feature weights with respect to the response, to reduce model complexity and prevent overfitting.\n",
        "- But the intercept is not a feature weight, it simply measures the mean response when $x_{i1} = x_{i2} =...=x_{ip}=0.$\n",
        "- If we penalize $\\beta_0$, the model would be forced toward zero-centered predictions (i.e. small $\\beta_0$) which might be wrong.\n",
        "\n",
        "#### Closed-form solution in matrix form\n",
        "$$\n",
        "\\mathbf{\\beta}= (\\mathbf{\\tilde{X}^T} \\mathbf{\\tilde{X}}+\\lambda \\mathbf{\\tilde{I}})^{-1}\\mathbf{\\tilde{X}^T} \\mathbf{y}\n",
        "$$\n",
        "where:\n",
        "- $\\tilde{X}$ is the expanded matrix, with all the features standardized (to have zero-mean and unit variance) and where we added the column of 1s:\n",
        "$$\n",
        "\\tilde{x}_{ij} = \\frac{x_{ij}-\\mu_j}{\\sigma_j}\\quad\n",
        "\\rightarrow \\quad\n",
        "\\mathbf{\\tilde{X}} =\n",
        "\\begin{bmatrix}\n",
        "    1 & \\tilde{x}_{11} & \\tilde{x}_{12} & \\dots & \\tilde{x}_{1p} \\\\\n",
        "    1 & \\tilde{x}_{21} & \\tilde{x}_{22} & \\dots & \\tilde{x}_{2p} \\\\\n",
        "    \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
        "    1 & \\tilde{x}_{n1} & \\tilde{x}_{n2} & \\dots & \\tilde{x}_{np}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- $\\lambda$ is the regularization parameter controlling the penalty on the coefficients magnitude.\n",
        "- $\\mathbf{\\tilde{I}}$ is a identity-like matrix with a zero in first entry (because the column of 1s is the first one):\n",
        "$$\n",
        "\\mathbf{\\tilde{I}} =\n",
        "\\begin{bmatrix}\n",
        "    \\textcolor{red}{0} & 0 & \\dots & 0 & 0 \\\\\n",
        "    0 & 1 & \\dots & 0 & 0 \\\\\n",
        "    \\vdots & \\vdots & \\ddots & \\vdots & \\vdots \\\\\n",
        "    0 & 0 & \\dots & 1 & 0 \\\\\n",
        "    0 & 0 & \\dots & 0 & 1 \\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "This structure ensures that $\\lambda$ only penalizes $\\beta_1,...\\beta_p$ and not $\\beta_0$. You can write the solution in expanded form to verify this.\n",
        "\n",
        "<div style=\"background-color: lightblue; padding: 10px; color: black\">\n",
        "    <strong> ‚ö†Ô∏è Warning! Standardization in Ridge Regression is necessary. </strong>\n",
        "    Ridge Regression shrinks the magnitude of the coefficients to prevent overfitting. However, if features have different scales, Ridge will penalize large-scale features less, making regularization ineffective (look at 'Introduction to statistical learning'!)\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N9O-vdJ0u7zZ"
      },
      "outputs": [],
      "source": [
        "# 1. Standardize the features\n",
        "X_mean = np.mean(X_poly, axis=0) # con axis=0 -> media per colonna non quella generale\n",
        "X_std = np.std(X_poly, axis=0)\n",
        "X_norm = (X_poly - X_mean) / X_std\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ljS5OFpcu7zZ",
        "outputId": "6ff2cb2a-ed88-46e5-8ff3-6fcf63765c4f"
      },
      "outputs": [],
      "source": [
        "# 2. Define and train the model\n",
        "def compute_beta_ridge(X, y, lambda_):\n",
        "    # expand the input matrix to include the bias term\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "\n",
        "    # define the identity-like matrix\n",
        "    I_tilde = np.eye(X.shape[1])\n",
        "    I_tilde[0, 0] = 0\n",
        "\n",
        "    # compute the coefficients\n",
        "    #beta = np.linalg.inv(X @ X.T + lambda_ * I_tilde) @ X.T @ y\n",
        "    beta = np.linalg.inv(X.T @ X + lambda_ * I_tilde) @ X.T @ y # FIXED - invertito XT con X\n",
        "    return beta\n",
        "\n",
        "lambda_ = 0.1\n",
        "beta_ridge = compute_beta_ridge(X_norm, y, lambda_)\n",
        "print(beta_ridge.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qQk2i5Ju7zZ",
        "outputId": "01d1324f-8773-4cca-84d1-f47d07232766"
      },
      "outputs": [],
      "source": [
        "# Compute also the beta_OLS on the normalized data, and let's compare the coefficients\n",
        "beta_ols = compute_beta_lin_reg(X_norm, y)\n",
        "\n",
        "# Let's print the coefficients\n",
        "print(\"{:<10} {:<20} {:<20}\".format(\"Coeff\", \"Œ≤_Ridge\", \"Œ≤_OLS\"))\n",
        "print(\"-\" * 50)\n",
        "for i in range(len(beta_ridge)):\n",
        "    index_label = f\"Œ≤{i}\"\n",
        "    print(\"{:<10} {:<20.6f} {:<20.6f}\".format(index_label, beta_ridge[i], beta_ols[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xVC_4vn-u7za"
      },
      "outputs": [],
      "source": [
        "# 3. Compute predictions\n",
        "y_pred_ridge = beta_ridge[0] + X_norm @ beta_ridge[1:]\n",
        "y_pred_ols = beta_ols[0] + X_norm @ beta_ols[1:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3KTXuAAou7za",
        "outputId": "e1a59d8d-4a08-4d5f-a64b-90bd464351ef"
      },
      "outputs": [],
      "source": [
        "# 4. Evaluate the model\n",
        "mse_ridge = mse(y, y_pred_ridge)\n",
        "mse_ols = mse(y, y_pred_ols)\n",
        "r2_ridge = r_squared(y, y_pred_ridge)\n",
        "r2_ols = r_squared(y, y_pred_ols)\n",
        "\n",
        "print(\"MSE-Ridge: \", mse_ridge)\n",
        "print(\"MSE-OLS: \", mse_ols)\n",
        "print(\"R^2-Ridge: \", r2_ridge)\n",
        "print(\"R^2-OLS: \", r2_ols)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQIzA0V3u7za"
      },
      "source": [
        "<strong> ‚ùå This is the wrong way to evaluate a model! </strong> Oh no! Ridge appears to perform worse than OLS üò¢. This happens because we did not split the dataset into training and testing sets. Therefore, we are not evaluating the model's generalization ability but only how well it fits (or overfit!) the training data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tRAYNKtYu7za",
        "outputId": "b280b37c-bade-4dbd-d2ce-6d61ef851521"
      },
      "outputs": [],
      "source": [
        "# let's create a test set (not seen in training)\n",
        "n_test = 20\n",
        "x_test = np.random.rand(n_test)\n",
        "y_test = 3 * x_test ** 2 - 2 * x_test + 1 + np.random.normal(0, 0.1, n_test)\n",
        "\n",
        "# augment the test set\n",
        "X_test = augment_feature(x_test, degree)\n",
        "\n",
        "# standardize the test set\n",
        "X_test_norm = (X_test - X_mean) / X_std\n",
        "\n",
        "# compute the predictions\n",
        "y_pred_test_ridge = beta_ridge[0] + X_test_norm @ beta_ridge[1:]\n",
        "y_pred_test_ols = beta_ols[0] + X_test_norm @ beta_ols[1:]\n",
        "\n",
        "# compute the MSE\n",
        "mse_test_ridge = mse(y_test, y_pred_test_ridge)\n",
        "mse_test_ols = mse(y_test, y_pred_test_ols)\n",
        "r2_test_ridge = r_squared(y_test, y_pred_test_ridge)\n",
        "r2_test_ols = r_squared(y_test, y_pred_test_ols)\n",
        "\n",
        "# Ridge actually works üòÑ\n",
        "print(\"MSE-Ridge (test): \", mse_test_ridge)\n",
        "print(\"MSE-OLS (test): \", mse_test_ols)\n",
        "print(\"R^2-Ridge (test): \", r2_test_ridge)\n",
        "print(\"R^2-OLS (test): \", r2_test_ols)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 449
        },
        "id": "FRiUAl7Hu7za",
        "outputId": "d23f7154-5e63-4f40-f215-2a60121d5389"
      },
      "outputs": [],
      "source": [
        "# Visualize the model\n",
        "X_grid = np.linspace(np.min(x), np.max(x), 1000)\n",
        "X_grid = augment_feature(X_grid, degree)\n",
        "X_grid_norm = (X_grid - X_mean) / X_std\n",
        "\n",
        "y_pred_ridge = beta_ridge[0] + X_grid_norm @ beta_ridge[1:]\n",
        "y_pred_ols = beta_ols[0] + X_grid_norm @ beta_ols[1:]\n",
        "\n",
        "sns.scatterplot(data=df, x=\"x\", y=\"y\", label=\"Training data\")\n",
        "plt.plot(X_grid[:, 0], y_pred_ols, c=\"r\", label=\"OLS\")\n",
        "plt.plot(X_grid[:, 0], y_pred_ridge, c=\"g\", label=\"Ridge\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1REeM3Iu7za"
      },
      "source": [
        "### How to go back to coefficients of unnormalized data (for interpretability)\n",
        "If we standardize data, we are finding the coefficients of the model:\n",
        "$$\n",
        "\\hat{Y} = \\beta_0 + \\sum_{j=1}^{p} \\beta_j \\frac{X_j-\\mu_j}{\\sigma_j}\n",
        "$$\n",
        "\n",
        "What if we want to go back, i.e. we want find the coefficients for unnormalize data? We can decompose the formula above as:\n",
        "$$\n",
        "\\hat{Y} = \\left( \\beta_0 - \\sum_{j=1}^{p} \\beta_j \\frac{\\mu_j}{\\sigma_j} \\right) + \\sum_{j=1}^{p} \\beta_j \\frac{X_j}{\\sigma_j}\n",
        "$$\n",
        "so, the intercept will become\n",
        "$$\n",
        "\\beta_0^{un} = \\beta_0 - \\sum_{j=1}^{p} \\beta_j \\frac{\\mu_j}{\\sigma_j}\n",
        "$$\n",
        "and the weights:\n",
        "$$\n",
        "\\beta_j^{un} = \\frac{\\beta_j}{\\sigma_j}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62yk0kklu7za",
        "outputId": "b2ffa6d7-7ac9-498e-c9cf-b5c347512e4d"
      },
      "outputs": [],
      "source": [
        "# let's try to compute the unnormalized version of the coefficients\n",
        "beta0_ols_unnorm = beta_ols[0] - np.sum(beta_ols[1:] * X_mean / X_std)\n",
        "beta_ols_unnorm = beta_ols[1:] / X_std\n",
        "\n",
        "print(\"Unnormalized OLS coefficients: \", beta0_ols_unnorm, beta_ols_unnorm)\n",
        "print(\"Coefficient on unnormalized data: \", beta)\n",
        "print(\"They are the same!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTSnMbDRu7ze"
      },
      "source": [
        "### Coefficients magnitude as function of Œª"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        },
        "id": "4YBsOqlou7zf",
        "outputId": "8048e245-6170-483a-b6eb-0b9bcf4cfc8b"
      },
      "outputs": [],
      "source": [
        "lambdas_ = np.logspace(-2, 3, 100)                          # let's plot the coefficients for a range of lambdas from 0.01 to 1000 with 100 points\n",
        "coefficients = np.zeros((len(lambdas_), X_poly.shape[1]))\n",
        "\n",
        "for i, lambda_ in enumerate(lambdas_):\n",
        "    beta_ridge = compute_beta_ridge(X_norm, y, lambda_)\n",
        "    coefficients[i, :] = beta_ridge[1:]                     # let's exclude the bias term (which is NOT regularized)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i in range(X_poly.shape[1]):\n",
        "    plt.plot(lambdas_, coefficients[:, i], label=f\"Œ≤{i+1}\")\n",
        "plt.xlabel(\"Œª\")\n",
        "plt.xscale(\"log\")\n",
        "plt.ylabel(\"Coefficient value\")\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_74dgqu7zf"
      },
      "source": [
        "## Cross validation in a real dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBrKpUPTu7zf"
      },
      "source": [
        "| **Procedure w/out Cross-Validation** | **Procedure with k-Fold Cross-Validation** | **Procedure with Monte Carlo Cross-Validation** |\n",
        "|--------------------------------------|--------------------------------------------|-----------------------------------------------|\n",
        "| 1. Load data <br> 2. Train-test split <br> 3. (Standardize train and test features using training statistics) <br> 4. Expand the input matrix to account for bias <br> 5. Train the model: find Œ≤ <br> 6. Make predictions <br> 7. Evaluate model on test set | 1. Load data <br> 2. Split data into k folds <br> 3. For each fold: <br> &nbsp;&nbsp;&nbsp; a. Use k-1 folds for training and 1-fold for test <br> &nbsp;&nbsp;&nbsp; b. (Standardize train and test features using training statistics) <br> &nbsp;&nbsp;&nbsp; c. Train the model: find Œ≤ <br> &nbsp;&nbsp;&nbsp; d. Make predictions <br> &nbsp;&nbsp;&nbsp; e. Evaluate performance on test set <br> 4. Aggregate results across folds | 1. Load data <br> 2. Repeat for N iterations: <br> &nbsp;&nbsp;&nbsp; a. Randomly split the data into training and test set <br> &nbsp;&nbsp;&nbsp; b. (Standardize train and test features using training statistics) <br> &nbsp;&nbsp;&nbsp; c. Train the model: find Œ≤ <br> &nbsp;&nbsp;&nbsp; d. Make predictions <br> &nbsp;&nbsp;&nbsp; e. Evaluate performance on test set <br> 3. Aggregate results across iterations |\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cq5WbAWvu7zf"
      },
      "source": [
        "### K-fold cross validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shUiRb5Qu7zf",
        "outputId": "ba9a5c16-e19e-4860-f276-08aaafd20953"
      },
      "outputs": [],
      "source": [
        "# 1. Load data\n",
        "np.random.seed(0)\n",
        "\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "cali_house = fetch_california_housing()\n",
        "X = cali_house['data']\n",
        "y = cali_house['target']\n",
        "feature_names = cali_house['feature_names']\n",
        "print(\"Feature names:\", feature_names)\n",
        "\n",
        "# select only features MedInc, HouseAge, AveRooms, AveOccup as done in class\n",
        "X = X[:, [0, 1, 2, 5]]\n",
        "\n",
        "\n",
        "# Note: this dataset is already clean, and all features are relevant. There is no benefit to use ridge regression\n",
        "# in this case. So, let's corrupt it for didactic purposes by adding multicollinearity and irrelevant features.\n",
        "# comment from here --------------\n",
        "X_collinear1 = X[:, 0] * 2.5 + X[:, 1] * 0.5  # Collinear with MedInc and HouseAge\n",
        "X_collinear2 = X[:, 2] * 1.2 + X[:, 3] * 0.8  # Collinear with AveRooms and AveOccup\n",
        "X_collinear3 = X[:, 0] * 1.1 + X[:, 2] * 0.9  # Collinear with MedInc and AveRooms\n",
        "X_collinear1 = X_collinear1.reshape(-1, 1)\n",
        "X_collinear2 = X_collinear2.reshape(-1, 1)\n",
        "X_collinear3 = X_collinear3.reshape(-1, 1)\n",
        "\n",
        "X_noise1 = np.random.normal(0, 1, size=(X.shape[0], 10))     # 10 features of white noise\n",
        "X_noise2 = np.random.uniform(-3, 2, size=(X.shape[0], 10))  # 10 features of uniform noise\n",
        "\n",
        "# add the new features to the original dataset\n",
        "X = np.hstack((X, X_collinear1, X_collinear2, X_collinear3, X_noise1, X_noise2))\n",
        "# ----------------- to here to use the original dataset\n",
        "\n",
        "n_samples = X.shape[0]\n",
        "n_features = X.shape[1]\n",
        "print(\"Number of samples:\", n_samples)\n",
        "print(\"Number of features:\", n_features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bxwLtNlJu7zf"
      },
      "outputs": [],
      "source": [
        "# define model functions\n",
        "def fit_OLS(X, y):\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    beta = np.linalg.inv(X.T @ X) @ X.T @ y\n",
        "    return beta\n",
        "\n",
        "def fit_ridge(X, y, lambda_):\n",
        "    X = np.hstack((np.ones((X.shape[0], 1)), X))\n",
        "    I_tilde = np.eye(X.shape[1])\n",
        "    I_tilde[0, 0] = 0\n",
        "    beta = np.linalg.inv(X.T @ X + lambda_ * I_tilde) @ X.T @ y\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rb6Wf_0Gu7zg"
      },
      "outputs": [],
      "source": [
        "# 2. Split data into k fold\n",
        "k = 5                       # number of folds\n",
        "fold_size = len(X) // k     # size of each fold\n",
        "\n",
        "# shuffle data: k-fold cross validation randomly splits the data into k folds\n",
        "np.random.seed(0)\n",
        "shuffled_indices = np.random.permutation(len(X))\n",
        "X_shuffled = X[shuffled_indices]\n",
        "y_shuffled = y[shuffled_indices]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BfNLTOGUu7zg"
      },
      "outputs": [],
      "source": [
        "# 3. For each fold...\n",
        "mse_folds_ols = []\n",
        "mse_folds_ridge = []\n",
        "r2_folds_ols = []\n",
        "r2_folds_ridge = []\n",
        "\n",
        "for i in range(k):\n",
        "    # a. Split data into train and test\n",
        "    # define test indices\n",
        "    start = i*fold_size\n",
        "    end = (i+1)*fold_size\n",
        "    test_indices = np.arange(start, end)\n",
        "\n",
        "    # train indices are the remaining indices\n",
        "    train_indices = train_indices = np.concatenate((np.arange(0, start), np.arange(end, n_samples)))\n",
        "\n",
        "    X_train = X_shuffled[train_indices]\n",
        "    y_train = y_shuffled[train_indices]\n",
        "    X_test = X_shuffled[test_indices]\n",
        "    y_test = y_shuffled[test_indices]\n",
        "\n",
        "    # b. Standardize data\n",
        "    X_train_mean = np.mean(X_train, axis=0)\n",
        "    X_train_std = np.std(X_train, axis=0)\n",
        "    X_train = (X_train - X_train_mean) / X_train_std\n",
        "    X_test = (X_test - X_train_mean) / X_train_std\n",
        "\n",
        "    # c. fit models\n",
        "    beta_ols = fit_OLS(X_train, y_train)\n",
        "    beta_ridge = fit_ridge(X_train, y_train, lambda_=10)\n",
        "\n",
        "    # d. predict on test set\n",
        "    y_pred_ols = X_test @ beta_ols[1:] + beta_ols[0]\n",
        "    y_pred_ridge = X_test @ beta_ridge[1:] + beta_ridge[0]\n",
        "\n",
        "    # e. calculate metrics\n",
        "    mse_ols = np.mean((y_test - y_pred_ols)**2)\n",
        "    mse_ridge = np.mean((y_test - y_pred_ridge)**2)\n",
        "    r2_ols = 1 - np.sum((y_test - y_pred_ols)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
        "    r2_ridge = 1 - np.sum((y_test - y_pred_ridge)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
        "\n",
        "    mse_folds_ols.append(mse_ols)\n",
        "    mse_folds_ridge.append(mse_ridge)\n",
        "    r2_folds_ols.append(r2_ols)\n",
        "    r2_folds_ridge.append(r2_ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "-AzZ421qu7zg",
        "outputId": "3159fd81-fef1-4b9b-e339-97413933190c"
      },
      "outputs": [],
      "source": [
        "# 4. Aggregate metrics\n",
        "# plot boxplots\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(data=[mse_folds_ols, mse_folds_ridge], palette='Set3', showfliers=False)\n",
        "plt.ylabel('MSE')\n",
        "plt.xticks([0, 1], ['OLS', 'Ridge'])\n",
        "plt.title('MSE')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=[r2_folds_ols, r2_folds_ridge], palette='Set3', showfliers=False)\n",
        "plt.ylabel('R2')\n",
        "plt.xticks([0, 1], ['OLS', 'Ridge'])\n",
        "plt.title('R2')\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"K-fold Cross Validation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNHgJHiuu7zg"
      },
      "source": [
        "### Monte Carlo Cross Validation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Y8nxlCNu7zg"
      },
      "source": [
        "In simple terms: instead of splitting the data into k-folds, we randomly split the data into training and test sets.\n",
        "\n",
        "#### ‚õ∞ Monte Carlo digression\n",
        "\n",
        "**What is Monte Carlo estimation?**\n",
        "\n",
        "It is a method to estimate a quantity by using random sampling, and then averaging the results.\n",
        "\n",
        "Basically, a Monte Carlo estimation can be done in the following steps:\n",
        "\n",
        "1. Generate random samples from a distribution\n",
        "2. Compute the output for the samples\n",
        "3. Average the output\n",
        "\n",
        "**An example of Monte Carlo estimation: Approximating the value of $\\pi$**\n",
        "\n",
        "> Today is pi day! üéâ (3-14 in the American date format)\n",
        "\n",
        "Note: the ratio between the area of a circle and the area of a square that contains the circle is $\\pi / 4$.\n",
        "\n",
        "We start by sampling some points inside the square."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iLWvCR_1u7zg"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0) # set the random seed for reproducibility\n",
        "\n",
        "n = 1000\n",
        "Xc = np.random.rand(n, 2)*2 - 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8zD2Kl1u7zg"
      },
      "source": [
        "This function checks if a point is inside the circle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrLfIIlXu7zg"
      },
      "outputs": [],
      "source": [
        "# We want to compute x^2 + y^2 for each point (along axis 1) and check if less than 1\n",
        "def is_inside_circle(Xc):\n",
        "    return np.sum(Xc**2, axis=1) < 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "id": "1s6Z-oiru7zg",
        "outputId": "72e12114-28bd-4d64-aab6-eb89dc63c3a3"
      },
      "outputs": [],
      "source": [
        "from utils import plt_circle\n",
        "\n",
        "\n",
        "color = is_inside_circle(Xc)\n",
        "\n",
        "ax = plt_circle()\n",
        "ax.scatter(Xc[:, 0], Xc[:, 1], s=1, c=color, cmap=\"coolwarm\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2QUwAhNzu7zg",
        "outputId": "a0e4c61b-0cd0-455d-b6d3-5dd0d0ab21b0"
      },
      "outputs": [],
      "source": [
        "# Now we compute the ratio between the number of points inside the circle and the total number of points\n",
        "ratio = 4 * np.sum(color) / n\n",
        "ratio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ozt_VAG6u7zh"
      },
      "source": [
        "Nice! But not quite equal to $\\pi$ yet.\n",
        "\n",
        "Let's try increasing the number of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uv2av_qPu7zh",
        "outputId": "9562f880-6234-4124-928b-4d1e6a921d65"
      },
      "outputs": [],
      "source": [
        "# We will conduct an experiment for each number of points in n_points\n",
        "n_points = [10**i for i in range(7)]\n",
        "n_points"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "Ni4JzeKmu7zh",
        "outputId": "78f66e74-02ae-4b95-b0e7-a538e6dc7311"
      },
      "outputs": [],
      "source": [
        "ratios = []\n",
        "\n",
        "for n in n_points:\n",
        "    # Generate n random points in the square\n",
        "    Xc = np.random.rand(n, 2) * 2 - 1\n",
        "    # Check if they are inside the circle\n",
        "    color = is_inside_circle(Xc)\n",
        "    # Compute the ratio and save it for later\n",
        "    ratio = 4 * np.sum(color) / n\n",
        "    # Save the ratio and the number of points\n",
        "    ratios.append(ratio)\n",
        "\n",
        "plt.plot(n_points, ratios, \"o-\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Ratio\")\n",
        "plt.title(f\"Estimation of œÄ: {ratio}\")\n",
        "plt.axhline(np.pi, color=\"r\", linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU1q3w5Eu7zh"
      },
      "source": [
        "How fast does the error decrease as we increase the number of samples?\n",
        "\n",
        "It is known that the error decreases as $O(1/\\sqrt{n})$, where $n$ is the number of samples."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "LtBldFmYu7zh",
        "outputId": "cc94d3fe-4517-4f2a-e8df-3d31119593c8"
      },
      "outputs": [],
      "source": [
        "errors = np.abs(np.array(ratios) - np.pi)\n",
        "\n",
        "plt.plot(n_points, 1/np.sqrt(n_points), \"r--\")\n",
        "plt.plot(n_points, errors, \"o-\")\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2jQXai5u7zh"
      },
      "source": [
        "As we can see, the more samples we have, the closer the estimation is to the real value of $\\pi$.\n",
        "\n",
        "**We could have also kept the number of points fixed and repeated the experiment multiple times, averaging the results.**\n",
        "\n",
        "This works because averaging multiple indipedent estimates of the same quantity reduces the variance of the estimate."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 493
        },
        "id": "jS04cgQOu7zh",
        "outputId": "cb848d37-e715-46a7-86b2-bc8eb8dba318"
      },
      "outputs": [],
      "source": [
        "n_points = 1000\n",
        "n_experiments = 1000\n",
        "ratios = []\n",
        "\n",
        "for _ in range(n_experiments):\n",
        "    # Generate n random points in the square\n",
        "    Xc = np.random.rand(n_points, 2) * 2 - 1\n",
        "    # Check if they are inside the circle\n",
        "    color = is_inside_circle(Xc)\n",
        "    # Compute the ratio and save it for later\n",
        "    ratio = 4 * np.sum(color) / n_points\n",
        "    # Save the ratioWoulWWww\n",
        "    ratios.append(ratio)\n",
        "\n",
        "# get the cumsum of the ratiosn_points\n",
        "cumsum_ratios = np.cumsum(ratios)\n",
        "# compute the average ratio at each point\n",
        "average_ratios = cumsum_ratios / np.arange(1, n_experiments + 1)\n",
        "\n",
        "n_points_total = n_points * np.arange(1, n_experiments + 1)\n",
        "\n",
        "plt.plot(n_points_total, average_ratios)\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.xscale(\"log\")\n",
        "plt.ylabel(\"Ratio\")\n",
        "plt.title(f\"Estimation of œÄ: {average_ratios[-1]}\")\n",
        "plt.axhline(np.pi, color=\"r\", linestyle=\"--\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 471
        },
        "id": "L802NoRnu7zh",
        "outputId": "8aab08e6-1970-4718-fa7a-2530a6ee6854"
      },
      "outputs": [],
      "source": [
        "errors = np.abs(average_ratios - np.pi)\n",
        "\n",
        "plt.plot(n_points_total, 1 / np.sqrt(n_points_total), \"r--\")\n",
        "plt.plot(n_points_total, errors)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Number of points\")\n",
        "plt.ylabel(\"Error\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Re_C6KGu7zh"
      },
      "source": [
        "<div style=\"font-size: 2em;\"> ‚õ∞ ‚Üí ü•ß </div>\n",
        "\n",
        "We have seen how Monte Carlo helps us estimate a quantity by averaging multiple random samples, in this case, the value of $\\pi$.\n",
        "\n",
        "Now we need to use Monte Carlo Cross-Validation to estimate the performance of our model.\n",
        "\n",
        "#### Monte Carlo Cross-Validation\n",
        "\n",
        "**Remember**: the model is trained on a subset of the data (called training set) and tested on another subset (called test set).\n",
        "> This is important to evaluate generalization of the model: how well it performs on unseen data.\n",
        "\n",
        "‚ö†Ô∏è But what if the test set is a lucky or unlucky subset of the data? Meaning, what if the samples in the test are particularly easy or hard to predict?\n",
        "\n",
        "üí° The idea is to sample the train and test sets multiple times from the dataset, while performing training and test each time.\n",
        "\n",
        "The performance is then average of the variuos runs ‚Üí remember Monte Carlo estimation?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BZVrNUEpu7zh"
      },
      "source": [
        "Let's implement Monte Carlo Cross-Validation to estimate the performance of a model: first we need a train-test split function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_LYPxewOu7zi"
      },
      "outputs": [],
      "source": [
        "# 2. Train-test split: to properly evaluate generalization performance.\n",
        "# a. set the proportion p of samples to use in the training set (usually 0.8, i.e. 80%)\n",
        "# b. split the dataset randomly. One way is:\n",
        "#   i. shuffle the indices of the samples\n",
        "#   ii. use the first p% of the shuffled indices to index the training set, the rest for the test set\n",
        "#   iii. use the indexed samples to create the training and test sets\n",
        "\n",
        "def random_train_test_split():\n",
        "    p_train = 0.8                                           # proportion of samples to use in the training set\n",
        "    split_point = int(p_train * X.shape[0])                 # numer of samples in the training set\n",
        "\n",
        "    shuffled_indices = np.random.permutation(X.shape[0])\n",
        "\n",
        "    train_indices = shuffled_indices[:split_point]\n",
        "    test_indices = shuffled_indices[split_point:]\n",
        "\n",
        "    X_train, y_train = X[train_indices], y[train_indices]\n",
        "    X_test, y_test = X[test_indices], y[test_indices]\n",
        "\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gx_xSVRfu7zi"
      },
      "source": [
        "Now we can perform multiple runs with different train-test splits."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rYnzoJwBu7zi"
      },
      "outputs": [],
      "source": [
        "n_iterations = 10 # number of times to perform the train and test of the model\n",
        "\n",
        "# 3. For each iteration...\n",
        "mse_iters_ols = []\n",
        "mse_iters_ridge = []\n",
        "r2_iters_ols = []\n",
        "r2_iters_ridge = []\n",
        "\n",
        "for i in range(n_iterations):\n",
        "    # a. Split data into train and test\n",
        "    X_train, X_test, y_train, y_test = random_train_test_split()\n",
        "\n",
        "    # b. Standardize data\n",
        "    X_train_mean = np.mean(X_train, axis=0)\n",
        "    X_train_std = np.std(X_train, axis=0)\n",
        "    X_train = (X_train - X_train_mean) / X_train_std\n",
        "    X_test = (X_test - X_train_mean) / X_train_std\n",
        "\n",
        "    # c. fit models\n",
        "    beta_ols = fit_OLS(X_train, y_train)\n",
        "    beta_ridge = fit_ridge(X_train, y_train, lambda_=10)\n",
        "\n",
        "    # d. predict on test set\n",
        "    y_pred_ols = X_test @ beta_ols[1:] + beta_ols[0]\n",
        "    y_pred_ridge = X_test @ beta_ridge[1:] + beta_ridge[0]\n",
        "\n",
        "    # e. calculate metrics\n",
        "    mse_ols = np.mean((y_test - y_pred_ols)**2)\n",
        "    mse_ridge = np.mean((y_test - y_pred_ridge)**2)\n",
        "    r2_ols = 1 - np.sum((y_test - y_pred_ols)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
        "    r2_ridge = 1 - np.sum((y_test - y_pred_ridge)**2) / np.sum((y_test - np.mean(y_test))**2)\n",
        "\n",
        "    mse_iters_ols.append(mse_ols)\n",
        "    mse_iters_ridge.append(mse_ridge)\n",
        "    r2_iters_ols.append(r2_ols)\n",
        "    r2_iters_ridge.append(r2_ridge)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        },
        "id": "628rhBS7u7zi",
        "outputId": "72dc96f7-c1ed-4555-ab54-7ef95c01e0a2"
      },
      "outputs": [],
      "source": [
        "# 4. Aggregate metrics\n",
        "# plot boxplots\n",
        "plt.figure(figsize=(10, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "sns.boxplot(data=[mse_iters_ols, mse_iters_ridge], palette='Set3', showfliers=False)\n",
        "plt.ylabel('MSE')\n",
        "plt.xticks([0, 1], ['OLS', 'Ridge'])\n",
        "plt.title('MSE')\n",
        "\n",
        "plt.subplot(1, 2, 2)\n",
        "sns.boxplot(data=[r2_iters_ols, r2_iters_ridge], palette='Set3', showfliers=False)\n",
        "plt.ylabel('R2')\n",
        "plt.xticks([0, 1], ['OLS', 'Ridge'])\n",
        "plt.title('R2')\n",
        "plt.tight_layout()\n",
        "plt.suptitle(\"Monte Carlo Cross Validation\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Pb6ONtIu7zi"
      },
      "source": [
        "<span style=\"color:red\"><strong>üèãÔ∏è‚Äç‚ôÄÔ∏è Exercise</strong></span>\n",
        "\n",
        "Using `Advertising` data:\n",
        "- retrieve the input matrix **X** (with features TV, radio newspaper) and the target **Y** (sales) as numpy arrays.\n",
        "- select a cross-validation method and train and evaluate both the OLS model and the Ridge Regression model.\n",
        "- Show the performances using boxplots.\n",
        "- Comment the results.\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
