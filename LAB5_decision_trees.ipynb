{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p1_IuHkQSwAg"
      },
      "source": [
        "**ML COURSE 2024-2025**\n",
        "# LAB5: DECISION TREES AND FORESTS\n",
        "In this lab you will see implementations of a Decision Tree and a Random Forest, as well as how to use other forest-based models."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Chiara2804/ML_2024-25.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zyL0WP_fTGHF",
        "outputId": "6f7a7baf-9cbe-487d-ac57-ab99544b856f"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML_2024-25' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uNyzZsqjT5lB",
        "outputId": "57946d5e-35ae-4ec3-9d87-4c53a310fd2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-utils in /usr/local/lib/python3.11/dist-packages (3.9.1)\n",
            "Requirement already satisfied: typing_extensions>3.10.0.2 in /usr/local/lib/python3.11/dist-packages (from python-utils) (4.13.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 367
        },
        "id": "EJrHbUSLSwAh",
        "outputId": "b13b8f06-7d1c-4d36-8fa1-eebf225ffc4a"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ImportError",
          "evalue": "cannot import name 'stratified_train_test_split' from 'utils' (/content/ML_2024-25/utils.py)",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-d894b9acdc35>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mmatplotlib\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mstratified_train_test_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mImportError\u001b[0m: cannot import name 'stratified_train_test_split' from 'utils' (/content/ML_2024-25/utils.py)",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dataclasses import dataclass\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "from utils import stratified_train_test_split, accuracy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5E-ZUD3mSwAi"
      },
      "source": [
        "## Decision Tree Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aLtRUGnwSwAi"
      },
      "source": [
        "### Implementation from scratch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DqL-dtD6SwAj"
      },
      "source": [
        "Nodes indexing:\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqA1WwVySwAj"
      },
      "source": [
        "Decision Tree:\n",
        "\n",
        "![image.png](attachment:image.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPYnZM7tSwAj"
      },
      "source": [
        "For each criterion, we define a class with fundamental attributes/methods:\n",
        "\n",
        "0. **`OPTIMIZATION`**: this is an attribute (in this case a string) and defines whether we want to minimize or maximize the impurity.\n",
        "1. **`impurity()`**: this method calculates a score on a vector of target values.\n",
        "2. **`split_impurity()`**: this method calculates the impurity of a split, meaning the impurity of two arrays of target values, one for each side of the split.\n",
        "3. **`is_tolerance_reached()`**: this method checks if the improvement of impurity of a split is below a certain threshold.\n",
        "\n",
        "Note that based on the criterium, we may have different optimization targets, for example:\n",
        "- in the case of **Gini impurity**, we want to **minimize** the impurity\n",
        "- while in the case of **information gain**, we want to **maximize** it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "be9gFVq1SwAj"
      },
      "outputs": [],
      "source": [
        "# We can define a base class for the criterion, which tells us how a criterion should be defined\n",
        "\n",
        "# You can think of this as a \"blueprint\" to neatly define different criteria, we can also specify\n",
        "# the type of returned values for each method with a little arrow next to the method name like: -> float\n",
        "\n",
        "class Criterion:\n",
        "    OPTIMIZATION = \"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def impurity(y) -> float: ...\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right) -> float: ...\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement) -> bool: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkzfudyRSwAk"
      },
      "source": [
        "The Gini impurity is defined as:\n",
        "$$\n",
        "Gini(p) = 1 - \\sum_{i=1}^{C} p_i^2\n",
        "$$\n",
        "where $p_i$ is the proportion of samples in class $i$ and $C$ is the number of classes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "ztpBTvPNSwAk"
      },
      "outputs": [],
      "source": [
        "# The class is used to group functions, neatly packaged together, this makes the code more readable.\n",
        "\n",
        "# We could have defined normal functions like gini_impurity(...), gini_split_impurity(...), etc.\n",
        "# but that could become cumbersome if we have many other criteria.\n",
        "\n",
        "# @staticmethod means that methods work by themselves, just like normal functions\n",
        "\n",
        "# Adding Criterion in parentheses to tell Python that the base structure of the class is the one we defined before\n",
        "class GiniCriterion(Criterion):\n",
        "    OPTIMIZATION = \"minimize\"\n",
        "\n",
        "    @staticmethod\n",
        "    def impurity(y):\n",
        "        \"\"\"Gini impurity is the squared sum of the proportion/probability of each class\"\"\"\n",
        "        class_counts = np.unique(y, return_counts=True)[1]\n",
        "        probs = class_counts / len(y)\n",
        "        return 1 - np.sum(probs ** 2) # <-- gini\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right):\n",
        "        \"\"\"Weighted sum of the gini impurity of two subsets\"\"\"\n",
        "        nL = len(y_left)\n",
        "        nR = len(y_right)\n",
        "        n = nL + nR\n",
        "        g_left = GiniCriterion.impurity(y_left)\n",
        "        g_right = GiniCriterion.impurity(y_right)\n",
        "        return (nL * g_left + nR * g_right) / n\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
        "        improvement = node_impurity - split_impurity\n",
        "        return improvement < min_improvement\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlrGfiM0SwAk"
      },
      "source": [
        "Let's see an example of usage of the Criterion class (Gini in this case)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "splsHMQrSwAk",
        "outputId": "41f1c001-dc70-45a3-93f1-94c732eca281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gini impurity of y1 is 0.48\n",
            "Gini impurity of y2 is 0.00\n",
            "Gini impurity of y = concat(y1, y2) is 0.49\n",
            "Gini impurity of the split (y1, y2) is: 0.34\n"
          ]
        }
      ],
      "source": [
        "# Define an example of targets\n",
        "y1 = np.array([\"cat\", \"cat\", \"dog\", \"dog\", \"dog\"])\n",
        "\n",
        "# Access the methods of the class using the class name and the dot operator\n",
        "impurity = GiniCriterion.impurity(y1)\n",
        "print(f\"Gini impurity of y1 is {impurity:.2f}\")\n",
        "\n",
        "y2 = np.array([\"cat\", \"cat\"])\n",
        "impurity = GiniCriterion.impurity(y2)\n",
        "print(f\"Gini impurity of y2 is {impurity:.2f}\")\n",
        "\n",
        "total_impurity = GiniCriterion.impurity(np.concatenate((y1, y2)))\n",
        "print(f\"Gini impurity of y = concat(y1, y2) is {total_impurity:.2f}\")\n",
        "\n",
        "split_impurity = GiniCriterion.split_impurity(y1, y2)\n",
        "print(f\"Gini impurity of the split (y1, y2) is: {split_impurity:.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwOfDvexSwAl"
      },
      "source": [
        "A tree is in essence a set of rules that are applied to the data. We have to store the rules in a way that we can easily access them.\n",
        "\n",
        "A typical way to implement a tree is by defining a set of nodes that are related to eachother, so we also define a class `Node`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "OjdqRV4jSwAl"
      },
      "outputs": [],
      "source": [
        "# In this case, the class is just a bundle of variables, neatly packed together\n",
        "\n",
        "@dataclass\n",
        "class Node:\n",
        "    impurity: float | None          # None if leaf\n",
        "    split_impurity: float | None    # None if leaf\n",
        "    feature: int | None             # None if leaf\n",
        "    threshold: float | None         # None if leaf\n",
        "    prediction: float | None        # None if not leaf\n",
        "    n_samples: int"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EQF-4QNdSwAl"
      },
      "source": [
        "Let's see an example of a node object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RTD9r1BWSwAl"
      },
      "outputs": [],
      "source": [
        "# In this case we create an object of the class Node, since it has to store information\n",
        "node = Node(impurity=0.5, split_impurity = 0.3, feature=1, threshold=0.5, prediction=None, n_samples=10)\n",
        "\n",
        "# Access the attributes of the node with the dot operator\n",
        "print(f\"Impurity: {node.impurity}, Split Impurity: {node.split_impurity}, Feature: {node.feature}, Threshold: {node.threshold}, Prediction: {node.prediction}, Samples: {node.n_samples}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87tLg_LhSwAl"
      },
      "source": [
        "In this first implementation, we will study the **Decision Tree Classifier**, so the prediction will be a class label.\n",
        "\n",
        "Given a **leaf node** (a node with no children, so no further splits), we will assign a class label to it.\n",
        "\n",
        "The predicted label is most common class in the samples that reach that node during the training phase."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm7qGA0GSwAl"
      },
      "outputs": [],
      "source": [
        "# to make the class as general as possible, we define outside the 'node_prediction' function\n",
        "def node_prediction_classification(y):\n",
        "    \"\"\"Return the most common class in the node\"\"\"\n",
        "    classes, counts = np.unique(y, return_counts=True)\n",
        "    return classes[np.argmax(counts)]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bkbklg4jSwAl"
      },
      "source": [
        "Now we are ready to define the class of our decision tree classifier model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "f3j6hdLfSwAl"
      },
      "outputs": [],
      "source": [
        "from typing import Type\n",
        "\n",
        "\n",
        "class DecisionTree:\n",
        "    def __init__(self, criterion: Type[Criterion], node_pred_fnc, max_depth, min_impurity_improvement):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            criterion: class of the impurity criterion as defined above\n",
        "            node_pred_fnc: function to predict the class of a node, that takes as input y and return the prediction\n",
        "            max_depth: maximum depth of the tree\n",
        "            min_impurity_improvement: minimum impurity improvement to split a node\n",
        "        \"\"\"\n",
        "        # we define here this attribute to share across all the methods\n",
        "        self.criterion = criterion\n",
        "        self.node_pred_fnc = node_pred_fnc\n",
        "        self.max_depth = max_depth\n",
        "        self.min_impurity_improvement = min_impurity_improvement\n",
        "\n",
        "        # attributes to be set in fit method\n",
        "        self.categorical_feat_idxs = []\n",
        "        self.nodes = dict()\n",
        "\n",
        "    @staticmethod\n",
        "    def _split_dataset(X, feature, threshold, is_categorical):\n",
        "        if is_categorical:\n",
        "            # categorical feature\n",
        "            left_mask = X[:, feature] == threshold\n",
        "            right_mask = X[:, feature] != threshold\n",
        "        else:\n",
        "            # numerical feature\n",
        "            left_mask = X[:, feature] <= threshold\n",
        "            right_mask = X[:, feature] > threshold\n",
        "        return left_mask, right_mask\n",
        "\n",
        "    @staticmethod\n",
        "    def _child_id(node_id, is_left):\n",
        "        if is_left:\n",
        "            return 2 * node_id + 1\n",
        "        else:\n",
        "            return 2 * node_id + 2\n",
        "\n",
        "    def _find_best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split the data\"\"\"\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        best_feature:int = -1\n",
        "        best_threshold:float = 0.0\n",
        "        best_impurity = np.inf if self.criterion.OPTIMIZATION == \"minimize\" else -np.inf\n",
        "\n",
        "        # iterate over features\n",
        "        for f in range(n_features):\n",
        "            is_categorical = f in self.categorical_feat_idxs\n",
        "            split_impurity, threshold = self._find_best_threshold(X[:, f], y, is_categorical)\n",
        "\n",
        "            if self.criterion.OPTIMIZATION == \"minimize\":\n",
        "                is_better = split_impurity < best_impurity  # minimize\n",
        "            else:\n",
        "                is_better = split_impurity > best_impurity  # maximize\n",
        "\n",
        "            if is_better:\n",
        "                    best_feature = f\n",
        "                    best_threshold = threshold\n",
        "                    best_impurity = split_impurity\n",
        "\n",
        "        return best_impurity, best_feature, best_threshold\n",
        "\n",
        "    def _find_best_threshold(self, X_f, y, is_categorical):\n",
        "        \"\"\"Try all possible thresholds/categories and return the best one\"\"\"\n",
        "        unique_values = np.unique(X_f)\n",
        "        best_impurity = np.inf if self.criterion.OPTIMIZATION == \"minimize\" else -np.inf\n",
        "\n",
        "        if len(unique_values) < 2:\n",
        "            return best_impurity, unique_values[0]  # no split possible\n",
        "\n",
        "        for value in unique_values:\n",
        "            # divide according to threshold/category\n",
        "            left_mask, right_mask = self._split_dataset(X_f, ..., value, is_categorical)\n",
        "            y_left, y_right = y[left_mask], y[right_mask]\n",
        "\n",
        "            # skip if the value does not split the data\n",
        "            if len(y_left) == 0 or len(y_right) == 0:\n",
        "                continue\n",
        "\n",
        "            # evaluate impurity of the split\n",
        "            split_impurity = self.criterion.split_impurity(y_left, y_right)\n",
        "\n",
        "            if self.criterion.OPTIMIZATION == \"minimize\":\n",
        "                is_better = split_impurity < best_impurity  # minimize\n",
        "            else:\n",
        "                is_better = split_impurity > best_impurity  # maximize\n",
        "\n",
        "            if is_better:\n",
        "                best_impurity = split_impurity\n",
        "                best_threshold = value\n",
        "\n",
        "        return best_impurity, best_threshold\n",
        "\n",
        "    def _build_tree(self, X, y, depth, node_id):\n",
        "        \"\"\"Construct recursively the tree\"\"\"\n",
        "        n_samples_node = len(y)\n",
        "\n",
        "        # -- LEAF CONDITIONS --- posso splittare ancora visto le cose che mi sono arrivate?\n",
        "        # 0. All the feature values of all the samples are the same (no useful split possible)\n",
        "        # 1. The node has 0 or 1 samples (no split possible)\n",
        "        # 2. Max depth is reached\n",
        "        # 3. The impurity improvement < min_impurity_improvement (implemented after)\n",
        "\n",
        "        # (leaf condition 0)\n",
        "        X_all_same = all(len(np.unique(X[:, feat])) == 1 for feat in range(X.shape[1]))\n",
        "\n",
        "        if len(y) <= 1 or depth >= self.max_depth or X_all_same:\n",
        "            prediction = self.node_pred_fnc(y)\n",
        "            self.nodes[node_id] = Node(None, None, None, None, prediction, n_samples_node)    # leaf node\n",
        "            return\n",
        "\n",
        "        # -- Internal Node: find BEST SPLIT and create children --\n",
        "        best_split_impurity, best_feature, best_threshold = self._find_best_split(X, y)\n",
        "\n",
        "        # (leaf condition 3)\n",
        "        current_node_impurity = self.criterion.impurity(y)\n",
        "        if self.criterion.is_tolerance_reached(current_node_impurity, best_split_impurity, self.min_impurity_improvement):\n",
        "            prediction = self.node_pred_fnc(y)\n",
        "            self.nodes[node_id] = Node(None, None, None, None, prediction, n_samples_node)    # leaf node\n",
        "            return\n",
        "        # -- Save node --\n",
        "        self.nodes[node_id] = Node(current_node_impurity, best_split_impurity, best_feature, best_threshold, None, n_samples_node)\n",
        "\n",
        "        # -- Create children --\n",
        "        is_feat_cat = best_feature in self.categorical_feat_idxs\n",
        "        left_mask, right_mask = self._split_dataset(X, best_feature, best_threshold, is_feat_cat)\n",
        "\n",
        "        left_id = self._child_id(node_id, True)\n",
        "        right_id = self._child_id(node_id, False)\n",
        "\n",
        "        X_left, y_left = X[left_mask], y[left_mask]\n",
        "        self._build_tree(X_left, y_left, depth + 1, left_id)\n",
        "\n",
        "        X_right, y_right = X[right_mask], y[right_mask]\n",
        "        self._build_tree(X_right, y_right, depth + 1, right_id)\n",
        "\n",
        "    def fit(self, X, y, categorical_feat_idxs = None):\n",
        "        \"\"\"\n",
        "        Fit the decision tree to the data. The tree is built recursively by splitting the data at each node\n",
        "        based on the impurity criterion.\n",
        "        Args:\n",
        "            X: input matrix of shape (n_samples, n_features)\n",
        "            y: true target/labels of shape (n_samples,)\n",
        "            categorical_feat_idxs: list of categorical feature indices\n",
        "        \"\"\"\n",
        "        self.categorical_feat_idxs = categorical_feat_idxs if categorical_feat_idxs is not None else []\n",
        "\n",
        "        self.nodes = dict()     # Reset the nodes\n",
        "        self._build_tree(X, y, 0, 0)\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"\n",
        "        Predict the target variable for the given input data X.\n",
        "        Keep the same column order as in the training data.\n",
        "\n",
        "        Args:\n",
        "            X: input matrix of shape (n_samples, n_features)\n",
        "        \"\"\"\n",
        "        predictions = []\n",
        "        for x in X:\n",
        "            node_id = 0\n",
        "            node : Node = self.nodes[0]\n",
        "            while True:\n",
        "                if node.prediction is not None:\n",
        "                    predictions.append(node.prediction) # leaf node\n",
        "                    break\n",
        "                # else we need to go down the tree\n",
        "                if node.feature in self.categorical_feat_idxs:\n",
        "                    # categorical features\n",
        "                    is_left = x[node.feature] == node.threshold\n",
        "                else:\n",
        "                    # continuous features\n",
        "                    is_left = x[node.feature] <= node.threshold\n",
        "                node_id = self._child_id(node_id, is_left)\n",
        "                node = self.nodes[node_id]\n",
        "        return np.array(predictions)\n",
        "\n",
        "\n",
        "    def print_tree(self, node_id=0, prefx=\"\", is_left=True, feat_names=None):\n",
        "        \"\"\"\n",
        "        Prints the binary tree in a hierarchical format.\n",
        "        \"\"\"\n",
        "        if node_id not in self.nodes:\n",
        "            return\n",
        "\n",
        "        node = self.nodes[node_id]\n",
        "        conn = \"├── \" if is_left else \"└── \"\n",
        "\n",
        "        if node.feature is None:\n",
        "            print(f\"{prefx}{conn}{node_id}-Pred: {node.prediction}\")\n",
        "            return\n",
        "        else:\n",
        "            if feat_names is None:\n",
        "                f = f\"X[{node.feature}]\"\n",
        "            else:\n",
        "                f = feat_names[node.feature]\n",
        "            print(f\"{prefx}{conn}{node_id}-{f} <= {node.threshold} | Impurity:{node.impurity:.3f}\")\n",
        "\n",
        "        prefx += \"│   \" if is_left else \"    \"\n",
        "        l_id = self._child_id(node_id, is_left=True)\n",
        "        r_id = self._child_id(node_id, is_left=False)\n",
        "\n",
        "        self.print_tree(l_id, prefx, is_left=True, feat_names=feat_names)\n",
        "        self.print_tree(r_id, prefx, is_left=False, feat_names=feat_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3KOCZk-DSwAm"
      },
      "source": [
        "### Decision Tree Classification on IRIS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lAFwHWAvSwAm"
      },
      "outputs": [],
      "source": [
        "data = pd.read_csv('drive/MyDrive/AA24-25ML/iris.csv')\n",
        "display(data.head())\n",
        "\n",
        "feature_names = data.columns[:-1]\n",
        "target_col = 'species'\n",
        "\n",
        "X = data.drop(target_col, axis=1).to_numpy()\n",
        "y = data[target_col].to_numpy()\n",
        "\n",
        "print(\"X.shape_\", X.shape, \"y.shape_\", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Is7iokF7SwAm"
      },
      "outputs": [],
      "source": [
        "# define model using the Gini impurity criterion\n",
        "tree = DecisionTree(criterion=GiniCriterion,\n",
        "                    node_pred_fnc = node_prediction_classification,\n",
        "                    max_depth = 3,\n",
        "                    min_impurity_improvement=0.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-K5VjYYtSwAm"
      },
      "source": [
        "Let's test our class and find the best first split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dwsMrnI3SwAm"
      },
      "outputs": [],
      "source": [
        "best_gini, best_feature, best_threshold = tree._find_best_split(X, y)\n",
        "print(f\"Best gini impurity for the first split: {best_gini:.2e}\")\n",
        "print(f\"Best feature: '{feature_names[best_feature]}'\")\n",
        "print(f\"Best threshold: {best_threshold:.2f}\")\n",
        "\n",
        "plt.scatter(X[:, best_feature], y)\n",
        "plt.axvline(best_threshold, color=\"red\", linestyle=\"--\")\n",
        "plt.xlabel(feature_names[best_feature])\n",
        "plt.ylabel(\"species\")\n",
        "plt.title(f\"Best split for '{feature_names[best_feature]}' feature\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjsNYV8HSwAm"
      },
      "source": [
        "Find and test the binary decision tree"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_rq0ZpzqSwAm"
      },
      "outputs": [],
      "source": [
        "# fit the model\n",
        "tree.fit(X, y, categorical_feat_idxs = None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MsW-9IPcSwAm"
      },
      "outputs": [],
      "source": [
        "# print the tree\n",
        "tree.print_tree()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM-SuXFTSwAm"
      },
      "outputs": [],
      "source": [
        "# training prediction and accuracy\n",
        "y_pred = tree.predict(X)\n",
        "print(\"Training accuracy:\", accuracy(y, y_pred))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLPzfDA5SwAn"
      },
      "source": [
        "Visualize the decision boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hyh8NzErSwAn"
      },
      "outputs": [],
      "source": [
        "from utils import plot_decision_boundary_2d, create_2d_meshpoints\n",
        "\n",
        "def create_proba(dtree):\n",
        "    def tree_proba(X):\n",
        "        y_pred = dtree.predict(X)\n",
        "        y_pred = np.unique(y_pred, return_inverse=True)[1]\n",
        "        proba = np.zeros((X.shape[0], len(np.unique(y))))\n",
        "        for i, pred in enumerate(y_pred):\n",
        "            proba[i, pred] = 1\n",
        "        return proba\n",
        "    return tree_proba\n",
        "\n",
        "max_depth = 4\n",
        "print(\"Max depth:\", max_depth)\n",
        "tree2 = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "tree2.fit(X, y, categorical_feat_idxs=None)\n",
        "y_pred = tree2.predict(X)\n",
        "print(\"Train Accuracy:\", accuracy(y, y_pred))\n",
        "\n",
        "\n",
        "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, 200)\n",
        "\n",
        "probability_func = create_proba(tree2)\n",
        "\n",
        "n_features = X.shape[1]\n",
        "plot_decision_boundary_2d(\n",
        "    X_grid,\n",
        "    np.unique(y, return_inverse=True)[1],\n",
        "    probability_func,\n",
        "    xx,\n",
        "    yy,\n",
        "    X_2d,\n",
        "    n_features,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5Z1QTAWSwAn"
      },
      "source": [
        "### Decision Tree Classifier on Wine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WkrzGZvuSwAn"
      },
      "outputs": [],
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "wine = datasets.load_wine()\n",
        "assert not isinstance(wine, tuple) # optional: check that it's not a tuple\n",
        "print(wine.keys())\n",
        "\n",
        "# 'data': np array of shape (178 samples, 13 features), containing the data.\n",
        "# 'target': np array of shape (178 samples,) containing class labels (0, 1, or 2).\n",
        "# 'frame': not used, set to None. (in some datasets, it contains the csv file path)\n",
        "# 'target_names': np array of shape (3,) containing the class labels (['class_0' 'class_1' 'class_2']).\n",
        "# 'DESCR': string containing the description of the dataset.\n",
        "# 'feature_names': np array of shape (13,) containing the feature names.\n",
        "\n",
        "# It's still a tabular dataset, but instead of having the table stored in a csv file,\n",
        "# it's stored in a dictionary as a numpy array (data) and a numpy array (target) for the labels (+ additional information)\n",
        "\n",
        "\n",
        "# shuffle the data to make sure that the classes are not ordered in any way\n",
        "np.random.seed(0)\n",
        "X, y = shuffle(wine[\"data\"], wine[\"target\"], random_state=0)\n",
        "\n",
        "# divide in train and test set stratified\n",
        "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, train_prop=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5wrJ3UOxSwAn"
      },
      "outputs": [],
      "source": [
        "max_depths = [1,2,3]\n",
        "for max_depth in max_depths:\n",
        "    print(\"Max depth: \", max_depth)\n",
        "    tree = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "    tree.fit(X_train, y_train, categorical_feat_idxs=None)\n",
        "    y_pred = tree.predict(X_test)\n",
        "    print(\"  Test accuracy:\", accuracy(y_test, y_pred))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FLilC5PuSwAn"
      },
      "outputs": [],
      "source": [
        "# decision boundaries\n",
        "for max_depth in max_depths:\n",
        "    tree = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "    tree.fit(X_train, y_train, categorical_feat_idxs=None)\n",
        "\n",
        "    X_grid, xx, yy, X_2d = create_2d_meshpoints(X, 200)\n",
        "\n",
        "    probability_func = create_proba(tree)\n",
        "\n",
        "    n_features = X_train.shape[1]\n",
        "    plot_decision_boundary_2d(\n",
        "        X_grid,\n",
        "        np.unique(y, return_inverse=True)[1],\n",
        "        probability_func,\n",
        "        xx,\n",
        "        yy,\n",
        "        X_2d,\n",
        "        n_features,\n",
        "        title=f\"Decision boundary | max_depth={max_depth}\",\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8BqGGx7SwAn"
      },
      "source": [
        "### Post Hoc Pruning: Cost-Complexity Pruning\n",
        "\n",
        "As in CART approach, we will use a post-hoc pruning method.\n",
        "\n",
        "The idea is to prune the tree after it has been fully grown.\n",
        "\n",
        "The pruning is done by removing subtrees that minimally improve the impurity of the tree.\n",
        "\n",
        "> The impurity is calculated on a separate validation set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LJhENyXVSwAn"
      },
      "outputs": [],
      "source": [
        "def compute_cost_complexity(X, y, node_id, tree, ccps, total_N):\n",
        "    node = tree.nodes[node_id]\n",
        "\n",
        "    # Compute node impurity\n",
        "    node_impurity = tree.criterion.impurity(y) * len(y) / total_N\n",
        "\n",
        "    if node.feature is None:  # Leaf node\n",
        "        return node_impurity, 1  # (impurity, number of leaves)\n",
        "\n",
        "    # This is a parent node: Split dataset and process children\n",
        "    left_id = tree._child_id(node_id, is_left=True)\n",
        "    right_id = tree._child_id(node_id, is_left=False)\n",
        "    left_mask, right_mask = tree._split_dataset(\n",
        "        X, node.feature, node.threshold, node.feature in tree.categorical_feat_idxs\n",
        "    )\n",
        "\n",
        "    left_impurity, left_leaves = compute_cost_complexity(\n",
        "        X[left_mask], y[left_mask], left_id, tree, ccps, total_N\n",
        "    )\n",
        "    right_impurity, right_leaves = compute_cost_complexity(\n",
        "        X[right_mask], y[right_mask], right_id, tree, ccps, total_N\n",
        "    )\n",
        "\n",
        "    total_leaves = left_leaves + right_leaves\n",
        "\n",
        "    impurity_sum = left_impurity + right_impurity\n",
        "    ccps[node_id] = (node_impurity - impurity_sum) / (total_leaves - 1)\n",
        "\n",
        "    return impurity_sum, total_leaves"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3nIKD1qSwAn"
      },
      "source": [
        "Iris example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-3obBaBBSwAn"
      },
      "outputs": [],
      "source": [
        "# load the iris dataset\n",
        "data = pd.read_csv(\"drive/MyDrive/AA24-25ML/iris.csv\")\n",
        "feature_names, target_col = data.columns[:-1], \"species\"\n",
        "X = data.drop(target_col, axis=1).to_numpy()\n",
        "y = data[target_col].to_numpy()\n",
        "\n",
        "# define model using the Gini impurity criterion\n",
        "tree = DecisionTree(\n",
        "    criterion=GiniCriterion,\n",
        "    node_pred_fnc=node_prediction_classification,\n",
        "    max_depth=3,\n",
        "    min_impurity_improvement=0.0,\n",
        ")\n",
        "\n",
        "# fit the model\n",
        "tree.fit(X, y, categorical_feat_idxs=None)\n",
        "\n",
        "subtrees_impurities = {}\n",
        "compute_cost_complexity(X, y, 0, tree, subtrees_impurities, total_N=len(y))\n",
        "\n",
        "# keep only the internal nodes\n",
        "subtrees_impurities = {node_id: imp for node_id, imp in subtrees_impurities.items() if tree.nodes[node_id].feature is not None}\n",
        "\n",
        "\n",
        "for node_id, imp in subtrees_impurities.items():  # Reusing the same impurity dictionary\n",
        "    print(f\"Subtree {node_id} imp: {imp:.4f}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "tree.print_tree()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dNFx3g4qSwAx"
      },
      "source": [
        "Now that we know the subtrees that less improve the impurity, we can prune them with the following function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9hSgJqPwSwAx"
      },
      "outputs": [],
      "source": [
        "# given the node_id, we can remove the node and all its children\n",
        "# to get the prediction, we still need the training dataset\n",
        "def prune_tree(X, y, node_id, tree, ids_to_prune):\n",
        "    node = tree.nodes[node_id]\n",
        "\n",
        "    if node_id in ids_to_prune:\n",
        "        # Turn this node into a leaf\n",
        "        tree.nodes[node_id] = Node(None, None, None, None, tree.node_pred_fnc(y), node.n_samples)\n",
        "        return\n",
        "\n",
        "    if node.feature is None:  # Leaf node\n",
        "        return\n",
        "\n",
        "    left_mask, right_mask = tree._split_dataset(\n",
        "        X, node.feature, node.threshold, node.feature in tree.categorical_feat_idxs\n",
        "    )\n",
        "    left_id = tree._child_id(node_id, is_left=True)\n",
        "    right_id = tree._child_id(node_id, is_left=False)\n",
        "    prune_tree(X[left_mask], y[left_mask], left_id, tree, ids_to_prune)\n",
        "    prune_tree(X[right_mask], y[right_mask], right_id, tree, ids_to_prune)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CfJUHPNoSwAx"
      },
      "source": [
        "Let's see how the accuracy changes with varying the number of pruned subtrees."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRZWXqGqSwAx"
      },
      "outputs": [],
      "source": [
        "tree = DecisionTree(\n",
        "    criterion=GiniCriterion,\n",
        "    node_pred_fnc=node_prediction_classification,\n",
        "    max_depth=3,\n",
        "    min_impurity_improvement=0.0,\n",
        ")\n",
        "\n",
        "# order the ccps by increasing ccp\n",
        "sorted_ccps = sorted(subtrees_impurities.items(), key=lambda x: x[1])\n",
        "accuracies = []\n",
        "for i in range(len(sorted_ccps)):\n",
        "    nodes_to_prune = [node_id for node_id, _ in sorted_ccps[:i + 1]]\n",
        "    print(f\"Pruning nodes: {nodes_to_prune}\")\n",
        "    # start from the original tree\n",
        "    tree.fit(X, y, categorical_feat_idxs=None)\n",
        "    # prune the tree\n",
        "    prune_tree(X, y, 0, tree, nodes_to_prune)\n",
        "    # get the predictions\n",
        "    y_pred = tree.predict(X)\n",
        "    # compute the accuracy\n",
        "    acc = accuracy(y, y_pred)\n",
        "    print(f\"Accuracy after pruning nodes {nodes_to_prune}: {acc:.4f}\")\n",
        "    # plot the tree\n",
        "    tree.print_tree()\n",
        "    print(\"-\" * 50)\n",
        "    accuracies.append((acc, len(nodes_to_prune)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OwtwwhjcSwAx"
      },
      "outputs": [],
      "source": [
        "accuracies = np.array(accuracies)\n",
        "plt.plot(accuracies[:, 1], accuracies[:, 0], marker=\"o\")\n",
        "plt.xlabel(\"Number of pruned nodes\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Accuracy vs Number of pruned nodes\")\n",
        "plt.grid()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr344HUoSwAx"
      },
      "source": [
        "## Random Forest"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q2YKSxv5SwAx"
      },
      "source": [
        "Decision trees offer advantages in terms of interpretability and computational efficiency. However, relying on a **single tree** for classification can lead to **high variance** and overfitting.\n",
        "\n",
        "To address this, we build an **ensemble** of multiple decision trees, known as a **forest**. Instead of training just one tree, we train several trees on **slightly different versions** of the training dataset. At test time, we predict the class that receives the majority vote across all trees.\n",
        "\n",
        "What does **slightly different versions** of the training dataset mean?\n",
        "\n",
        "Each tree in the forest is trained on a **randomly sampled subset** of the training data. This randomness introduces diversity among the trees, reducing variance and improving generalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UiDnJu4vSwAx"
      },
      "source": [
        "### Bagging (Bootstrap Aggregating)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o1EIcE1BSwAy"
      },
      "source": [
        "Bagging involves creating **bootstrap samples** from the original training dataset. This means:\n",
        "\n",
        "- Given a dataset with $n$ samples, we randomly select $n$ samples **with replacement** to create a new training set.\n",
        "- Since sampling is done **with replacement**, some samples may appear multiple times, while others may not appear at all.\n",
        "\n",
        "A useful analogy:  \n",
        "> Imagine an urn containing all the training samples. You randomly pick a sample, record it in the new dataset, then **return it to the urn** before drawing again. This process continues until you have selected $n$ samples.\n",
        "\n",
        "Each tree in the forest is trained on a different bootstrap sample, ensuring variability in the dataset used for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xMxqC9TISwAy"
      },
      "outputs": [],
      "source": [
        "def bootstrap_sample(X, y):\n",
        "    # select n samples from X with replacement\n",
        "    # first parameter: values among which we are selecting, second parameter: number of values that we are\n",
        "    # selecting, third parameter: with replacement\n",
        "    indices = np.random.choice(len(X), size=len(X), replace=True)\n",
        "    # filter X and y based on indices\n",
        "    return X[indices], y[indices]\n",
        "\n",
        "def fit_bagging(X, y, T, max_depth):\n",
        "    \"\"\"Fit T trees using bagging\"\"\"\n",
        "    # forest\n",
        "    trees = []\n",
        "    for j in range(T):\n",
        "        # alternative dataset\n",
        "        X_sample, y_sample = bootstrap_sample(X, y)\n",
        "        # initialize the tree\n",
        "        tree = DecisionTree(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "        # fit the tree\n",
        "        tree.fit(X_sample, y_sample, categorical_feat_idxs=None)\n",
        "        # add the tree to the forest\n",
        "        trees.append(tree)\n",
        "    return trees\n",
        "\n",
        "def predict_bagging(X, trees):\n",
        "    # matrix where each row corresponds to a different tree (each column correspond to a different sample)\n",
        "    predictions = np.array([tree.predict(X) for tree in trees]) # Shape (num_trees, num_samples)\n",
        "    y_pred = []\n",
        "    confidences = []\n",
        "    # iterate through the columns\n",
        "    for i in range(predictions.shape[1]):\n",
        "        # for column (sample) i, consider all rows (all predictions of all trees)\n",
        "        sample_predictions = predictions[:, i]\n",
        "        # get the number of times each class appears in the column\n",
        "        # (array containing the number of times class 0 appears, the number of times class 1 appears ...)\n",
        "        class_counts = np.bincount(sample_predictions)\n",
        "        # get the class that appears most frequently\n",
        "        majority_class = np.argmax(class_counts)\n",
        "        # we compute the confidence interval as the proportion of times that the majority class gets predicted\n",
        "        confidence = class_counts[majority_class] / len(trees)\n",
        "        # add the prediction to the prediction list\n",
        "        y_pred.append(majority_class)\n",
        "        # add the confidence to the confidence list\n",
        "        confidences.append(confidence)\n",
        "    return np.array(y_pred), np.array(confidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cfwWCRR_SwAy"
      },
      "outputs": [],
      "source": [
        "# number of trees in the forest\n",
        "T = 4\n",
        "# depth of each tree\n",
        "max_depth = 2\n",
        "\n",
        "accuracies = []\n",
        "\n",
        "# train the forest\n",
        "trees = fit_bagging(X_train, y_train, T, max_depth)\n",
        "# get the vector of predictions\n",
        "y_pred, confidences = predict_bagging(X_test, trees)\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Predictions: \", y_pred)\n",
        "print(\"True labels: \", y_test)\n",
        "print(\"Confidence of the predictions: \", confidences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_mq9X4ZSwAy"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_pred)):\n",
        "    if y_pred[i] != y_test[i]:\n",
        "        print(\"Index: \", i)\n",
        "        print(\"Prediction: \", y_pred[i])\n",
        "        print(\"True label: \", y_test[i])\n",
        "        print(\"Confidence of the prediction: \", confidences[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmR_y2NtSwAy"
      },
      "outputs": [],
      "source": [
        "from tqdm import trange  # to print a progress bar\n",
        "\n",
        "np.random.seed(0)\n",
        "\n",
        "# we make the same computation several times, reporting the mean and variance of the accuracy metric\n",
        "trials = 5\n",
        "# number of trees in the forest\n",
        "T_sizes = [1, 4, 8, 16]\n",
        "# depth of each tree\n",
        "max_depth = 4\n",
        "\n",
        "trial_accuracies = []\n",
        "for trial in trange(trials, desc=\"Trials\", leave=True): # while this loop is computed, print a progress bar\n",
        "    accuracies = []\n",
        "    for T in T_sizes:\n",
        "        # train the forest\n",
        "        trees = fit_bagging(X_train, y_train, T, max_depth)\n",
        "        # get the vector of predictions\n",
        "        y_pred, confidences = predict_bagging(X_test, trees)\n",
        "        # compute the accuracy\n",
        "        acc = accuracy(y_test, y_pred)\n",
        "        accuracies.append(acc)\n",
        "    trial_accuracies.append(accuracies)\n",
        "\n",
        "# Calculate mean and std of accuracies\n",
        "mean_accuracies = np.mean(trial_accuracies, axis=0)\n",
        "std_accuracies = np.std(trial_accuracies, axis=0)\n",
        "\n",
        "# Plot accuracies over T_sizes with error bars\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.errorbar(\n",
        "    T_sizes, mean_accuracies, yerr=std_accuracies, fmt=\"o-\", color=\"b\", label=\"Accuracy\"\n",
        ")\n",
        "plt.xlabel(\"Number of Trees (T)\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.title(\"Bagging: Accuracy vs Number of Trees\")\n",
        "plt.grid(True)\n",
        "plt.xticks(T_sizes)\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zlSQbFQoSwAy"
      },
      "source": [
        "### Feature Bagging"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDyymCCrSwAy"
      },
      "source": [
        "If certain features are **strong predictors**, they are likely to be selected repeatedly across multiple trees. This can lead to high correlation among the trees, reducing the effectiveness of the ensemble.\n",
        "\n",
        "To mitigate this, we introduce **feature bagging**:\n",
        "\n",
        "- At **each split** within each tree, we randomly select a **subset of features** instead of considering all features.\n",
        "- The number of features selected at each split is typically **$\\sqrt{p}$**, where $p$ is the total number of features.\n",
        "\n",
        "This forces each tree to explore different features, reducing overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOKjK9F3SwAy"
      },
      "source": [
        "Unlike **Bootstrap Aggregating (Bagging)**, where we only modify the dataset used to train each tree, **feature bagging** alters the way the tree itself is built. Specifically, the sampling of $\\sqrt{p}$ features occurs *at every split*, meaning that the best split is chosen only from the sampled features. As a result, we must modify the split function.  \n",
        "\n",
        "Fortunately, we can leverage the advantages of classes to implement this efficiently. Instead of rewriting all functions, we can create a class that inherits from `DecisionTree` and override only the split function and the constructor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ojtXU0f0SwAy"
      },
      "outputs": [],
      "source": [
        "# we inherit the DecisionTree class and modify the find_best_split method and the constructor\n",
        "class DecisionTreeFeatureBagging(DecisionTree):     # we are inherinting all attributes and methods from DecisionTree\n",
        "    def _find_best_split(self, X, y):\n",
        "        \"\"\"Find the best feature and threshold to split the data\"\"\"\n",
        "        n_features = X.shape[1]\n",
        "\n",
        "        # change!\n",
        "        # if we perform feature bagging, we select np.sqrt(p) features\n",
        "        subset_feature_size = max(1, int(np.sqrt(n_features)))\n",
        "        selected_features = np.random.choice(n_features, size=subset_feature_size, replace=False)\n",
        "\n",
        "        best_feature, best_threshold = None, None\n",
        "        best_impurity = np.inf if self.criterion.OPTIMIZATION == \"minimize\" else -np.inf\n",
        "\n",
        "        # iterate over features\n",
        "        for f in selected_features:         # change! (do not consider all features, only the selected ones)\n",
        "            is_categorical = f in self.categorical_feat_idxs\n",
        "            split_impurity, threshold = self._find_best_threshold(X[:, f], y, is_categorical)\n",
        "\n",
        "            if self.criterion.OPTIMIZATION == \"minimize\":\n",
        "                if split_impurity < best_impurity:\n",
        "                    best_feature = f\n",
        "                    best_threshold = threshold\n",
        "                    best_impurity = split_impurity\n",
        "            else: # maximize\n",
        "                if split_impurity > best_impurity:\n",
        "                    best_feature = f\n",
        "                    best_threshold = threshold\n",
        "                    best_impurity = split_impurity\n",
        "\n",
        "        return best_impurity, best_feature, best_threshold\n",
        "\n",
        "\n",
        "# redefine the fit_bagging function to use the new class\n",
        "def fit_bagging_feature_bagging(X, y, T, max_depth):\n",
        "    \"\"\"Fit T trees using bagging and Decision Tree with feature baggingS\"\"\"\n",
        "    trees = []\n",
        "    for j in range(T):\n",
        "        X_sample, y_sample = bootstrap_sample(X, y)\n",
        "        tree = DecisionTreeFeatureBagging(GiniCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "        tree.fit(X_sample, y_sample, categorical_feat_idxs=None)\n",
        "        trees.append(tree)\n",
        "    return trees"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2CipdmiqSwAy"
      },
      "outputs": [],
      "source": [
        "# example, the number of trees in the forest is fixed, we vary the depth of the trees\n",
        "T = 10\n",
        "max_depths = [2, 4, 10]\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    trees = fit_bagging_feature_bagging(X_train, y_train, T, max_depth)\n",
        "    y_pred, confidences = predict_bagging(X_test, trees)\n",
        "    print(\"Max depth:\", max_depth)\n",
        "    print(\"     Bagging Test accuracy:\", accuracy(y_test, y_pred))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XomkSZkSSwAy"
      },
      "source": [
        "## Gradient Boosting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0rdmHzxSwAy"
      },
      "source": [
        "Ensemble method: we combine several **weak learners** to produce a stronger model. The weak learners are **decision trees**.\n",
        "\n",
        "### Key Differences from Random Forest\n",
        "\n",
        "- Weak learners (decision trees) are trained *iteratively* rather than independently.\n",
        "\n",
        "- Each weak learner is trained on **residuals** or **pseudo-residuals**, i.e., errors of the predictions or values derived from the gradient of the loss function.\n",
        "\n",
        "### **Algorithm Overview**\n",
        "\n",
        "1. **Initialization**  \n",
        "\n",
        "   Start with a simple, naive prediction, denoted as $F_0(\\mathbf{x})$, typically a constant value for all samples.\n",
        "\n",
        "2. **Compute Residuals or Pseudo-Residuals**  \n",
        "\n",
        "   - **For regression**: Compute residuals as  \n",
        "     $$\n",
        "     r_i = y_i - F_m(\\mathbf{x}_i)\n",
        "     $$\n",
        "\n",
        "   - **For classification**: Compute pseudo-residuals based on the gradient of the loss function:  \n",
        "     $$\n",
        "     r_i = \\frac{\\partial L(y_i, p_i)}{\\partial F_m(\\mathbf{x}_i)}\n",
        "     $$\n",
        "     where $L(y, p)$ is the loss function (e.g., log likelihood for classification).\n",
        "\n",
        "3. **Train a Weak Learner**  \n",
        "\n",
        "   Fit a decision tree $h_m(\\mathbf{x})$ to predict the residuals or pseudo-residuals.\n",
        "\n",
        "4. **Update the Model**  \n",
        "\n",
        "   Update the ensemble (target prediction) by adding the scaled predictions of the new tree:  \n",
        "   $$\n",
        "   F_{m+1}(\\mathbf{x}) = F_m(\\mathbf{x}) + \\eta \\cdot h_m(\\mathbf{x})\n",
        "   $$\n",
        "\n",
        "5. **Repeat**  \n",
        "\n",
        "   Repeat steps 2–4 for a predefined number of iterations.\n",
        "\n",
        "### **Final Prediction**\n",
        "\n",
        "- **For regression**:  \n",
        "\n",
        "  $$\n",
        "  \\hat{y}(x) = F_M(x) = F_0(x) + \\sum_{m = 1}^M{h_m(x)}\n",
        "  $$\n",
        "- **For classification**: Convert log-odds into probabilities using the sigmoid function:  \n",
        "\n",
        "  $$\n",
        "  p(x) = \\sigma(F_M(x))\n",
        "  $$\n",
        "  where $\\sigma$ is the sigmoid function for binary classification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaLBUlZTSwAz"
      },
      "outputs": [],
      "source": [
        "# Gradient Boosting for classification (the dataset is still wine)\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "#set the seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# n_estimators: number of weak learners\n",
        "# learning_rate: eta parameter\n",
        "# max depth: of the weak learners\n",
        "# random_state: for reproducibility\n",
        "gb_model = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3, random_state=0)\n",
        "\n",
        "# We train the model on the training set\n",
        "gb_model.fit(X_train, y_train)\n",
        "\n",
        "# We infer the predictions on the test set\n",
        "y_pred = gb_model.predict(X_test)\n",
        "\n",
        "# Compute and print the accuracy\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bKE8uEOkSwAz"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCKxQqWKSwAz"
      },
      "source": [
        "Advanced version of Gradient Boosting.\n",
        "\n",
        "**Key Innovations**\n",
        "\n",
        "- **Regularization**: Introduces **L1 (Lasso)** and **L2 (Ridge)** regularization to prevent overfitting and improve generalization.\n",
        "\n",
        "- **Computational Efficiency**: Uses a **histogram-based algorithm** for split finding, making training significantly faster.\n",
        "\n",
        "- **Tree Pruning**: Implements **depth-first pruning**, which eliminates unnecessary splits and improves training efficiency.\n",
        "\n",
        "- **Adaptive Learning Rate**: to prevent overfitting.\n",
        "\n",
        "- **Column and row subsampling**: adding randomness to improve model robustness.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbpzgOs7SwAz"
      },
      "outputs": [],
      "source": [
        "from xgboost import XGBClassifier\n",
        "#set the seed\n",
        "np.random.seed(0)\n",
        "\n",
        "# Initialize the model with similar parameters\n",
        "xgb_model = XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=0, device=\"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "69VPkWGtSwAz"
      },
      "outputs": [],
      "source": [
        "# Train the model on the training set\n",
        "xgb_model.fit(X_train, y_train)\n",
        "\n",
        "# Infer the predictions on the test set\n",
        "y_pred = xgb_model.predict(X_test)\n",
        "\n",
        "# Compute and print the accuracy\n",
        "acc = accuracy(y_test, y_pred)\n",
        "print(f\"Test Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGXT5wluSwAz"
      },
      "source": [
        "## Feature Importance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iUWb5vXSwAz"
      },
      "source": [
        "**Feature importance** quantifies how much each feature each feature contributed to the model predictions. In this case, it is **global**, namely it measures the impact of each feature across the entire dataset.\n",
        "\n",
        "In decision tree or tree-based ensembles, feature importance is calculated **after training** and it is based on the **impurity reduction** at each split.\n",
        "\n",
        "⚠️ To be consistent with sklearn, the impurity reduction of a node is computed as:\n",
        "\n",
        "\n",
        "$$\n",
        "\\text{impurity reduction} = \\frac{n_p}{n} \\big(\\text{impurity(node)} - \\frac{n_L}{n_p} \\cdot \\text{impurity(left)} - \\frac{n_R}{n_p} \\cdot \\text{impurity(right)}\\big)\n",
        "$$\n",
        "where:\n",
        "- $n_p$ is the number of samples in the node\n",
        "- $n_L$ and $n_R$ are the number of samples in the left and right child\n",
        "- $n$ is the number of samples at the root.\n",
        "\n",
        "How feature importance is computed?\n",
        "1. **Evaluate splits**: calculate the impurity reduction associated to each split/node\n",
        "2. **Assign the impact to the feature**: each time a feature is used for a split, attribute the impurity reduction to that feature\n",
        "3. **Aggregate contributions**: sum the impurity reductions for each feature across all the splits and the trees it was used.\n",
        "4. (Optional) **Normalize**: scale the importance scores so that they sum 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vgyrAmC8SwAz"
      },
      "outputs": [],
      "source": [
        "def feature_importance(tree, features):\n",
        "    \"\"\"\n",
        "    Evaluate feature importance for each feature in the tree.\n",
        "\n",
        "    Args:\n",
        "        tree: DecisionTree object\n",
        "        features: list of feature names\n",
        "\n",
        "    Returns:\n",
        "        importances: array of feature importances, same length as features\n",
        "    \"\"\"\n",
        "    importances = np.zeros(len(features))\n",
        "    n_samples_root = tree.nodes[0].n_samples\n",
        "    # iterate over nodes in the tree\n",
        "    for node_id, node in tree.nodes.items():\n",
        "        # check if it is not a leaf node\n",
        "        if node.feature is not None:\n",
        "            impurity_reduction = node.n_samples/n_samples_root * (node.impurity - node.split_impurity)\n",
        "            # Note: if we use Information Gain (for classification) or Variance Reduction\n",
        "            # (for regression) the impurity reduction is already computed by split_impurity\n",
        "\n",
        "            # update the importance of the feature used in the node\n",
        "            importances[node.feature] += impurity_reduction\n",
        "\n",
        "    # normalize the importances\n",
        "    importances = importances / np.sum(importances)\n",
        "    return importances\n",
        "\n",
        "\n",
        "# Example usage with the wine dataset\n",
        "feature_names = wine[\"feature_names\"]\n",
        "n_total_samples = len(X_train)\n",
        "tree_fi = DecisionTree(GiniCriterion, node_prediction_classification, max_depth=4, min_impurity_improvement=0.0)\n",
        "tree_fi.fit(X_train, y_train, categorical_feat_idxs=None)\n",
        "\n",
        "importances = feature_importance(tree_fi, feature_names)\n",
        "df_fi = pd.DataFrame({\"Feature\": feature_names, \"Importance\": importances})\n",
        "display(df_fi)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6nihHUQzSwAz"
      },
      "source": [
        "## 🏋️‍♀️ Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UODl5n2eSwAz"
      },
      "source": [
        "### Alternative Impurity Measures\n",
        "Complete the <code>EntropyCriterion</code> and the <code>InformationGainCriterion</code> and test your implementations in **Iris** and **Wine** datasets."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ECIKp1AnSwAz"
      },
      "outputs": [],
      "source": [
        "# COMPLETE THE CODE\n",
        "\n",
        "class EntropyCriterion:\n",
        "    OPTIMIZATION = \"minimize\"\n",
        "\n",
        "    @staticmethod\n",
        "    def impurity(y):\n",
        "        \"\"\"Entropy is the sum of the probability of each class times the log2 of that probability\"\"\"\n",
        "        class_counts = ...\n",
        "        probs = class_counts / len(y)\n",
        "        entropy = ...\n",
        "        return ...\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right):\n",
        "        \"\"\"Weighted sum of the entropy of two subsets\"\"\"\n",
        "        nL = len(y_left)\n",
        "        nR = len(y_right)\n",
        "        n = nL + nR\n",
        "        e_left = ... # entropy of the left subset\n",
        "        e_right = ... # entropy of the right subset\n",
        "        weighted_sum = ... # weighted sum of the entropy of two subsets\n",
        "        return weighted_sum\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
        "        \"\"\"The improvement is the difference between the node impurity and the split impurity\"\"\"\n",
        "        improvement = node_impurity - split_impurity\n",
        "        return improvement < min_improvement\n",
        "\n",
        "class InformationGainCriterion(Criterion):\n",
        "    OPTIMIZATION = \"maximize\"\n",
        "\n",
        "    @staticmethod\n",
        "    def impurity(y):\n",
        "        \"\"\"Entropy is the sum of the probability of each class times the log2 of that probability\"\"\"\n",
        "        class_counts = ...\n",
        "        probs = class_counts / len(y)\n",
        "        entropy = ...\n",
        "        return ...\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right):\n",
        "        \"\"\"Information gain is the difference between the entropy of the node and the weighted sum of the entropy of the two subsets\"\"\"\n",
        "        nL = len(y_left)\n",
        "        nR = len(y_right)\n",
        "        n = nL + nR\n",
        "        y_parent = np.concatenate((y_left, y_right))\n",
        "        e_node = ... # entropy (impurity) of the parent\n",
        "        e_left = ... # entropy (impurity) of the left child\n",
        "        e_right = ... # entropy (impurity) of the right child\n",
        "        weighted_sum = ... # weighted sum of the entropy of the two subsets\n",
        "        information_gain = ... # difference between the entropy of the node and the weighted sum of the entropy of the two subsets\n",
        "        return information_gain\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
        "        \"\"\"The improvement is the split impurity\"\"\"\n",
        "        improvement = split_impurity\n",
        "        return improvement < min_improvement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixf8gsJISwAz"
      },
      "outputs": [],
      "source": [
        "# COMPLETE THE CODE\n",
        "\n",
        "# Load the IRIS dataset\n",
        "np.random.seed(0)\n",
        "data = pd.read_csv('drive/MyDrive/AA24-25ML/iris.csv')\n",
        "target_col = 'species'\n",
        "feature_names = data.columns[:-1]\n",
        "\n",
        "X = data.drop(target_col, axis=1).to_numpy()\n",
        "y = data[target_col].to_numpy()\n",
        "print(\"X.shape_\", X.shape, \"y.shape_\", y.shape)\n",
        "\n",
        "# split the dataset in training and test set\n",
        "X_train, X_test, y_train, y_test = ...\n",
        "\n",
        "max_depth = 3\n",
        "\n",
        "tree_entropy = DecisionTree(EntropyCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "# fit the tree on the training set\n",
        "...\n",
        "# get the predictions on the test set\n",
        "y_pred = ...\n",
        "# get the accuracy\n",
        "accuracy_entropy = ...\n",
        "print(\"Test accuracy with EntropyCriterion on IRIS \", accuracy_entropy)\n",
        "#print the tree\n",
        "...\n",
        "\n",
        "tree_infgain = DecisionTree(InformationGainCriterion, node_prediction_classification, max_depth, 0.0)\n",
        "# fit the tree on the training set\n",
        "...\n",
        "# get the predictions on the test set\n",
        "y_pred = ...\n",
        "# get the accuracy\n",
        "accuracy_infgain = ...\n",
        "print(\"Test accuracy with InformationGainCriterion on IRIS \", accuracy_infgain)\n",
        "#print the tree\n",
        "...\n",
        "\n",
        "# wine is the same"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QHx_9d1rSwA0"
      },
      "source": [
        "### Exercise 2: gradient boosting with regression\n",
        "1. Decision Tree can be used also for regression if the criterion classes and the node prediction function are properly defined. Define the MSECriterion and VarianceReductionCriterion classes and node_prediction_regression function.\n",
        "\n",
        "2. Define a function to fit gradient boosting and one to predict gradient boosting similarly to what is done for bagging.\n",
        "\n",
        "3. Test on housing_kaggle.csv"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XaTM6DiFSwA0"
      },
      "outputs": [],
      "source": [
        "# COMPLETE THE CODE\n",
        "\n",
        "class MSECriterion(Criterion):\n",
        "    OPTIMIZATION = \"minimize\"\n",
        "\n",
        "    @staticmethod\n",
        "    def impurity(y):\n",
        "        \"\"\"MSE between the targets and the node prediction which is the mean of the targets\"\"\"\n",
        "        return ...\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right):\n",
        "        \"\"\"Weighted sum of the MSE of two subsets\"\"\"\n",
        "        nL = len(y_left)\n",
        "        nR = len(y_right)\n",
        "        n = nL + nR\n",
        "        mse_left = ... # MSE of the left subset\n",
        "        mse_right = ... # MSE of the right subset\n",
        "        weighted_sum = ...\n",
        "        return weighted_sum\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
        "        \"\"\"The improvement is the difference between the node impurity and the split impurity\"\"\"\n",
        "        improvement = ...\n",
        "        return improvement < min_improvement\n",
        "\n",
        "class VarianceReductionCriterion(Criterion):\n",
        "    OPTIMIZATION = \"maximize\"\n",
        "    @staticmethod\n",
        "    def impurity(y):\n",
        "        \"\"\"Variance and MSE are equal if the prediction is the mean\"\"\"\n",
        "        return np.mean((y - np.mean(y)) ** 2)\n",
        "\n",
        "    @staticmethod\n",
        "    def split_impurity(y_left, y_right):\n",
        "        \"\"\"Variance reduction is the difference between the variance of the node and the weighted sum of the variance of the two subsets\"\"\"\n",
        "        nL = len(y_left)\n",
        "        nR = len(y_right)\n",
        "        n = nL + nR\n",
        "        y_parent = np.concatenate((y_left, y_right))\n",
        "        var_node = ... # variance of the node\n",
        "        var_left = ... # vairance of the left subset\n",
        "        var_right = ... # variance of the right subset\n",
        "        weighted_sum = ... # weighted sum of the variance of the two subsets\n",
        "        variance_reduction = ... # difference between the variance of the node and the weighted sum of the variance of the two subsets\n",
        "        return variance_reduction\n",
        "\n",
        "    @staticmethod\n",
        "    def is_tolerance_reached(node_impurity, split_impurity, min_improvement):\n",
        "        \"\"\"The improvement is the split impurity\"\"\"\n",
        "        improvement = split_impurity\n",
        "        return improvement < min_improvement\n",
        "\n",
        "\n",
        "# prediction function for regression\n",
        "def node_prediction_regression(y):\n",
        "    \"\"\"Return the mean of the targets in the node\"\"\"\n",
        "    return np.mean(y)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-ibKoxBSwA0"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "data = pd.read_csv(\"drive/MyDrive/AA24-25ML/housing_kaggle.csv\")\n",
        "# remove nan values\n",
        "data = data.dropna()\n",
        "\n",
        "data = data.sample(n=2000, random_state=0)\n",
        "X_df = data.drop(columns=[\"median_house_value\"])\n",
        "y_df = data[\"median_house_value\"]\n",
        "display(X_df.head())\n",
        "\n",
        "X = X_df.to_numpy()\n",
        "y = y_df.to_numpy()\n",
        "\n",
        "features = X_df.columns\n",
        "cat_features_idx = [8]\n",
        "\n",
        "shuffled_indices = np.random.permutation(len(X))\n",
        "train_prop = 0.8\n",
        "X_train, y_train = X[shuffled_indices[:int(len(X) * train_prop)]], y[shuffled_indices[:int(len(X) * train_prop)]]\n",
        "X_test, y_test = X[shuffled_indices[int(len(X) * train_prop):]], y[shuffled_indices[int(len(X) * train_prop):]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nh99StrSwA0"
      },
      "outputs": [],
      "source": [
        "# Try only one decision tree regressor\n",
        "treeReg = DecisionTree(criterion=MSECriterion, node_pred_fnc=node_prediction_regression, max_depth=5, min_impurity_improvement=0.0)\n",
        "treeReg.fit(X_train, y_train, categorical_feat_idxs=cat_features_idx)\n",
        "\n",
        "y_pred = treeReg.predict(X_test)\n",
        "r2_test = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "print(\"Test R2 score with MSECriterion on Housing dataset: \", r2_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPaI1c9QSwA0"
      },
      "source": [
        "### Gradient Boosting\n",
        "\n",
        "Previously, we tested gradient boosting on regression using the functions of scikit learn. Now, we test it on regression implementing the functions by hand, and using the class Decision Tree with the MSE criterion."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BloHTQIVSwA0"
      },
      "outputs": [],
      "source": [
        "# COMPLETE THE CODE\n",
        "\n",
        "def fit_gradient_boosting_regression(X_train, y_train, X_test, y_test, n_estimators=100, learning_rate=0.1, max_depth=3):\n",
        "\n",
        "    # Step 1: Initialize with a constant value (mean of y_train)\n",
        "    F_m = np.full_like(y_train, np.mean(y_train), dtype=np.float64) # same shape of y_train, all values are np.mean(y_train)\n",
        "\n",
        "    # Store weak learners\n",
        "    trees = []\n",
        "\n",
        "    for m in range(n_estimators):\n",
        "        # Step 2: Compute residuals\n",
        "        residuals = ...\n",
        "\n",
        "        # Step 3. Fit a weak learner to the residuals\n",
        "        tree = DecisionTree(MSECriterion, node_prediction_regression, max_depth, 0.0)\n",
        "        # fit the tree\n",
        "        ...\n",
        "        trees.append(tree)\n",
        "        # retrieve the predictions on the test set\n",
        "        pred = ...\n",
        "        # 4. Update the prediction\n",
        "        F_m = F_m + ...\n",
        "\n",
        "    return trees\n",
        "\n",
        "def predict_gradient_boosting_regression(X_test, y_train, y_test, trees, learning_rate=0.1):\n",
        "    # Initialize with the mean of y_train\n",
        "    F_test = np.full((X_test.shape[0]), np.mean(y_train), dtype=np.float64)\n",
        "    for tree in trees:\n",
        "        # update the prediction\n",
        "        F_test = F_test + ...\n",
        "    return F_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9xGp6L5PSwA0"
      },
      "outputs": [],
      "source": [
        "# fit gradient boosting regression (the dataset is still housing)\n",
        "T = 10\n",
        "max_depths = [2, 4, 10]\n",
        "learning_rate = 0.1\n",
        "\n",
        "\n",
        "for max_depth in max_depths:\n",
        "    trees = fit_gradient_boosting_regression(X_train, y_train, T, learning_rate, max_depth, cat_features = [8])\n",
        "    y_pred = predict_gradient_boosting_regression(X_test, y_train, y_test, trees, learning_rate)\n",
        "    r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "    print(\"Max depth:\", max_depth)\n",
        "    print(\"Gradient Boosting Test R2 score:\", r2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vfA347w_SwA0"
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "\n",
        "# NOTE: sklearn requires that also categorical features are converted to numbers (the last feature ocean_proximity is categorical)\n",
        "x_ocean_proximity_train = X_train[:, -1]\n",
        "x_ocean_proximity_test = X_test[:, -1]\n",
        "\n",
        "_, x_oncean_train_numeric = np.unique(x_ocean_proximity_train, return_inverse=True)\n",
        "_, x_oncean_test_numeric = np.unique(x_ocean_proximity_test, return_inverse=True)\n",
        "\n",
        "X_train_num = np.concatenate((X_train[:, :-1], x_oncean_train_numeric.reshape(-1, 1)), axis=1)\n",
        "X_test_num = np.concatenate((X_test[:, :-1], x_oncean_test_numeric.reshape(-1, 1)), axis=1)\n",
        "\n",
        "\n",
        "gboost_sklearn = GradientBoostingRegressor(n_estimators=10, learning_rate=0.1, max_depth=4, random_state=0)\n",
        "gboost_sklearn.fit(X_train_num, y_train)\n",
        "y_pred = gboost_sklearn.predict(X_test_num)\n",
        "r2_test = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - np.mean(y_test)) ** 2)\n",
        "print(\"Test R2 score with sklearn GradientBoostingRegressor on Housing dataset: \", r2_test)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "capsule",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}