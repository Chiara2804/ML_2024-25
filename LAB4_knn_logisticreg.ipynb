{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x3_LBcP2MY0S"
      },
      "source": [
        "**ML COURSE 2024-2025**\n",
        "# LAB4: Classification with KNN and Logistic Regression\n",
        "\n",
        "In this notebook you will implement K-Nearest Neighbors and Logistic Regression for classification tasks.\n",
        "\n",
        "#### Summary\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Chiara2804/ML_2024-25.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6BNEyQp7Mbh7",
        "outputId": "ddb807af-584d-4c61-89ef-f3a706e88cdd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'ML_2024-25' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "wxEdRIz4MY0U"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import random"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVnnoa3uMY0V"
      },
      "source": [
        "| Task                   | Target                 | Example                     |\n",
        "|------------------------|-----------------------|-----------------------------|\n",
        "| Regression        | Continuous Values     | `{3.2, 5.7, 8.9}`            |\n",
        "| Classification     | Unordered/Ordered Categories  | `{üê∂ Dog, üê± Cat, üê∞ Rabbit}`        |\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3xPTzBZRMY0W"
      },
      "source": [
        "## Classification metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTJSZXuoMY0W"
      },
      "source": [
        "<span style=\"color: #4abde8;\">**Accuracy / classification rate**</span>\n",
        "$$\n",
        "\\text{Accuracy} = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}}\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "usm9i-2MMY0X",
        "outputId": "95039ea2-95e8-456e-bf77-17984ec3e4d8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.6666666666666666\n"
          ]
        }
      ],
      "source": [
        "def accuracy(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    y_true: array of true labels. Shape (n_samples, ) or (n_samples, 1)\n",
        "    y_pred: array of predicted labels. Shape (n_samples, ) or (n_samples, 1)\n",
        "    \"\"\"\n",
        "    total_predictions = len(y_true)\n",
        "    is_correct = (y_true == y_pred)\n",
        "    n_correct_pred = np.sum(is_correct)\n",
        "    accuracy = n_correct_pred / total_predictions\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "true_labels = np.array(['cat', 'dog', 'cat'])\n",
        "predicted_labels = np.array(['cat', 'dog', 'dog'])\n",
        "acc = accuracy(true_labels, predicted_labels)\n",
        "print(acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CV0g_40pMY0X"
      },
      "source": [
        "<span style=\"color: #1663aa;\">**Confusion Matrix**</span>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "R4O_8excMY0Y",
        "outputId": "07321a47-4f95-4d5a-f6cb-b8369ffb7757"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "def get_confusion_matrix(y_true, y_pred, classes):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    y_true: array of true labels. Shape (n_samples, ) or (n_samples, 1)\n",
        "    y_pred: array of predicted labels. Shape (n_samples, ) or (n_samples, 1)\n",
        "    classes: list of class labels\n",
        "    \"\"\"\n",
        "    n_classes = len(classes)\n",
        "    confusion_matrix = np.zeros((n_classes, n_classes))\n",
        "    for i in range(n_classes):\n",
        "        for j in range(n_classes):\n",
        "            confusion_matrix[i, j] = np.sum((y_true == classes[i]) & (y_pred == classes[j]))\n",
        "    return confusion_matrix\n",
        "\n",
        "\n",
        "classes = ['cat', 'dog', 'rabbit']\n",
        "true_labels      = np.array(['cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'rabbit', 'rabbit' ])\n",
        "predicted_labels = np.array(['cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'cat', 'rabbit', 'dog'])\n",
        "\n",
        "confusion_matrix = get_confusion_matrix(true_labels, predicted_labels, classes)\n",
        "sns.heatmap(confusion_matrix, annot=True, xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
        "plt.xlabel('Predicted labels')\n",
        "plt.ylabel('True labels')\n",
        "plt.title('Confusion matrix')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iH554iGlMY0Y"
      },
      "source": [
        "### Classification metrics for Binary Classification (Positive/Negative or 1/0 classes)\n",
        "| Actual \\ Predicted | Positive (P) | Negative (N) |\n",
        "|-------------------|-------------|-------------|\n",
        "| **Positive (P)**  | True Positive (TP) ‚úÖ | False Negative (FN) ‚ùå |\n",
        "| **Negative (N)**  | False Positive (FP) ‚ùå | True Negative (TN) ‚úÖ |\n",
        "\n",
        "\n",
        "Then we can define:\n",
        "- <span style=\"color: #4abde8;\">**Accuracy**</span>: $\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
        "\n",
        "- <span style=\"color: #f4a43a;\">**Specificity**</span>: $\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$\n",
        "\n",
        "- <span style=\"color: #0ea782;\">**Precision**</span>: $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$\n",
        "\n",
        "- <span style=\"color: #d586ab;\">**Recall**</span>: $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hokehcPKMY0Z",
        "outputId": "91e18a48-780f-4473-e595-fef7a5784987"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True Positives:  4  True Negatives:  2  False Positives:  1  False Negatives:  3\n",
            "Accuracy:  0.6\n",
            "Specificity:  0.6666666666666666\n",
            "Precision:  0.8\n",
            "Recall:  0.5714285714285714\n"
          ]
        }
      ],
      "source": [
        "def TP_TN_FP_FN(y_true, y_pred):\n",
        "    \"\"\"\n",
        "    Intuitive way to calculate TP, TN, FP, FN\n",
        "    \"\"\"\n",
        "    positive = (y_true == 1)            # array of the same shape as y_true, with True if y_true is pos/1 and False otherwise\n",
        "    negative = (y_true == 0)            # array of the same shape as y_true, with True if y_true is neg/0 and False otherwise\n",
        "    pred_positive = (y_pred == 1)       # array with True if the predicted label is pos/1, 0 otherwise\n",
        "    pred_negative = (y_pred == 0)       # array with True if the predicted label is neg/0, 0 otherwise\n",
        "\n",
        "    # element-wise boolean operation that returns 1 if both are True/1, 0 otherwise\n",
        "    TP = np.sum(positive & pred_positive)  # True Positives\n",
        "    TN = np.sum(negative & pred_negative)  # True Negatives\n",
        "    FP = np.sum(negative & pred_positive)  # False Positives\n",
        "    FN = np.sum(positive & pred_negative)  # False Negatives\n",
        "    return TP, TN, FP, FN\n",
        "\n",
        "\n",
        "def specificity(y_true, y_pred):\n",
        "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
        "    return TN / (TN + FP)\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
        "    return TP / (TP + FP)\n",
        "\n",
        "def recall(y_true, y_pred):\n",
        "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
        "    return TP / (TP + FN)\n",
        "\n",
        "\n",
        "y_true = np.array([1, 1, 1, 1, 0, 0, 0, 1, 1, 1])\n",
        "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 0, 0, 0])\n",
        "TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
        "acc = accuracy(y_true, y_pred)\n",
        "spec = specificity(y_true, y_pred)\n",
        "prec = precision(y_true, y_pred)\n",
        "rec = recall(y_true, y_pred)\n",
        "\n",
        "print(\"True Positives: \", TP, \" True Negatives: \", TN, \" False Positives: \", FP, \" False Negatives: \", FN)\n",
        "print(\"Accuracy: \", acc)\n",
        "print(\"Specificity: \", spec)\n",
        "print(\"Precision: \", prec)\n",
        "print(\"Recall: \", rec)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hL3pjZDkMY0a"
      },
      "source": [
        "## Stratified Splitting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9yyP9yEPMY0a"
      },
      "source": [
        "I.e., how to split data into training and test sets while keeping the same proportions of different classes (i.e. if your dataset has 70% cats and 30% dogs, the training and test sets will also have about 70% dogs and 30% cats). This helps the model to generalize better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "AW7iNcQ1MY0a"
      },
      "outputs": [],
      "source": [
        "def stratified_train_test_split(X, y, train_prop):\n",
        "    \"\"\"\n",
        "    Split randomly the dataset into training and test sets while preserving class proportions.\n",
        "\n",
        "    Args:\n",
        "    X: array of samples. Shape (n_samples, n_features)\n",
        "    y: array of labels. Shape (n_samples, ) or (n_samples, 1)\n",
        "    train_prop: proportion of samples to include in the training set. Must be between 0 and 1.\n",
        "\n",
        "    Returns:\n",
        "    X_train: array of training samples. Shape (n_train_samples, n_features)\n",
        "    X_test: array of test samples. Shape (n_test_samples, n_features)\n",
        "    y_train: array of training labels. Shape (n_train_samples, ) or (n_train_samples, 1)\n",
        "    y_test: array of test labels. Shape (n_test_samples, ) or (n_test_samples, 1)\n",
        "    \"\"\"\n",
        "    unique_classes = np.unique(y)                           # get unique class labels (i.e., the different classes)\n",
        "\n",
        "    #liste vuote per memorizzare gli indici\n",
        "    train_indices = []\n",
        "    test_indices = []\n",
        "\n",
        "    for cls in unique_classes:\n",
        "        indices = np.where(y == cls)[0]                     # find indices of samples belonging to class cls\n",
        "        np.random.shuffle(indices)                          # shuffle the indices for randomness\n",
        "        split_idx = int(indices.shape[0] * train_prop)      # determine split index\n",
        "\n",
        "        #lo posso fare perch√® prima ho fatto lo shuffle\n",
        "        cls_train_indices = indices[:split_idx].tolist()    # get the first split_idx indices for training for class cls\n",
        "        cls_test_indices = indices[split_idx:].tolist()     # get the remaining indices for testing for class cls\n",
        "\n",
        "        train_indices = train_indices + cls_train_indices   # concatenate the training indices for all classes\n",
        "        test_indices = test_indices + cls_test_indices      # concatenate the test indices for all classes\n",
        "\n",
        "    # shuffle again to avoid unintended ordering (right now the indices are ordered by class)\n",
        "    np.random.shuffle(train_indices)\n",
        "    np.random.shuffle(test_indices)\n",
        "\n",
        "    X_train, y_train = X[train_indices], y[train_indices]\n",
        "    X_test, y_test = X[test_indices], y[test_indices]\n",
        "    return X_train, X_test, y_train, y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZyfdiGnMY0a"
      },
      "source": [
        "## k-Nearest Neighbors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rq2HlOt1MY0b"
      },
      "source": [
        "Key idea: find a predefined number k of training samples closests in distance to the new point, and predict the label from these:\n",
        "- for classification: majority voting, i.e, the most frequent class among the k neighbors.\n",
        "- for regression: average (or weighted average) of neighbors' values.\n",
        "\n",
        "![image.png](https://www.jcchouinard.com/wp-content/uploads/2021/08/image-8.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U8LP8iqfMY0b"
      },
      "source": [
        "Different type of distances can be used (most common: Euclidean)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cc8W4hEbMY0b"
      },
      "outputs": [],
      "source": [
        "def euclidean_distance(x1, x2):\n",
        "    return np.sqrt(np.sum((x1 - x2)**2))\n",
        "\n",
        "def manhattan_distance(x1, x2):\n",
        "    return np.sum(np.abs(x1 - x2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "_4YBz1dTMY0b"
      },
      "outputs": [],
      "source": [
        "def KNN_classifier(X_train, y_train, X_test, k, distance_metric):\n",
        "    \"\"\"\n",
        "    Args:\n",
        "    X_train: array of training samples. Shape (n_samples, n_features)\n",
        "    y_train: array of training labels. Shape (n_samples,) or (n_samples, 1)\n",
        "    X_test: array of test samples. Shape (n_samples_test, n_features)\n",
        "    k: number of neighbors to consider\n",
        "    distance_metric: 'euclidean' or 'manhattan'\n",
        "    \"\"\"\n",
        "    n_train_samples = X_train.shape[0]\n",
        "    n_test_samples = X_test.shape[0]\n",
        "\n",
        "    y_train = y_train.flatten()                 # make sure y_train is 1D array\n",
        "\n",
        "    # check if k>number of training samples: if so  set k to the number of training samples\n",
        "    if k > n_train_samples:\n",
        "        print(\"Warning: k > number of training samples. Setting k to number of training samples\")\n",
        "        k = n_train_samples\n",
        "\n",
        "    predictions = []\n",
        "\n",
        "    # iterate over each test sample\n",
        "    for j in range(n_test_samples):\n",
        "        x_test = X_test[j, :]\n",
        "\n",
        "        # compute the distance between the each test sample and all training samples\n",
        "        pairs_dist_lab = []\n",
        "        for j in range(n_train_samples):\n",
        "            if distance_metric == 'euclidean':\n",
        "                dist = euclidean_distance(X_train[j,:], x_test)\n",
        "            elif distance_metric == 'manhattan':\n",
        "                dist = manhattan_distance(X_train[j,:], x_test)\n",
        "\n",
        "            # store each pair (distance, label)\n",
        "            pairs_dist_lab.append((dist, y_train[j]))\n",
        "\n",
        "        # sort the pairs (distance, label) by distance in ascending order\n",
        "        pairs_dist_lab = sorted(pairs_dist_lab, key = lambda x:x[0])\n",
        "\n",
        "        # get the first k pairs (i.e. the k-nearest neighbors, with smallest distance)\n",
        "        k_nearest = pairs_dist_lab[:k]\n",
        "\n",
        "        # from the k pairs extract the labels\n",
        "        k_nearest_labels = []\n",
        "        for dist, label in k_nearest:\n",
        "            k_nearest_labels.append(label)\n",
        "\n",
        "        # get the most common label\n",
        "        unique_labels, counts = np.unique(k_nearest_labels, return_counts=True)\n",
        "        idx_max_count = np.argmax(counts)\n",
        "        most_common_label = unique_labels[idx_max_count]\n",
        "        predictions.append(most_common_label)\n",
        "\n",
        "    predictions = np.array(predictions)\n",
        "    return predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnfEDNfCMY0c"
      },
      "source": [
        "### k-NN on IRIS data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "rS5nS16HMY0c"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 362
        },
        "id": "qzuBN8t2MY0c",
        "outputId": "1f3d7f8f-6f8d-4d8d-bfc2-2ef15f586398"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
              "0                5.1               3.5                1.4               0.2   \n",
              "1                4.9               3.0                1.4               0.2   \n",
              "2                4.7               3.2                1.3               0.2   \n",
              "3                4.6               3.1                1.5               0.2   \n",
              "4                5.0               3.6                1.4               0.2   \n",
              "\n",
              "       species  \n",
              "0  Iris-setosa  \n",
              "1  Iris-setosa  \n",
              "2  Iris-setosa  \n",
              "3  Iris-setosa  \n",
              "4  Iris-setosa  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-604372a5-9f57-484d-a381-70619debb8c9\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal length (cm)</th>\n",
              "      <th>sepal width (cm)</th>\n",
              "      <th>petal length (cm)</th>\n",
              "      <th>petal width (cm)</th>\n",
              "      <th>species</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-604372a5-9f57-484d-a381-70619debb8c9')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-604372a5-9f57-484d-a381-70619debb8c9 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-604372a5-9f57-484d-a381-70619debb8c9');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-2a390d4d-16b4-4f53-98f9-0ade957e3e1d\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-2a390d4d-16b4-4f53-98f9-0ade957e3e1d')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-2a390d4d-16b4-4f53-98f9-0ade957e3e1d button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"print(\\\"y shape: \\\", y\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"sepal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2073644135332772,\n        \"min\": 4.6,\n        \"max\": 5.1,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          4.9,\n          5.0,\n          4.7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"sepal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.2588435821108957,\n        \"min\": 3.0,\n        \"max\": 3.6,\n        \"num_unique_values\": 5,\n        \"samples\": [\n          3.0,\n          3.6,\n          3.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal length (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.07071067811865474,\n        \"min\": 1.3,\n        \"max\": 1.5,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1.4,\n          1.3,\n          1.5\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"petal width (cm)\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.0,\n        \"min\": 0.2,\n        \"max\": 0.2,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.2\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"species\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          \"Iris-setosa\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classes:  ['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
            "X shape:  (150, 2)\n",
            "y shape:  (150, 1)\n"
          ]
        }
      ],
      "source": [
        "# Load data: IRIS considering only the features 'sepal_length', 'sepal_width' and with target 'species'\n",
        "data = pd.read_csv('/content/ML_2024-25/drive/MyDrive/AA24-25ML/iris.csv')\n",
        "display(data.head())\n",
        "data = data[['sepal length (cm)', 'sepal width (cm)', 'species']]\n",
        "\n",
        "# let's see what are the classes in the dataset\n",
        "classes = data['species'].unique()\n",
        "print(\"Classes: \", classes)\n",
        "\n",
        "# Split data into X and y numpy arrays\n",
        "X = data.drop('species', axis=1).to_numpy()\n",
        "y = data[['species']].to_numpy()\n",
        "n_samples = X.shape[0]\n",
        "n_features = X.shape[1]\n",
        "\n",
        "# check the shapes\n",
        "print(\"X shape: \", X.shape)\n",
        "print(\"y shape: \", y.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqjeszCbMY0c"
      },
      "source": [
        "<div style=\"background-color: lightblue; padding: 10px; color: black\">\n",
        "<strong> ‚ö†Ô∏è Warning! </strong> The expression <code>y = data[['species']].to_numpy() </code> returns a 2D array of shape (n_samples, 1), while <code>y = data['species'].to_numpy() </code> returns a 1D array of shape (n_samples,).\n",
        "Both are valid, but be careful of the following:\n",
        "<ol>\n",
        "    <li>Ensure true labels and predicted labels have consistent shapes.</li>\n",
        "    <li>Gradient computation formulas usually assume <code>y</code> is a column vector <code>(n_samples, 1)</code>.</li>\n",
        "    <li>External libraries may require a 1D array instead.</li>\n",
        "</ol>\n",
        "Always verify shapes at each step: many errors, even silent ones, stem from mismatched dimensions!\n",
        "</div>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DTeGMGwFMY0c"
      },
      "source": [
        "**Tip üïµ**\n",
        "If you need (and you will) to convert the labels into numerical values (e.g., _Iris-setosa_ ‚Üí 0, _Iris-versicolor_ ‚Üí 1, _Iris-virginica_ ‚Üí 2) you can use:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JU0zYXGMMY0c",
        "outputId": "e071bac4-6038-4957-c95c-b00631c4b18a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [0]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [1]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]\n",
            " [2]]\n",
            "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
          ]
        }
      ],
      "source": [
        "classes, y_numeric = np.unique(y, return_inverse=True) #converte le classi in numeri\n",
        "print(y_numeric)\n",
        "print(classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jdv1dZ0SMY0d"
      },
      "source": [
        "Let's see KNN in action:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qq8HI9_MY0d"
      },
      "outputs": [],
      "source": [
        "k_values = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
        "\n",
        "# MC-CV parameters\n",
        "train_set_prop = 0.8\n",
        "n_iter_mc_cv = 25    #standard\n",
        "\n",
        "miss_c_rates = []\n",
        "\n",
        "for k in k_values:\n",
        "    miss_c_rates_k = []   # miss classification rate = 1 - accuracy\n",
        "\n",
        "    # MonteCarlo Cross Validation Loop\n",
        "    for i in range(n_iter_mc_cv):\n",
        "        # split the data into random train and stest\n",
        "        X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, train_set_prop)\n",
        "\n",
        "        y_pred = KNN_classifier(X_train, y_train, X_test, k, 'euclidean')\n",
        "        y_pred = y_pred.reshape(y_test.shape)                               # make sure y_pred has the same shape as y_test\n",
        "\n",
        "        # calculate the accuracy\n",
        "        mcr = 1- accuracy(y_test, y_pred)\n",
        "        miss_c_rates_k.append(mcr)\n",
        "\n",
        "    miss_c_rates.append(miss_c_rates_k)\n",
        "\n",
        "# BOXPLOTS + mean values\n",
        "plt.figure()\n",
        "plt.boxplot(miss_c_rates, labels=k_values, showmeans=True)\n",
        "plt.ylim(bottom=0)\n",
        "plt.xlabel('k-values')\n",
        "plt.ylabel('Miss Classification Rate')\n",
        "plt.title('KNN classifier with different k-values')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQ0UXitVMY0d"
      },
      "source": [
        "## Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UTDyqobtMY0d"
      },
      "source": [
        "### Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8jRC4KXFMY0d"
      },
      "source": [
        "#### The logistic (sigmoid)\n",
        "The **logistic regression** model uses a linear combination of the input through a logistic function trasformation, also called *sigmoid*:\n",
        "$$\n",
        "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\sigma(x_{i}^T\\beta) = \\frac{1}{1+e^{-x_i^T\\beta}}\n",
        "$$\n",
        "\n",
        "Since the sigmoid directly outputs numbers in (0,1], it has a probabilistic interpretation: we can consider the outputs as the probability of the positive outcome.\n",
        "$$\n",
        "P(y_i=1|x_i) = \\sigma(x_i^T\\beta)\n",
        "$$\n",
        "$$\n",
        "P(y_i=0|x_i) = 1-\\sigma(x_i^T\\beta)\n",
        "$$\n",
        "hence:\n",
        "$$\n",
        "\\hat{y_i} =\n",
        "\\begin{cases}\n",
        "    1, & \\text{if } \\sigma(x_i^T\\beta) \\geq 0.5 \\\\\n",
        "    0, & \\text{if } \\sigma(x_i^T\\beta) < 0.5 \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "#### Linear vs Logistic Regression\n",
        "By passing the linear function through a sigmoid function, real values are mapped in (0,1).\n",
        "\n",
        "![Linear vs Logistic Regression](https://github.com/Chiara2804/ML_2024-25/blob/main/figures/linearVSlogistic.png?raw=1)\n",
        "\n",
        "- üü® Yellow Square: z = -10 , y' = 0\n",
        "- üî¥ Red Dot: z = 0, y' = 0.5\n",
        "- üíú‚≠ê Purple Star: z = 5, y' = 0.99\n",
        "\n",
        "#### Solving Logistic Regression\n",
        "As in LASSO regression, there is no closed solution when a sigmoid function is applied; we need to resort to Gradient Descent.  \n",
        "What is our loss function? Can we use the minimum squared error?\n",
        "\n",
        "$$\n",
        "J(\\beta,y) = \\textcolor{red}{\\frac{1}{n}}\\sum_{i=1}^n\\textcolor{red}{\\frac{1}{2}}(y_i-\\sigma(\\beta_0+\\sum_{j=1}^p\\beta_j x_{ij}))^2\n",
        "$$\n",
        "\n",
        "The function above is generally **NOT** convex, gradient descent doesn't work. We need to find an alternative loss (error) function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Yvo1YYzxMY0d"
      },
      "outputs": [],
      "source": [
        "# Function that, given X_expanded and beta, computes the sigmoid of X_expanded@beta\n",
        "def sigmoid(z):\n",
        "    return ...\n",
        "\n",
        "# Minimum squared error\n",
        "def mse(y, y_pred):\n",
        "    return ...\n",
        "\n",
        "# Function that creates a synthetic dataset, with binary target\n",
        "def create_dataset():\n",
        "    # mean of the features\n",
        "    mean = np.array([0, 0, 0])\n",
        "    # covariance between the features\n",
        "    cov = np.array([\n",
        "        [1, 0.1, 0.5],\n",
        "        [0.1, 1, 0.9],\n",
        "        [0.5, 0.9, 1]])\n",
        "    # number of samples\n",
        "    n_samples = 100\n",
        "\n",
        "    np.random.seed(0)\n",
        "    # generate samples from a multivariate normal distribution with mean and covariance defined above\n",
        "    ds = np.random.multivariate_normal(mean, cov, n_samples)\n",
        "\n",
        "    # exclude the target\n",
        "    X = ds[:, :-1]\n",
        "    #consider only the last feature as target\n",
        "    y = ds[:, -1]\n",
        "    # binarize y (difference with respect to laboratory 3)\n",
        "    y = (y > np.median(y)).astype(int)\n",
        "    y = y.reshape(-1, 1)\n",
        "\n",
        "    return X, y"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gspswpErMY0d"
      },
      "outputs": [],
      "source": [
        "#create the dataset\n",
        "X, y = create_dataset()\n",
        "beta_fixed = [-3, 0, 30]\n",
        "# consider various values for beta[1]\n",
        "beta_1_vec = np.linspace(0, 50, 1000)\n",
        "\n",
        "# compute the errors for each value of beta_1_vec\n",
        "errors_mse = []\n",
        "for beta_1 in beta_1_vec:\n",
        "    # beta[0] and beta[2] are fixed\n",
        "    beta = np.array([beta_fixed[0], beta_1, beta_fixed[2]])\n",
        "    # add the column of ones to X\n",
        "    X_expanded = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "\n",
        "    # compute the sigmoid\n",
        "    y_pred = sigmoid(X_expanded @ beta)\n",
        "    # compute the error (MSE)\n",
        "    error = mse(y.flatten(), y_pred.flatten())\n",
        "    errors_mse.append(error)\n",
        "\n",
        "# plot the errors (over the values of beta_1)\n",
        "plt.plot(beta_1_vec, errors_mse)\n",
        "plt.xlabel('beta_1')\n",
        "plt.ylabel('MSE')\n",
        "plt.title('MSE vs beta_1')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLIULVDKMY0d"
      },
      "source": [
        "#### Negative Log Likelihood\n",
        "Metric that captures a classification error.\n",
        "$$\n",
        "\\begin{align*}\n",
        "\\mathcal{L}(\\beta) &= - \\sum_{i=1}^{n} \\left[ y_i \\log \\sigma\\left(\\sum_{j=1}^{p} \\beta_j x_{ij} \\right) + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\sum_{j=1}^{p} \\beta_j x_{ij} \\right)\\right) \\right] \\\\\n",
        "&= - \\sum_{i=1}^{n} \\left[ y_i \\log \\sigma(x_i^\\top \\beta) + (1 - y_i) \\log (1 - \\sigma(x_i^\\top \\beta)) \\right]\n",
        "\\end{align*}\n",
        "$$\n",
        "It's convex! We can use it compute gradient descent and fine the optimal values of $\\beta$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aRys0zrHMY0e"
      },
      "outputs": [],
      "source": [
        "# Negative log-likelihood\n",
        "def negLogLikelihood(y, y_pred):\n",
        "    # Clip predictions to avoid log(0)\n",
        "    return ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4jjW2m7tMY0e"
      },
      "outputs": [],
      "source": [
        "#consider various values for beta[1]\n",
        "beta_1_vec = np.linspace(0, 50, 1000)\n",
        "\n",
        "#compute the errors for each value of beta_1_vec\n",
        "errors_nll = []\n",
        "for beta_1 in beta_1_vec:\n",
        "    #beta[0] and beta[2] are fixed\n",
        "    beta = np.array([beta_fixed[0], beta_1, beta_fixed[2]])\n",
        "    #add the column of ones to X\n",
        "    X_expanded = np.hstack([np.ones((X.shape[0], 1)), X])\n",
        "    #compute the sigmoid\n",
        "    y_pred = sigmoid(X_expanded @ beta)\n",
        "    #compute the error (NLL)\n",
        "    error = negLogLikelihood(y.flatten(), y_pred.flatten())\n",
        "    errors_nll.append(error)\n",
        "\n",
        "#plot the errors\n",
        "plt.plot(beta_1_vec, errors_nll)\n",
        "plt.xlabel('beta_1')\n",
        "plt.ylabel('NLL')\n",
        "plt.title('NLL vs beta_1')\n",
        "plt.grid()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N-0XyzdMY0e"
      },
      "source": [
        "#### Gradient Descent on Logistic Regression\n",
        "The gradient descent is performed similarly to the gradient descent of LASSO. Main differences:\n",
        "- Instead of the gradient of MSE + Lasso term, we compute the gradient of the negative log likelihood.\n",
        "- Instead of computing the prediction as $y_{pred} = X\\beta$, we compute it as $y_{pred} = \\sigma(X\\beta)$\n",
        "\n",
        "The gradient of the negative log likelihood is:\n",
        "$$\n",
        "\\nabla L(\\beta) = \\frac{X^T(\\hat{y}-y)}{n}\n",
        "$$\n",
        "where the matrix $X$ has been expanded to include an additional column of ones at the beginning.\n",
        "\n",
        "We do not show explicitly how the formula of the gradient is derived; if you're interested in having more details, you can find them [here](https://ml-explained.com/blog/logistic-regression-explained#deriving-the-gradient-descent-formula-for-logistic-regression-optional)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9dhrmE1LMY0e"
      },
      "outputs": [],
      "source": [
        "def nll_gradient(X_expanded, y_true, y_pred):\n",
        "    \"\"\"Compute the gradient of the negative log-likelihood loss.\n",
        "\n",
        "    Args:\n",
        "    X_expanded: array of shape (n_samples, n_features + 1) with the samples and a column of ones\n",
        "    y_true: array of shape (n_samples, 1) with the true labels\n",
        "    y_pred: array of shape (n_samples, 1) with the predicted labels\n",
        "    \"\"\"\n",
        "    # check if the dimensions of y_true and y_pred are correct\n",
        "    assert y_true.ndim == 2, \"y_true should be a column vector of shape (n_samples, 1)\"\n",
        "    assert y_pred.ndim == 2, \"y_pred should be a column vector of shape (n_samples, 1)\"\n",
        "\n",
        "    n_samples = X_expanded.shape[0]  # Number of samples\n",
        "    return ...  # Shape (p+1, 1)\n",
        "\n",
        "def GD_update(beta, gradient, learning_rate):\n",
        "    \"\"\"Performs a gradient descent update.\"\"\"\n",
        "    return beta - learning_rate * gradient\n",
        "\n",
        "def train_logistic_regression_GD(X, y, initial_learning_rate, decay_rate, n_iter):\n",
        "    \"\"\"Train logistic regression using gradient descent.\n",
        "\n",
        "    Args:\n",
        "    X: (n_samples, n_features) - Input features\n",
        "    y: (n_samples, ) - Binary labels\n",
        "    initial_learning_rate: float - Initial learning rate\n",
        "    decay_rate: float - Learning rate decay factor\n",
        "    n_iter: int - Number of iterations\n",
        "\n",
        "    Returns:\n",
        "    beta: (n_features + 1, 1) - Learned parameters\n",
        "    \"\"\"\n",
        "    np.random.seed(0)\n",
        "    n_samples, n_features = X.shape\n",
        "\n",
        "    X_expanded = np.hstack([np.ones((n_samples, 1)), X])  # Add column of ones\n",
        "\n",
        "    beta = np.random.rand(n_features + 1, 1)  # Shape (p+1, 1) for binary classification\n",
        "\n",
        "    # Training loop\n",
        "    for i in range(n_iter):\n",
        "        learning_rate = initial_learning_rate/(1+decay_rate*i)  # update the learning rate\n",
        "        y_pred = ... # Sigmoid activation for binary classification.\n",
        "        gradient = ...\n",
        "        beta = ...\n",
        "\n",
        "    return beta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FvgEdRTpMY0f"
      },
      "source": [
        "### Logistic Regression for multiclass classsification on IRIS data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wGAAvWDnMY0f"
      },
      "source": [
        "In some cases, we want to predict the class of a sample, and the possible set of classes has cardinality greater than two. For example, in the Iris dataset, the prediction is the species, and there are three possible predictions: 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ceotSUfMY0g"
      },
      "outputs": [],
      "source": [
        "#set the seed\n",
        "np.random.seed(0)\n",
        "# Load again the data: IRIS considering only the features 'sepal_length', 'sepal_width' and with target 'species'\n",
        "data = pd.read_csv('drive/MyDrive/AA24-25ML/iris.csv')\n",
        "data = data[['sepal length (cm)', 'sepal width (cm)', 'species']]\n",
        "classes = data['species'].unique()\n",
        "\n",
        "# Split data into X and y numpy arrays\n",
        "X = data.drop('species', axis=1).to_numpy()\n",
        "y = data['species'].to_numpy()\n",
        "\n",
        "n_samples = X.shape[0]\n",
        "n_features = X.shape[1]\n",
        "# check the shapes\n",
        "print(\"X shape: \", X.shape)\n",
        "print(\"y shape: \", y.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AJGX6tQRMY0u"
      },
      "outputs": [],
      "source": [
        "# Divide X and y into training and test sets using stratified sampling\n",
        "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, 0.8)\n",
        "print(\"Training set size: \", X_train.shape[0])\n",
        "print(\"Test set size: \", X_test.shape[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oq4Q30ZgMY0v"
      },
      "source": [
        "#### One VS One\n",
        "One possible strategy for solving multi-class classification is called **one vs one**. Specifically:\n",
        "- For each pair of class <code>(class_1, class_2)</code> we train a classifier that distinguishes between <code>(class_1, class_2)</code> (samples that were associated with the other classes are temporally removed from the training set). If we have $K$ classes, we will have $\\frac{K(K-1)}{2}$ classifiers.\n",
        "- At test time, we pass the new samples to classify through each of the $\\frac{K(K-1)}{2}$ classifiers.\n",
        "- We count how many time each class was predicted, hence \"won\" against the other classes.\n",
        "- We predict the class that won most frequently.\n",
        "There are different ways in which ties can be handled; today, we will break them arbitrarly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4tctMkpMY0v"
      },
      "outputs": [],
      "source": [
        "def train_oneVSone(X_train, y_train , classes, learning_rate, decay_rate, n_iter):\n",
        "    \"\"\"\n",
        "    X_train: matrix of training data\n",
        "    y_train: vector of training labels\n",
        "    classes: list of classes to consider\n",
        "\n",
        "    Returns a dictionary containing the optimal parameters for each model, trained on each pair of classes using Logistic Regression\n",
        "    \"\"\"\n",
        "    # Dictionary to store the optimal betas for each model (one for each pair of classes)\n",
        "    optimal_parameters = {}\n",
        "\n",
        "    # Iterate through class pairs and train classifiers\n",
        "    pairs = [(classes[0], classes[1]), (classes[1], classes[2]), (classes[2], classes[0])]\n",
        "\n",
        "    for class_1, class_2 in pairs:\n",
        "        print(f\"Training classifier for {class_1} vs {class_2}\")\n",
        "\n",
        "        # Filter training data for the selected class pair\n",
        "        mask = (y_train == class_1) | (y_train == class_2)  # Boolean mask for filtering\n",
        "        X_train_subset = X_train[mask]  # Select relevant samples\n",
        "        y_train_subset = y_train[mask]  # Select relevant labels\n",
        "\n",
        "        # Convert labels to 0 or 1 (1 if class_1, 0 otherwise)\n",
        "        y_train_subset = np.where(y_train_subset == class_1, 1, 0).astype(int)\n",
        "\n",
        "        # Train logistic regression model using gradient descent\n",
        "        beta = train_logistic_regression_GD(X_train_subset, y_train_subset, learning_rate, decay_rate, n_iter)\n",
        "\n",
        "        # Save model parameters\n",
        "        optimal_parameters[(class_1, class_2)] = beta\n",
        "\n",
        "    return optimal_parameters\n",
        "\n",
        "def predict_oneVSone(X_test, y_test, optimal_parameters):\n",
        "    \"\"\"\n",
        "    X_test : matrix of test data\n",
        "    y_test : vector of test labels\n",
        "    optimal_parameters : dictionary containing the optimal parameters for each model\n",
        "\n",
        "    Returns a vector of final predictions based on majority voting of all the models trained on classes pairs using Logistic Regression\n",
        "    \"\"\"\n",
        "    # Initialize vote counts; it contains one dictionary for each test sample\n",
        "    vote_counts = [{} for _ in range(len(y_test))]  # One dictionary for each test sample\n",
        "\n",
        "    # Evaluate classifiers on the test set\n",
        "    for (class_1, class_2), beta in optimal_parameters.items(): #optimal_parameters.items() returns the tuple (key, value) for each item in the dictionary (the keys are tuples (class_1, class_2), the values are betas)\n",
        "        #add column of ones to the test set\n",
        "        X_test_expanded = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
        "        #compute the probabilities\n",
        "        probabilities = ...\n",
        "        #compute the prediction\n",
        "        y_pred = ...\n",
        "\n",
        "        for idx in range(len(y_test)):\n",
        "            pred = y_pred[idx]  # Get the predicted class label\n",
        "            predicted_class = class_1 if pred else class_2 # the model predicts class_1 if pred is 1, class_2 if pred is 0\n",
        "\n",
        "            # if the class is already in the dictionary, increment the count, otherwise add the class to the dictionary\n",
        "            if predicted_class in vote_counts[idx]:\n",
        "                vote_counts[idx][predicted_class] += 1\n",
        "            else:\n",
        "                vote_counts[idx][predicted_class] = 1\n",
        "\n",
        "    # Determine final prediction based on majority voting\n",
        "    final_predictions = []\n",
        "    for i in range(len(vote_counts)):\n",
        "        votes = vote_counts[i] # Get the dictionary of votes for the i-th test sample\n",
        "\n",
        "        # Extract keys and values separately\n",
        "        classes = list(votes.keys()) # Get the classes\n",
        "        vote_numbers = list(votes.values()) # Get how many times each class has been predicted\n",
        "\n",
        "        # Find the maximum vote count\n",
        "        max_votes = ...\n",
        "\n",
        "        # Get all classes that have the max vote count (to handle ties)\n",
        "        tied_classes = ...\n",
        "\n",
        "        # Randomly select one class in case of a tie\n",
        "        predicted_class = ...\n",
        "\n",
        "        final_predictions.append(predicted_class)\n",
        "\n",
        "    return final_predictions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mhb2UkDxMY0w"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "decay_rate = 0.0001\n",
        "n_iter = 10000              # Number of iterations\n",
        "learning_rate = 0.1\n",
        "optimal_parameters_oneVSone = train_oneVSone(X_train, y_train, classes, learning_rate, decay_rate, n_iter)\n",
        "final_predictions_oneVSone = predict_oneVSone(X_test, y_test, optimal_parameters_oneVSone)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gvxZpcBxMY0w"
      },
      "outputs": [],
      "source": [
        "# compute accuracy of prediction\n",
        "acc = accuracy(y_test, final_predictions_oneVSone)\n",
        "print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pRhh7DveMY0w"
      },
      "source": [
        "#### One VS All"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mlUn5S97MY0w"
      },
      "source": [
        "Another possible strategy for solving multi-class classification is called **one vs all**. Specifically:\n",
        "- For each class <code>class_1</code> we train a classifier that distinguishes between <code>class_1</code> and <code>not class_1</code> (all samples from the training set are considered). If we have $K$ classes, we will have $K$ classifiers.\n",
        "- At test time, we pass the new samples to classify through each of the $K$ classifiers.\n",
        "- We save the confidence score (probability) derived from each classifier.\n",
        "- We predict the class with the highest confidence score."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HXnRAFn5MY0x"
      },
      "outputs": [],
      "source": [
        "def train_oneVSall(X_train, y_train, classes, learning_rate, decay_rate, n_iter):\n",
        "    \"\"\"\n",
        "    X_train: matrix of training data\n",
        "    y_train: vector of training labels\n",
        "    classes: list of classes to consider\n",
        "\n",
        "    Returns a dictionary containing the optimal parameters for each model, trained on each class using Logistic Regression\n",
        "    \"\"\"\n",
        "    # Dictionary to store the optimal betas for each model (one for each class)\n",
        "    optimal_parameters = {}\n",
        "\n",
        "    # Iterate through each unique class and train a one-vs-all classifier\n",
        "    for i in range(len(classes)):\n",
        "        class_1 = classes[i]  # Get the class label\n",
        "\n",
        "        print(f\"Training classifier for {class_1} vs all\")\n",
        "\n",
        "        # Convert class labels to binary (1 if the class is class_1, 0 otherwise)\n",
        "        y_train_binary = np.where(y_train == class_1, 1, 0).astype(int)  # 1 for class_1, 0 for all others\n",
        "\n",
        "        # Train logistic regression model using gradient descent\n",
        "        beta = train_logistic_regression_GD(X_train, y_train_binary, learning_rate, decay_rate, n_iter)\n",
        "\n",
        "        # Save model parameters (betas)\n",
        "        optimal_parameters[class_1] = beta  # Keyed by the class label\n",
        "\n",
        "    return optimal_parameters\n",
        "\n",
        "def predict_oneVSall(X_test, y_test, optimal_parameters):\n",
        "    \"\"\"\n",
        "    X_test : matrix of test data\n",
        "    y_test : vector of test labels\n",
        "    optimal_parameters: dictionary containing the optimal parameters for each model\n",
        "\n",
        "    Returns a vector of final predictions based on majority voting of all the models trained on each class using Logistic Regression\n",
        "    \"\"\"\n",
        "    # Initialize probability dictionary for each test sample (for each sample, it will contain the probability of each class, as computed by the corresponding model)\n",
        "    probabilities = [{} for _ in range(len(y_test))]\n",
        "\n",
        "    # Iterate through each class and compute probabilities using the corresponding model (beta)\n",
        "    for class_1, beta in optimal_parameters.items():\n",
        "        # Predict probabilities for class_1 using the custom logistic regression\n",
        "        X_test_expanded = np.hstack([np.ones((X_test.shape[0], 1)), X_test])  # Add a column of ones (bias term)\n",
        "        class_probs = ...  # Compute the probabilities using the sigmoid function\n",
        "\n",
        "        # Store probabilities in the dictionary for each test sample\n",
        "        for i in range(len(y_test)):\n",
        "            probabilities[i][class_1] = class_probs[i, 0]  # Assign the probability for class_1\n",
        "\n",
        "    # Initialize final predictions list\n",
        "    final_predictions = []\n",
        "\n",
        "    # Iterate through each test sample's probability dictionary\n",
        "    for probs in probabilities:\n",
        "        # Extract class labels\n",
        "        class_labels = list(probs.keys())\n",
        "\n",
        "        # Extract probability values\n",
        "        prob_values = list(probs.values())\n",
        "\n",
        "        # Find the class with the highest probability\n",
        "        max_index = ...\n",
        "        predicted_class = ...\n",
        "\n",
        "        # Store the final prediction\n",
        "        final_predictions.append(predicted_class)\n",
        "\n",
        "    return final_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiQ_CqU0MY0x"
      },
      "outputs": [],
      "source": [
        "# Training hyperparameters\n",
        "decay_rate = 0.0001\n",
        "n_iter = 10000              # Number of iterations\n",
        "learning_rate = 0.1\n",
        "optimal_parameters_oneVSall = train_oneVSall(X_train, y_train, classes, learning_rate, decay_rate, n_iter)\n",
        "final_predictions_oneVSall = predict_oneVSall(X_test, y_test, optimal_parameters_oneVSall)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WYfHe9IeMY0x"
      },
      "outputs": [],
      "source": [
        "# Compute and print the accuracy of the predictions\n",
        "acc = accuracy(y_test, final_predictions_oneVSall)\n",
        "print(\"Accuracy: \", acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SpZMuVTuMY0x"
      },
      "source": [
        "## Decision boundaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LxQXCaBbMY0y"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.datasets import make_blobs\n",
        "from utils import plot_decision_boundary_2d, create_2d_meshpoints, plot_probability_boundary, plot_combined_probability_boundary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dma-oWU2MY0y"
      },
      "outputs": [],
      "source": [
        "# 2-dimensional dataset\n",
        "\n",
        "# Generate synthetic dataset\n",
        "data = make_blobs(n_samples=100, centers=3, n_features=2, random_state=0) #make_blobs creates a synthetic dataset such that in the y target there are 3 classes and 3 clusters\n",
        "assert len(data) == 2\n",
        "X, y = data\n",
        "\n",
        "n_features = X.shape[1]\n",
        "resolution = 400\n",
        "\n",
        "# Generate a dense grid of points covering the 2D feature space.\n",
        "# If X has more than two features, PCA is applied to reduce it to two dimensions.\n",
        "# X_grid: Flattened array of all grid points in the 2D space.\n",
        "# xx, yy: Meshgrid matrices representing x and y coordinates for visualization.\n",
        "# X_2d: Original dataset X, potentially reduced to 2D if necessary.\n",
        "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, resolution)\n",
        "\n",
        "# Train logistic regression model using scikit-learn\n",
        "# parameters: multi_class='ovr' (one-vs-rest), solver='lbfgs' (optimization algorithm), max_iter=1000 (maximum number of iterations), random_state=0 (seed)\n",
        "clf = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=0)\n",
        "# train the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Define the probability function using the trained model\n",
        "probability_function = clf.predict_proba\n",
        "\n",
        "# Plot decision boundary\n",
        "plot_decision_boundary_2d(X_grid, y, probability_function, xx, yy, X_2d, n_features)\n",
        "\n",
        "# Plot probability boundary (it shows how confident the model is about its predictions)\n",
        "plot_probability_boundary(probability_function, X, y)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-_PtyMOaMY0y"
      },
      "source": [
        "If the features are > 2, PCA is used to plot the decision boundary in 2D: the first and the second principal components are shown."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5E4t-nYXMY0y"
      },
      "outputs": [],
      "source": [
        "# Generate multi-class data (3 classes)\n",
        "data = make_blobs(n_samples=100, centers=3, n_features=6, random_state=0) # in this case, there are 6 features\n",
        "assert len(data) == 2\n",
        "X, y = data\n",
        "\n",
        "n_features = X.shape[1]\n",
        "resolution = 400\n",
        "\n",
        "# Generate a dense grid of points covering the 2D feature space.\n",
        "# If X has more than two features, PCA is applied to reduce it to two dimensions.\n",
        "# X_grid: Flattened array of all grid points in the 2D space.\n",
        "# xx, yy: Meshgrid matrices representing x and y coordinates for visualization.\n",
        "# X_2d: Original dataset X, potentially reduced to 2D if necessary.\n",
        "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, resolution)\n",
        "\n",
        "# Train logistic regression model using scikit-learn\n",
        "# parameters: multi_class='ovr' (one-vs-rest), solver='lbfgs' (optimization algorithm), max_iter=1000 (maximum number of iterations), random_state=0 (seed)\n",
        "clf = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=0)\n",
        "# train the model\n",
        "clf.fit(X, y)\n",
        "\n",
        "# Define the probability function using the trained model\n",
        "probability_function = clf.predict_proba\n",
        "\n",
        "# Plot decision boundary\n",
        "plot_decision_boundary_2d(X_grid, y, probability_function, xx, yy, X_2d, n_features)\n",
        "\n",
        "# Plot probability boundary\n",
        "plot_probability_boundary(probability_function, X, y)\n",
        "\n",
        "# combined the boundaries computed above in one figure\n",
        "plot_combined_probability_boundary(probability_function, X_grid, X_2d, xx, yy, X, y, linewidth=3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Olmecd6sMY0z"
      },
      "source": [
        "## üèãÔ∏è‚Äç‚ôÄÔ∏è Exercises"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNFQPGZOMY0z"
      },
      "source": [
        "### Binary classification with K-NN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_4UNIYCMY0z"
      },
      "source": [
        "1. Load the 'breast_cancer.csv' file in drive/MyDrive/AA24-25ML\n",
        "    - Get X and y arrays. Use as target the 'target' column.\n",
        "    - Divide randomly, using stratified sampling, in X_train, X_test, y_train, y_test.\n",
        "2. Rewrite the K-NN classification function:\n",
        "    - Such that, together with the most common labels it returns the average distance from neighbors.\n",
        "    - Using only euclidean distance.\n",
        "    - Be careful not to overwrite the method by choosing the same name of the one already implemented.\n",
        "3. Make predictions on the test set. Use k=20.\n",
        "4. Compute, by re-implementing the functions from scratch:\n",
        "    - accuracy\n",
        "    - specificity\n",
        "    - precision\n",
        "    - recall\n",
        "\n",
        "Compare with the already implemented ones."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DgjKZXx2MY0z"
      },
      "outputs": [],
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YvjkIPRGMY0z"
      },
      "source": [
        "### Logistic Regression with Scikit-learn\n",
        "Using the same dataset:\n",
        "- Train a Logistic Regression model using the Scikit-learn implementation.\n",
        "- Make predictions on the test set using the `.predict(...)` method.\n",
        "- Compute accuracy, specificity, recall, precision and plot confusion matrix.\n",
        "- Comment the results and compare with K-NN ones.\n",
        "- What if we standardize data? What happens? Comment the results."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}