{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ML COURSE 2024-2025**\n",
    "# LAB4: Classification with KNN and Logistic Regression\n",
    "\n",
    "In this notebook you will implement K-Nearest Neighbors and Logistic Regression for classification tasks. \n",
    "\n",
    "#### Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Task                   | Target                 | Example                     |\n",
    "|------------------------|-----------------------|-----------------------------|\n",
    "| Regression        | Continuous Values     | `{3.2, 5.7, 8.9}`            |\n",
    "| Classification     | Unordered/Ordered Categories  | `{üê∂ Dog, üê± Cat, üê∞ Rabbit}`        |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #4abde8;\">**Accuracy / classification rate**</span>\n",
    "$$\n",
    "\\text{Accuracy} = \\frac{\\text{number of correct predictions}}{\\text{total number of predictions}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_true, y_pred): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    y_true: array of true labels. Shape (n_samples, ) or (n_samples, 1)\n",
    "    y_pred: array of predicted labels. Shape (n_samples, ) or (n_samples, 1)\n",
    "    \"\"\"\n",
    "    total_predictions = len(y_true)\n",
    "    is_correct = (y_true == y_pred)\n",
    "    n_correct_pred = np.sum(is_correct)\n",
    "    accuracy = n_correct_pred / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "true_labels = np.array(['cat', 'dog', 'cat'])\n",
    "predicted_labels = np.array(['cat', 'dog', 'dog'])\n",
    "acc = accuracy(true_labels, predicted_labels)\n",
    "print(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color: #1663aa;\">**Confusion Matrix**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_confusion_matrix(y_true, y_pred, classes): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    y_true: array of true labels. Shape (n_samples, ) or (n_samples, 1)\n",
    "    y_pred: array of predicted labels. Shape (n_samples, ) or (n_samples, 1)\n",
    "    classes: list of class labels\n",
    "    \"\"\"\n",
    "    n_classes = len(classes)\n",
    "    confusion_matrix = np.zeros((n_classes, n_classes))\n",
    "    for i in range(n_classes):\n",
    "        for j in range(n_classes):\n",
    "            confusion_matrix[i, j] = np.sum((y_true == classes[i]) & (y_pred == classes[j]))\n",
    "    return confusion_matrix\n",
    "\n",
    "\n",
    "classes = ['cat', 'dog', 'rabbit']\n",
    "true_labels      = np.array(['cat', 'cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'rabbit', 'rabbit' ])\n",
    "predicted_labels = np.array(['cat', 'cat', 'cat', 'dog', 'dog', 'dog', 'cat', 'rabbit', 'dog'])\n",
    "\n",
    "confusion_matrix = get_confusion_matrix(true_labels, predicted_labels, classes)\n",
    "sns.heatmap(confusion_matrix, annot=True, xticklabels=classes, yticklabels=classes, cmap='Blues')\n",
    "plt.xlabel('Predicted labels')\n",
    "plt.ylabel('True labels')\n",
    "plt.title('Confusion matrix')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification metrics for Binary Classification (Positive/Negative or 1/0 classes)\n",
    "| Actual \\ Predicted | Positive (P) | Negative (N) |\n",
    "|-------------------|-------------|-------------|\n",
    "| **Positive (P)**  | True Positive (TP) ‚úÖ | False Negative (FN) ‚ùå |\n",
    "| **Negative (N)**  | False Positive (FP) ‚ùå | True Negative (TN) ‚úÖ |\n",
    "\n",
    "\n",
    "Then we can define: \n",
    "- <span style=\"color: #4abde8;\">**Accuracy**</span>: $\\frac{\\text{TP} + \\text{TN}}{\\text{TP} + \\text{TN} + \\text{FP} + \\text{FN}}$\n",
    "\n",
    "- <span style=\"color: #f4a43a;\">**Specificity**</span>: $\\frac{\\text{TN}}{\\text{TN} + \\text{FP}}$\n",
    "\n",
    "- <span style=\"color: #0ea782;\">**Precision**</span>: $\\frac{\\text{TP}}{\\text{TP} + \\text{FP}}$ \n",
    "\n",
    "- <span style=\"color: #d586ab;\">**Recall**</span>: $\\frac{\\text{TP}}{\\text{TP} + \\text{FN}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TP_TN_FP_FN(y_true, y_pred): \n",
    "    \"\"\" \n",
    "    Intuitive way to calculate TP, TN, FP, FN\n",
    "    \"\"\"\n",
    "    positive = (y_true == 1)            # array of the same shape as y_true, with True if y_true is pos/1 and False otherwise\n",
    "    negative = (y_true == 0)            # array of the same shape as y_true, with True if y_true is neg/0 and False otherwise\n",
    "    pred_positive = (y_pred == 1)       # array with True if the predicted label is pos/1, 0 otherwise\n",
    "    pred_negative = (y_pred == 0)       # array with True if the predicted label is neg/0, 0 otherwise\n",
    "\n",
    "    # element-wise boolean operation that returns 1 if both are True/1, 0 otherwise\n",
    "    TP = np.sum(positive & pred_positive)  # True Positives\n",
    "    TN = np.sum(negative & pred_negative)  # True Negatives\n",
    "    FP = np.sum(negative & pred_positive)  # False Positives\n",
    "    FN = np.sum(positive & pred_negative)  # False Negatives\n",
    "    return TP, TN, FP, FN\n",
    "\n",
    "\n",
    "def specificity(y_true, y_pred): \n",
    "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
    "    return TN / (TN + FP)\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
    "    return TP / (TP + FP)\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
    "    return TP / (TP + FN)\n",
    "\n",
    "\n",
    "y_true = np.array([1, 1, 1, 1, 0, 0, 0, 1, 1, 1])\n",
    "y_pred = np.array([1, 1, 1, 1, 0, 0, 1, 0, 0, 0])\n",
    "TP, TN, FP, FN = TP_TN_FP_FN(y_true, y_pred)\n",
    "acc = accuracy(y_true, y_pred)\n",
    "spec = specificity(y_true, y_pred)\n",
    "prec = precision(y_true, y_pred)\n",
    "rec = recall(y_true, y_pred)\n",
    "\n",
    "print(\"True Positives: \", TP, \" True Negatives: \", TN, \" False Positives: \", FP, \" False Negatives: \", FN)\n",
    "print(\"Accuracy: \", acc)\n",
    "print(\"Specificity: \", spec)\n",
    "print(\"Precision: \", prec)\n",
    "print(\"Recall: \", rec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stratified Splitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I.e., how to split data into training and test sets while keeping the same proportions of different classes (i.e. if your dataset has 70% cats and 30% dogs, the training and test sets will also have about 70% dogs and 30% cats). This helps the model to generalize better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_train_test_split(X, y, train_prop): \n",
    "    \"\"\"\n",
    "    Split randomly the dataset into training and test sets while preserving class proportions.\n",
    "\n",
    "    Args: \n",
    "    X: array of samples. Shape (n_samples, n_features)\n",
    "    y: array of labels. Shape (n_samples, ) or (n_samples, 1)\n",
    "    train_prop: proportion of samples to include in the training set. Must be between 0 and 1.\n",
    "\n",
    "    Returns: \n",
    "    X_train: array of training samples. Shape (n_train_samples, n_features)\n",
    "    X_test: array of test samples. Shape (n_test_samples, n_features)\n",
    "    y_train: array of training labels. Shape (n_train_samples, ) or (n_train_samples, 1)\n",
    "    y_test: array of test labels. Shape (n_test_samples, ) or (n_test_samples, 1)\n",
    "    \"\"\"\n",
    "    unique_classes = np.unique(y)                           # get unique class labels (i.e., the different classes)\n",
    "\n",
    "    train_indices = []                     \n",
    "    test_indices = []\n",
    "\n",
    "    for cls in unique_classes: \n",
    "        indices = np.where(y == cls)[0]                     # find indices of samples belonging to class cls\n",
    "        np.random.shuffle(indices)                          # shuffle the indices for randomness\n",
    "        split_idx = int(indices.shape[0] * train_prop)      # determine split index \n",
    "\n",
    "        cls_train_indices = indices[:split_idx].tolist()    # get the first split_idx indices for training for class cls\n",
    "        cls_test_indices = indices[split_idx:].tolist()     # get the remaining indices for testing for class cls\n",
    "\n",
    "        train_indices = train_indices + cls_train_indices   # concatenate the training indices for all classes\n",
    "        test_indices = test_indices + cls_test_indices      # concatenate the test indices for all classes\n",
    "\n",
    "    # shuffle again to avoid unintended ordering (right now the indices are ordered by class)\n",
    "    np.random.shuffle(train_indices)\n",
    "    np.random.shuffle(test_indices)\n",
    "    \n",
    "    X_train, y_train = X[train_indices], y[train_indices]\n",
    "    X_test, y_test = X[test_indices], y[test_indices]\n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## k-Nearest Neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Key idea: find a predefined number k of training samples closests in distance to the new point, and predict the label from these:\n",
    "- for classification: majority voting, i.e, the most frequent class among the k neighbors.\n",
    "- for regression: average (or weighted average) of neighbors' values.\n",
    "\n",
    "![image.png](https://www.jcchouinard.com/wp-content/uploads/2021/08/image-8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Different type of distances can be used (most common: Euclidean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_distance(x1, x2): \n",
    "    return np.sqrt(np.sum((x1 - x2)**2))\n",
    "\n",
    "def manhattan_distance(x1, x2): \n",
    "    return np.sum(np.abs(x1 - x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KNN_classifier(X_train, y_train, X_test, k, distance_metric): \n",
    "    \"\"\"\n",
    "    Args:\n",
    "    X_train: array of training samples. Shape (n_samples, n_features)\n",
    "    y_train: array of training labels. Shape (n_samples,) or (n_samples, 1)\n",
    "    X_test: array of test samples. Shape (n_samples_test, n_features)\n",
    "    k: number of neighbors to consider\n",
    "    distance_metric: 'euclidean' or 'manhattan'\n",
    "    \"\"\"\n",
    "    n_train_samples = X_train.shape[0]\n",
    "    n_test_samples = X_test.shape[0]\n",
    "    \n",
    "    y_train = y_train.flatten()                 # make sure y_train is 1D array\n",
    "\n",
    "    # check if k>number of training samples: if so  set k to the number of training samples\n",
    "    if k > n_train_samples: \n",
    "        print(\"Warning: k > number of training samples. Setting k to number of training samples\")\n",
    "        k = n_train_samples\n",
    "\n",
    "    predictions = []\n",
    "    \n",
    "    # iterate over each test sample\n",
    "    for j in range(n_test_samples):\n",
    "        x_test = X_test[j, :]\n",
    "\n",
    "        # compute the distance between the each test sample and all training samples                     \n",
    "        pairs_dist_lab = []\n",
    "        for j in range(n_train_samples):\n",
    "            if distance_metric == 'euclidean':\n",
    "                dist = euclidean_distance(X_train[j,:], x_test)\n",
    "            elif distance_metric == 'manhattan':\n",
    "                dist = manhattan_distance(X_train[j,:], x_test)\n",
    "                \n",
    "            # store each pair (distance, label)\n",
    "            pairs_dist_lab.append((dist, y_train[j]))                    \n",
    "\n",
    "        # sort the pairs (distance, label) by distance in ascending order \n",
    "\n",
    "        # get the first k pairs (i.e. the k-nearest neighbors, with smallest distance)\n",
    "\n",
    "        # from the k pairs extract the labels \n",
    "        \n",
    "        # get the most common label\n",
    "                                        \n",
    "    \n",
    "    predictions = np.array(predictions)                                         \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### k-NN on IRIS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data: IRIS considering only the features 'sepal_length', 'sepal_width' and with target 'species'\n",
    "data = pd.read_csv('drive/MyDrive/AA24-25ML/iris.csv')\n",
    "display(data.head())\n",
    "data = data[['sepal length (cm)', 'sepal width (cm)', 'species']]\n",
    "\n",
    "# let's see what are the classes in the dataset\n",
    "classes = data['species'].unique()\n",
    "print(\"Classes: \", classes)\n",
    "\n",
    "# Split data into X and y numpy arrays\n",
    "X = data.drop('species', axis=1).to_numpy()\n",
    "y = data[['species']].to_numpy()                     \n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# check the shapes\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color: lightblue; padding: 10px; color: black\">\n",
    "<strong> ‚ö†Ô∏è Warning! </strong> The expression <code>y = data[['species']].to_numpy() </code> returns a 2D array of shape (n_samples, 1), while <code>y = data['species'].to_numpy() </code> returns a 1D array of shape (n_samples,). \n",
    "Both are valid, but be careful of the following:\n",
    "<ol> \n",
    "    <li>Ensure true labels and predicted labels have consistent shapes.</li>\n",
    "    <li>Gradient computation formulas usually assume <code>y</code> is a column vector <code>(n_samples, 1)</code>.</li>\n",
    "    <li>External libraries may require a 1D array instead.</li>\n",
    "</ol> \n",
    "Always verify shapes at each step: many errors, even silent ones, stem from mismatched dimensions!\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Tip üïµ**\n",
    "If you need (and you will) to convert the labels into numerical values (e.g., _Iris-setosa_ ‚Üí 0, _Iris-versicolor_ ‚Üí 1, _Iris-virginica_ ‚Üí 2) you can use: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes, y_numeric = np.unique(y, return_inverse=True)\n",
    "print(y_numeric)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see KNN in action:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_values = [1, 2, 3, 5, 10, 20, 30, 50, 100]\n",
    "\n",
    "# MC-CV parameters\n",
    "train_set_prop = 0.8  \n",
    "n_iter_mc_cv = 25     \n",
    "\n",
    "miss_c_rates = []\n",
    "\n",
    "for k in k_values:\n",
    "    miss_c_rates_k = []   # miss classification rate = 1 - accuracy\n",
    "    \n",
    "    # MC Cross Validation Loop\n",
    "    for i in range(n_iter_mc_cv):\n",
    "        # split the data into random train and stest\n",
    "        X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, train_set_prop)\n",
    "\n",
    "        y_pred = KNN_classifier(X_train, y_train, X_test, k, 'euclidean')\n",
    "        y_pred = y_pred.reshape(y_test.shape)                               # make sure y_pred has the same shape as y_test\n",
    "\n",
    "        # calculate the accuracy\n",
    "        mcr = 1- accuracy(y_test, y_pred)\n",
    "        miss_c_rates_k.append(mcr)\n",
    "\n",
    "    miss_c_rates.append(miss_c_rates_k)\n",
    "\n",
    "# BOXPLOTS + mean values\n",
    "plt.figure()\n",
    "plt.boxplot(miss_c_rates, labels=k_values, showmeans=True) \n",
    "plt.ylim(bottom=0)        \n",
    "plt.xlabel('k-values')\n",
    "plt.ylabel('Miss Classification Rate')\n",
    "plt.title('KNN classifier with different k-values')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The logistic (sigmoid) \n",
    "The **logistic regression** model uses a linear combination of the input through a logistic function trasformation, also called *sigmoid*:\n",
    "$$\n",
    "\\sigma(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\sigma(x_{i}^T\\beta) = \\frac{1}{1+e^{-x_i^T\\beta}}\n",
    "$$\n",
    "\n",
    "Since the sigmoid directly outputs numbers in (0,1], it has a probabilistic interpretation: we can consider the outputs as the probability of the positive outcome.\n",
    "$$\n",
    "P(y_i=1|x_i) = \\sigma(x_i^T\\beta)\n",
    "$$\n",
    "$$\n",
    "P(y_i=0|x_i) = 1-\\sigma(x_i^T\\beta)\n",
    "$$\n",
    "hence:\n",
    "$$\n",
    "\\hat{y_i} =\n",
    "\\begin{cases}\n",
    "    1, & \\text{if } \\sigma(x_i^T\\beta) \\geq 0.5 \\\\\n",
    "    0, & \\text{if } \\sigma(x_i^T\\beta) < 0.5 \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "#### Linear vs Logistic Regression\n",
    "By passing the linear function through a sigmoid function, real values are mapped in (0,1).\n",
    "\n",
    "![Linear vs Logistic Regression](figures/linearVSlogistic.png)\n",
    "\n",
    "- üü® Yellow Square: z = -10 , y' = 0\n",
    "- üî¥ Red Dot: z = 0, y' = 0.5\n",
    "- üíú‚≠ê Purple Star: z = 5, y' = 0.99\n",
    "\n",
    "#### Solving Logistic Regression\n",
    "As in LASSO regression, there is no closed solution when a sigmoid function is applied; we need to resort to Gradient Descent.  \n",
    "What is our loss function? Can we use the minimum squared error?\n",
    "\n",
    "$$\n",
    "J(\\beta,y) = \\textcolor{red}{\\frac{1}{n}}\\sum_{i=1}^n\\textcolor{red}{\\frac{1}{2}}(y_i-\\sigma(\\beta_0+\\sum_{j=1}^p\\beta_j x_{ij}))^2\n",
    "$$\n",
    "\n",
    "The function above is generally **NOT** convex, gradient descent doesn't work. We need to find an alternative loss (error) function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that, given X_expanded and beta, computes the sigmoid of X_expanded@beta\n",
    "def sigmoid(z):\n",
    "    return ...\n",
    "\n",
    "# Minimum squared error\n",
    "def mse(y, y_pred):\n",
    "    return ...\n",
    "\n",
    "# Function that creates a synthetic dataset, with binary target\n",
    "def create_dataset():\n",
    "    # mean of the features\n",
    "    mean = np.array([0, 0, 0])\n",
    "    # covariance between the features\n",
    "    cov = np.array([\n",
    "        [1, 0.1, 0.5], \n",
    "        [0.1, 1, 0.9],\n",
    "        [0.5, 0.9, 1]])\n",
    "    # number of samples\n",
    "    n_samples = 100\n",
    "\n",
    "    np.random.seed(0)\n",
    "    # generate samples from a multivariate normal distribution with mean and covariance defined above\n",
    "    ds = np.random.multivariate_normal(mean, cov, n_samples)\n",
    "\n",
    "    # exclude the target\n",
    "    X = ds[:, :-1]\n",
    "    #consider only the last feature as target\n",
    "    y = ds[:, -1]\n",
    "    # binarize y (difference with respect to laboratory 3)\n",
    "    y = (y > np.median(y)).astype(int)\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create the dataset\n",
    "X, y = create_dataset()\n",
    "beta_fixed = [-3, 0, 30]\n",
    "# consider various values for beta[1]\n",
    "beta_1_vec = np.linspace(0, 50, 1000)\n",
    "\n",
    "# compute the errors for each value of beta_1_vec\n",
    "errors_mse = []\n",
    "for beta_1 in beta_1_vec:\n",
    "    # beta[0] and beta[2] are fixed\n",
    "    beta = np.array([beta_fixed[0], beta_1, beta_fixed[2]])\n",
    "    # add the column of ones to X\n",
    "    X_expanded = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "\n",
    "    # compute the sigmoid\n",
    "    y_pred = sigmoid(X_expanded @ beta)\n",
    "    # compute the error (MSE)\n",
    "    error = mse(y.flatten(), y_pred.flatten())\n",
    "    errors_mse.append(error)\n",
    "\n",
    "# plot the errors (over the values of beta_1)\n",
    "plt.plot(beta_1_vec, errors_mse)\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('MSE')\n",
    "plt.title('MSE vs beta_1')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Negative Log Likelihood\n",
    "Metric that captures a classification error.\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\mathcal{L}(\\beta) &= - \\sum_{i=1}^{n} \\left[ y_i \\log \\sigma\\left(\\sum_{j=1}^{p} \\beta_j x_{ij} \\right) + (1 - y_i) \\log \\left(1 - \\sigma\\left(\\sum_{j=1}^{p} \\beta_j x_{ij} \\right)\\right) \\right] \\\\\n",
    "&= - \\sum_{i=1}^{n} \\left[ y_i \\log \\sigma(x_i^\\top \\beta) + (1 - y_i) \\log (1 - \\sigma(x_i^\\top \\beta)) \\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "It's convex! We can use it compute gradient descent and fine the optimal values of $\\beta$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Negative log-likelihood\n",
    "def negLogLikelihood(y, y_pred):\n",
    "    # Clip predictions to avoid log(0)\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#consider various values for beta[1]\n",
    "beta_1_vec = np.linspace(0, 50, 1000)\n",
    "\n",
    "#compute the errors for each value of beta_1_vec\n",
    "errors_nll = []\n",
    "for beta_1 in beta_1_vec:\n",
    "    #beta[0] and beta[2] are fixed\n",
    "    beta = np.array([beta_fixed[0], beta_1, beta_fixed[2]])\n",
    "    #add the column of ones to X\n",
    "    X_expanded = np.hstack([np.ones((X.shape[0], 1)), X])\n",
    "    #compute the sigmoid\n",
    "    y_pred = sigmoid(X_expanded @ beta)\n",
    "    #compute the error (NLL)\n",
    "    error = negLogLikelihood(y.flatten(), y_pred.flatten())\n",
    "    errors_nll.append(error)\n",
    "\n",
    "#plot the errors\n",
    "plt.plot(beta_1_vec, errors_nll)\n",
    "plt.xlabel('beta_1')\n",
    "plt.ylabel('NLL')\n",
    "plt.title('NLL vs beta_1')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gradient Descent on Logistic Regression\n",
    "The gradient descent is performed similarly to the gradient descent of LASSO. Main differences:\n",
    "- Instead of the gradient of MSE + Lasso term, we compute the gradient of the negative log likelihood.\n",
    "- Instead of computing the prediction as $y_{pred} = X\\beta$, we compute it as $y_{pred} = \\sigma(X\\beta)$\n",
    "\n",
    "The gradient of the negative log likelihood is:\n",
    "$$\n",
    "\\nabla L(\\beta) = \\frac{X^T(\\hat{y}-y)}{n}\n",
    "$$\n",
    "where the matrix $X$ has been expanded to include an additional column of ones at the beginning.\n",
    "\n",
    "We do not show explicitly how the formula of the gradient is derived; if you're interested in having more details, you can find them [here](https://ml-explained.com/blog/logistic-regression-explained#deriving-the-gradient-descent-formula-for-logistic-regression-optional)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nll_gradient(X_expanded, y_true, y_pred):\n",
    "    \"\"\"Compute the gradient of the negative log-likelihood loss.\n",
    "    \n",
    "    Args:\n",
    "    X_expanded: array of shape (n_samples, n_features + 1) with the samples and a column of ones\n",
    "    y_true: array of shape (n_samples, 1) with the true labels\n",
    "    y_pred: array of shape (n_samples, 1) with the predicted labels\n",
    "    \"\"\"\n",
    "    # check if the dimensions of y_true and y_pred are correct\n",
    "    assert y_true.ndim == 2, \"y_true should be a column vector of shape (n_samples, 1)\"\n",
    "    assert y_pred.ndim == 2, \"y_pred should be a column vector of shape (n_samples, 1)\"\n",
    "\n",
    "    n_samples = X_expanded.shape[0]  # Number of samples\n",
    "    return ...  # Shape (p+1, 1)\n",
    "\n",
    "def GD_update(beta, gradient, learning_rate):\n",
    "    \"\"\"Performs a gradient descent update.\"\"\"\n",
    "    return beta - learning_rate * gradient\n",
    "\n",
    "def train_logistic_regression_GD(X, y, initial_learning_rate, decay_rate, n_iter):\n",
    "    \"\"\"Train logistic regression using gradient descent.\n",
    "    \n",
    "    Args:\n",
    "    X: (n_samples, n_features) - Input features\n",
    "    y: (n_samples, ) - Binary labels\n",
    "    initial_learning_rate: float - Initial learning rate\n",
    "    decay_rate: float - Learning rate decay factor\n",
    "    n_iter: int - Number of iterations\n",
    "    \n",
    "    Returns:\n",
    "    beta: (n_features + 1, 1) - Learned parameters\n",
    "    \"\"\"\n",
    "    np.random.seed(0)\n",
    "    n_samples, n_features = X.shape\n",
    "\n",
    "    X_expanded = np.hstack([np.ones((n_samples, 1)), X])  # Add column of ones\n",
    "\n",
    "    beta = np.random.rand(n_features + 1, 1)  # Shape (p+1, 1) for binary classification\n",
    "\n",
    "    # Training loop\n",
    "    for i in range(n_iter):\n",
    "        learning_rate = initial_learning_rate/(1+decay_rate*i)  # update the learning rate\n",
    "        y_pred = ... # Sigmoid activation for binary classification. \n",
    "        gradient = ...\n",
    "        beta = ...\n",
    "\n",
    "    return beta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression for multiclass classsification on IRIS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, we want to predict the class of a sample, and the possible set of classes has cardinality greater than two. For example, in the Iris dataset, the prediction is the species, and there are three possible predictions: 'Iris-setosa', 'Iris-versicolor' and 'Iris-virginica'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set the seed\n",
    "np.random.seed(0)\n",
    "# Load again the data: IRIS considering only the features 'sepal_length', 'sepal_width' and with target 'species'\n",
    "data = pd.read_csv('drive/MyDrive/AA24-25ML/iris.csv')\n",
    "data = data[['sepal length (cm)', 'sepal width (cm)', 'species']]\n",
    "classes = data['species'].unique()\n",
    "\n",
    "# Split data into X and y numpy arrays\n",
    "X = data.drop('species', axis=1).to_numpy()\n",
    "y = data['species'].to_numpy()\n",
    "\n",
    "n_samples = X.shape[0]\n",
    "n_features = X.shape[1]\n",
    "# check the shapes\n",
    "print(\"X shape: \", X.shape)\n",
    "print(\"y shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Divide X and y into training and test sets using stratified sampling\n",
    "X_train, X_test, y_train, y_test = stratified_train_test_split(X, y, 0.8)\n",
    "print(\"Training set size: \", X_train.shape[0])\n",
    "print(\"Test set size: \", X_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One VS One\n",
    "One possible strategy for solving multi-class classification is called **one vs one**. Specifically:\n",
    "- For each pair of class <code>(class_1, class_2)</code> we train a classifier that distinguishes between <code>(class_1, class_2)</code> (samples that were associated with the other classes are temporally removed from the training set). If we have $K$ classes, we will have $\\frac{K(K-1)}{2}$ classifiers.\n",
    "- At test time, we pass the new samples to classify through each of the $\\frac{K(K-1)}{2}$ classifiers. \n",
    "- We count how many time each class was predicted, hence \"won\" against the other classes.\n",
    "- We predict the class that won most frequently.\n",
    "There are different ways in which ties can be handled; today, we will break them arbitrarly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oneVSone(X_train, y_train , classes, learning_rate, decay_rate, n_iter):\n",
    "    \"\"\"\n",
    "    X_train: matrix of training data\n",
    "    y_train: vector of training labels\n",
    "    classes: list of classes to consider\n",
    "\n",
    "    Returns a dictionary containing the optimal parameters for each model, trained on each pair of classes using Logistic Regression\n",
    "    \"\"\"\n",
    "    # Dictionary to store the optimal betas for each model (one for each pair of classes)\n",
    "    optimal_parameters = {}\n",
    "\n",
    "    # Iterate through class pairs and train classifiers\n",
    "    pairs = [(classes[0], classes[1]), (classes[1], classes[2]), (classes[2], classes[0])]\n",
    "\n",
    "    for class_1, class_2 in pairs:\n",
    "        print(f\"Training classifier for {class_1} vs {class_2}\")\n",
    "        \n",
    "        # Filter training data for the selected class pair\n",
    "        mask = (y_train == class_1) | (y_train == class_2)  # Boolean mask for filtering\n",
    "        X_train_subset = X_train[mask]  # Select relevant samples\n",
    "        y_train_subset = y_train[mask]  # Select relevant labels\n",
    "        \n",
    "        # Convert labels to 0 or 1 (1 if class_1, 0 otherwise)\n",
    "        y_train_subset = np.where(y_train_subset == class_1, 1, 0).astype(int)\n",
    "            \n",
    "        # Train logistic regression model using gradient descent\n",
    "        beta = train_logistic_regression_GD(X_train_subset, y_train_subset, learning_rate, decay_rate, n_iter)\n",
    "        \n",
    "        # Save model parameters\n",
    "        optimal_parameters[(class_1, class_2)] = beta\n",
    "\n",
    "    return optimal_parameters\n",
    "\n",
    "def predict_oneVSone(X_test, y_test, optimal_parameters):\n",
    "    \"\"\"\n",
    "    X_test : matrix of test data\n",
    "    y_test : vector of test labels\n",
    "    optimal_parameters : dictionary containing the optimal parameters for each model\n",
    "\n",
    "    Returns a vector of final predictions based on majority voting of all the models trained on classes pairs using Logistic Regression\n",
    "    \"\"\"\n",
    "    # Initialize vote counts; it contains one dictionary for each test sample\n",
    "    vote_counts = [{} for _ in range(len(y_test))]  # One dictionary for each test sample\n",
    "\n",
    "    # Evaluate classifiers on the test set\n",
    "    for (class_1, class_2), beta in optimal_parameters.items(): #optimal_parameters.items() returns the tuple (key, value) for each item in the dictionary (the keys are tuples (class_1, class_2), the values are betas)\n",
    "        #add column of ones to the test set\n",
    "        X_test_expanded = np.hstack([np.ones((X_test.shape[0], 1)), X_test])\n",
    "        #compute the probabilities\n",
    "        probabilities = ...\n",
    "        #compute the prediction\n",
    "        y_pred = ...\n",
    "\n",
    "        for idx in range(len(y_test)):\n",
    "            pred = y_pred[idx]  # Get the predicted class label\n",
    "            predicted_class = class_1 if pred else class_2 # the model predicts class_1 if pred is 1, class_2 if pred is 0\n",
    "            \n",
    "            # if the class is already in the dictionary, increment the count, otherwise add the class to the dictionary\n",
    "            if predicted_class in vote_counts[idx]:\n",
    "                vote_counts[idx][predicted_class] += 1\n",
    "            else:\n",
    "                vote_counts[idx][predicted_class] = 1\n",
    "\n",
    "    # Determine final prediction based on majority voting\n",
    "    final_predictions = []\n",
    "    for i in range(len(vote_counts)):\n",
    "        votes = vote_counts[i] # Get the dictionary of votes for the i-th test sample\n",
    "        \n",
    "        # Extract keys and values separately\n",
    "        classes = list(votes.keys()) # Get the classes\n",
    "        vote_numbers = list(votes.values()) # Get how many times each class has been predicted\n",
    "\n",
    "        # Find the maximum vote count\n",
    "        max_votes = ...\n",
    "\n",
    "        # Get all classes that have the max vote count (to handle ties)\n",
    "        tied_classes = ...\n",
    "\n",
    "        # Randomly select one class in case of a tie\n",
    "        predicted_class = ...\n",
    "        \n",
    "        final_predictions.append(predicted_class)\n",
    "\n",
    "    return final_predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "decay_rate = 0.0001\n",
    "n_iter = 10000              # Number of iterations\n",
    "learning_rate = 0.1\n",
    "optimal_parameters_oneVSone = train_oneVSone(X_train, y_train, classes, learning_rate, decay_rate, n_iter)\n",
    "final_predictions_oneVSone = predict_oneVSone(X_test, y_test, optimal_parameters_oneVSone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute accuracy of prediction\n",
    "acc = accuracy(y_test, final_predictions_oneVSone)\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One VS All"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another possible strategy for solving multi-class classification is called **one vs all**. Specifically:\n",
    "- For each class <code>class_1</code> we train a classifier that distinguishes between <code>class_1</code> and <code>not class_1</code> (all samples from the training set are considered). If we have $K$ classes, we will have $K$ classifiers.\n",
    "- At test time, we pass the new samples to classify through each of the $K$ classifiers. \n",
    "- We save the confidence score (probability) derived from each classifier.\n",
    "- We predict the class with the highest confidence score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_oneVSall(X_train, y_train, classes, learning_rate, decay_rate, n_iter):\n",
    "    \"\"\"\n",
    "    X_train: matrix of training data\n",
    "    y_train: vector of training labels\n",
    "    classes: list of classes to consider\n",
    "\n",
    "    Returns a dictionary containing the optimal parameters for each model, trained on each class using Logistic Regression\n",
    "    \"\"\"\n",
    "    # Dictionary to store the optimal betas for each model (one for each class)\n",
    "    optimal_parameters = {}\n",
    "\n",
    "    # Iterate through each unique class and train a one-vs-all classifier\n",
    "    for i in range(len(classes)):\n",
    "        class_1 = classes[i]  # Get the class label\n",
    "        \n",
    "        print(f\"Training classifier for {class_1} vs all\")\n",
    "        \n",
    "        # Convert class labels to binary (1 if the class is class_1, 0 otherwise)\n",
    "        y_train_binary = np.where(y_train == class_1, 1, 0).astype(int)  # 1 for class_1, 0 for all others\n",
    "        \n",
    "        # Train logistic regression model using gradient descent\n",
    "        beta = train_logistic_regression_GD(X_train, y_train_binary, learning_rate, decay_rate, n_iter)\n",
    "        \n",
    "        # Save model parameters (betas)\n",
    "        optimal_parameters[class_1] = beta  # Keyed by the class label\n",
    "    \n",
    "    return optimal_parameters\n",
    "\n",
    "def predict_oneVSall(X_test, y_test, optimal_parameters):\n",
    "    \"\"\"\n",
    "    X_test : matrix of test data\n",
    "    y_test : vector of test labels\n",
    "    optimal_parameters: dictionary containing the optimal parameters for each model\n",
    "\n",
    "    Returns a vector of final predictions based on majority voting of all the models trained on each class using Logistic Regression\n",
    "    \"\"\"\n",
    "    # Initialize probability dictionary for each test sample (for each sample, it will contain the probability of each class, as computed by the corresponding model)\n",
    "    probabilities = [{} for _ in range(len(y_test))]\n",
    "\n",
    "    # Iterate through each class and compute probabilities using the corresponding model (beta)\n",
    "    for class_1, beta in optimal_parameters.items():\n",
    "        # Predict probabilities for class_1 using the custom logistic regression\n",
    "        X_test_expanded = np.hstack([np.ones((X_test.shape[0], 1)), X_test])  # Add a column of ones (bias term)\n",
    "        class_probs = ...  # Compute the probabilities using the sigmoid function\n",
    "\n",
    "        # Store probabilities in the dictionary for each test sample\n",
    "        for i in range(len(y_test)):\n",
    "            probabilities[i][class_1] = class_probs[i, 0]  # Assign the probability for class_1\n",
    "\n",
    "    # Initialize final predictions list\n",
    "    final_predictions = []\n",
    "\n",
    "    # Iterate through each test sample's probability dictionary\n",
    "    for probs in probabilities:\n",
    "        # Extract class labels\n",
    "        class_labels = list(probs.keys())\n",
    "        \n",
    "        # Extract probability values\n",
    "        prob_values = list(probs.values())\n",
    "        \n",
    "        # Find the class with the highest probability\n",
    "        max_index = ...\n",
    "        predicted_class = ...\n",
    "        \n",
    "        # Store the final prediction\n",
    "        final_predictions.append(predicted_class)\n",
    "\n",
    "    return final_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training hyperparameters\n",
    "decay_rate = 0.0001\n",
    "n_iter = 10000              # Number of iterations\n",
    "learning_rate = 0.1\n",
    "optimal_parameters_oneVSall = train_oneVSall(X_train, y_train, classes, learning_rate, decay_rate, n_iter)\n",
    "final_predictions_oneVSall = predict_oneVSall(X_test, y_test, optimal_parameters_oneVSall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute and print the accuracy of the predictions\n",
    "acc = accuracy(y_test, final_predictions_oneVSall)\n",
    "print(\"Accuracy: \", acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_blobs\n",
    "from utils import plot_decision_boundary_2d, create_2d_meshpoints, plot_probability_boundary, plot_combined_probability_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2-dimensional dataset\n",
    "\n",
    "# Generate synthetic dataset\n",
    "data = make_blobs(n_samples=100, centers=3, n_features=2, random_state=0) #make_blobs creates a synthetic dataset such that in the y target there are 3 classes and 3 clusters\n",
    "assert len(data) == 2\n",
    "X, y = data\n",
    "\n",
    "n_features = X.shape[1]\n",
    "resolution = 400\n",
    "\n",
    "# Generate a dense grid of points covering the 2D feature space.\n",
    "# If X has more than two features, PCA is applied to reduce it to two dimensions.\n",
    "# X_grid: Flattened array of all grid points in the 2D space.\n",
    "# xx, yy: Meshgrid matrices representing x and y coordinates for visualization.\n",
    "# X_2d: Original dataset X, potentially reduced to 2D if necessary.\n",
    "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, resolution)\n",
    "\n",
    "# Train logistic regression model using scikit-learn\n",
    "# parameters: multi_class='ovr' (one-vs-rest), solver='lbfgs' (optimization algorithm), max_iter=1000 (maximum number of iterations), random_state=0 (seed)\n",
    "clf = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=0)\n",
    "# train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Define the probability function using the trained model\n",
    "probability_function = clf.predict_proba\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary_2d(X_grid, y, probability_function, xx, yy, X_2d, n_features)\n",
    "\n",
    "# Plot probability boundary (it shows how confident the model is about its predictions)\n",
    "plot_probability_boundary(probability_function, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the features are > 2, PCA is used to plot the decision boundary in 2D: the first and the second principal components are shown. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate multi-class data (3 classes)\n",
    "data = make_blobs(n_samples=100, centers=3, n_features=6, random_state=0) # in this case, there are 6 features\n",
    "assert len(data) == 2\n",
    "X, y = data\n",
    "\n",
    "n_features = X.shape[1]\n",
    "resolution = 400\n",
    "\n",
    "# Generate a dense grid of points covering the 2D feature space.\n",
    "# If X has more than two features, PCA is applied to reduce it to two dimensions.\n",
    "# X_grid: Flattened array of all grid points in the 2D space.\n",
    "# xx, yy: Meshgrid matrices representing x and y coordinates for visualization.\n",
    "# X_2d: Original dataset X, potentially reduced to 2D if necessary.\n",
    "X_grid, xx, yy, X_2d = create_2d_meshpoints(X, resolution)\n",
    "\n",
    "# Train logistic regression model using scikit-learn\n",
    "# parameters: multi_class='ovr' (one-vs-rest), solver='lbfgs' (optimization algorithm), max_iter=1000 (maximum number of iterations), random_state=0 (seed)\n",
    "clf = LogisticRegression(multi_class='ovr', solver='lbfgs', max_iter=1000, random_state=0)\n",
    "# train the model\n",
    "clf.fit(X, y)\n",
    "\n",
    "# Define the probability function using the trained model\n",
    "probability_function = clf.predict_proba\n",
    "\n",
    "# Plot decision boundary\n",
    "plot_decision_boundary_2d(X_grid, y, probability_function, xx, yy, X_2d, n_features)\n",
    "\n",
    "# Plot probability boundary\n",
    "plot_probability_boundary(probability_function, X, y)\n",
    "\n",
    "# combined the boundaries computed above in one figure\n",
    "plot_combined_probability_boundary(probability_function, X_grid, X_2d, xx, yy, X, y, linewidth=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üèãÔ∏è‚Äç‚ôÄÔ∏è Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binary classification with K-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Load the 'breast_cancer.csv' file in drive/MyDrive/AA24-25ML\n",
    "    - Get X and y arrays. Use as target the 'target' column.\n",
    "    - Divide randomly, using stratified sampling, in X_train, X_test, y_train, y_test. \n",
    "2. Rewrite the K-NN classification function:\n",
    "    - Such that, together with the most common labels it returns the average distance from neighbors. \n",
    "    - Using only euclidean distance.\n",
    "    - Be careful not to overwrite the method by choosing the same name of the one already implemented.\n",
    "3. Make predictions on the test set. Use k=20.\n",
    "4. Compute, by re-implementing the functions from scratch: \n",
    "    - accuracy\n",
    "    - specificity\n",
    "    - precision\n",
    "    - recall\n",
    "\n",
    "Compare with the already implemented ones. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "\n",
    "...\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Scikit-learn\n",
    "Using the same dataset:\n",
    "- Train a Logistic Regression model using the Scikit-learn implementation. \n",
    "- Make predictions on the test set using the `.predict(...)` method.\n",
    "- Compute accuracy, specificity, recall, precision and plot confusion matrix. \n",
    "- Comment the results and compare with K-NN ones.\n",
    "- What if we standardize data? What happens? Comment the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
